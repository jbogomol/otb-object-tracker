DATASET INFO
total videos:  66
# training videos:    37
# validation videos:  7
# testing videos:     22
image pairs in train set:       126798
image pairs in validation set:  27452
image pairs in test set:        75668 

INPUT SHAPES
imgs.shape: torch.Size([64, 6, 256, 256])
v.shape: torch.Size([64, 2]) 

NETWORK INFO
NetworkClassifier(
  (conv1): Conv2d(6, 16, kernel_size=(3, 3), stride=(2, 2))
  (conv1_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))
  (conv2_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))
  (conv3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=61504, out_features=240, bias=True)
  (fc2): Linear(in_features=240, out_features=150, bias=True)
  (out): Linear(in_features=150, out_features=130, bias=True)
) 

BEGIN TRAINING LOOP
epoch 1,	batch    10,	training loss: 8.029
epoch 1,	batch    20,	training loss: 6.913
epoch 1,	batch    30,	training loss: 6.099
epoch 1,	batch    40,	training loss: 5.810
epoch 1,	batch    50,	training loss: 5.487
epoch 1,	batch    60,	training loss: 5.734
epoch 1,	batch    70,	training loss: 5.521
epoch 1,	batch    80,	training loss: 5.562
epoch 1,	batch    90,	training loss: 5.431
epoch 1,	batch   100,	training loss: 5.574
epoch 1,	batch   110,	training loss: 5.317
epoch 1,	batch   120,	training loss: 5.469
epoch 1,	batch   130,	training loss: 5.479
epoch 1,	batch   140,	training loss: 5.324
epoch 1,	batch   150,	training loss: 5.311
epoch 1,	batch   160,	training loss: 5.251
epoch 1,	batch   170,	training loss: 5.143
epoch 1,	batch   180,	training loss: 5.235
epoch 1,	batch   190,	training loss: 5.289
epoch 1,	batch   200,	training loss: 5.237
epoch 1,	batch   210,	training loss: 5.252
epoch 1,	batch   220,	training loss: 5.116
epoch 1,	batch   230,	training loss: 5.069
epoch 1,	batch   240,	training loss: 5.231
epoch 1,	batch   250,	training loss: 5.150
epoch 1,	batch   260,	training loss: 5.225
epoch 1,	batch   270,	training loss: 5.077
epoch 1,	batch   280,	training loss: 5.071
epoch 1,	batch   290,	training loss: 5.078
epoch 1,	batch   300,	training loss: 4.966
epoch 1,	batch   310,	training loss: 5.087
epoch 1,	batch   320,	training loss: 4.998
epoch 1,	batch   330,	training loss: 5.025
epoch 1,	batch   340,	training loss: 5.036
epoch 1,	batch   350,	training loss: 4.959
epoch 1,	batch   360,	training loss: 4.956
epoch 1,	batch   370,	training loss: 4.969
epoch 1,	batch   380,	training loss: 5.023
epoch 1,	batch   390,	training loss: 5.119
epoch 1,	batch   400,	training loss: 5.079
epoch 1,	batch   410,	training loss: 5.100
epoch 1,	batch   420,	training loss: 4.907
epoch 1,	batch   430,	training loss: 4.856
epoch 1,	batch   440,	training loss: 4.935
epoch 1,	batch   450,	training loss: 4.789
epoch 1,	batch   460,	training loss: 4.945
epoch 1,	batch   470,	training loss: 4.912
epoch 1,	batch   480,	training loss: 4.761
epoch 1,	batch   490,	training loss: 4.990
epoch 1,	batch   500,	training loss: 5.046
epoch 1,	batch   510,	training loss: 4.897
epoch 1,	batch   520,	training loss: 4.868
epoch 1,	batch   530,	training loss: 4.800
epoch 1,	batch   540,	training loss: 5.056
epoch 1,	batch   550,	training loss: 4.854
epoch 1,	batch   560,	training loss: 4.893
epoch 1,	batch   570,	training loss: 4.974
epoch 1,	batch   580,	training loss: 4.813
epoch 1,	batch   590,	training loss: 4.790
epoch 1,	batch   600,	training loss: 4.726
epoch 1,	batch   610,	training loss: 4.908
epoch 1,	batch   620,	training loss: 4.757
epoch 1,	batch   630,	training loss: 4.831
epoch 1,	batch   640,	training loss: 4.821
epoch 1,	batch   650,	training loss: 4.896
epoch 1,	batch   660,	training loss: 4.859
epoch 1,	batch   670,	training loss: 4.866
epoch 1,	batch   680,	training loss: 4.631
epoch 1,	batch   690,	training loss: 4.868
epoch 1,	batch   700,	training loss: 4.860
epoch 1,	batch   710,	training loss: 4.820
epoch 1,	batch   720,	training loss: 4.818
epoch 1,	batch   730,	training loss: 4.842
epoch 1,	batch   740,	training loss: 4.831
epoch 1,	batch   750,	training loss: 4.953
epoch 1,	batch   760,	training loss: 4.724
epoch 1,	batch   770,	training loss: 4.755
epoch 1,	batch   780,	training loss: 5.013
epoch 1,	batch   790,	training loss: 4.830
epoch 1,	batch   800,	training loss: 4.767
epoch 1,	batch   810,	training loss: 4.739
epoch 1,	batch   820,	training loss: 4.924
epoch 1,	batch   830,	training loss: 4.752
epoch 1,	batch   840,	training loss: 4.746
epoch 1,	batch   850,	training loss: 4.669
epoch 1,	batch   860,	training loss: 4.854
epoch 1,	batch   870,	training loss: 4.648
epoch 1,	batch   880,	training loss: 4.869
epoch 1,	batch   890,	training loss: 4.862
epoch 1,	batch   900,	training loss: 4.634
epoch 1,	batch   910,	training loss: 4.903
epoch 1,	batch   920,	training loss: 4.681
epoch 1,	batch   930,	training loss: 4.803
epoch 1,	batch   940,	training loss: 4.659
epoch 1,	batch   950,	training loss: 4.663
epoch 1,	batch   960,	training loss: 4.457
epoch 1,	batch   970,	training loss: 4.430
epoch 1,	batch   980,	training loss: 4.678
epoch 1,	batch   990,	training loss: 4.735
epoch 1,	batch  1000,	training loss: 4.790
epoch 1,	batch  1010,	training loss: 4.771
epoch 1,	batch  1020,	training loss: 4.599
epoch 1,	batch  1030,	training loss: 4.757
epoch 1,	batch  1040,	training loss: 4.632
epoch 1,	batch  1050,	training loss: 4.850
epoch 1,	batch  1060,	training loss: 4.556
epoch 1,	batch  1070,	training loss: 4.783
epoch 1,	batch  1080,	training loss: 4.667
epoch 1,	batch  1090,	training loss: 4.702
epoch 1,	batch  1100,	training loss: 4.602
epoch 1,	batch  1110,	training loss: 4.592
epoch 1,	batch  1120,	training loss: 4.648
epoch 1,	batch  1130,	training loss: 4.532
epoch 1,	batch  1140,	training loss: 4.559
epoch 1,	batch  1150,	training loss: 4.492
epoch 1,	batch  1160,	training loss: 4.674
epoch 1,	batch  1170,	training loss: 4.615
epoch 1,	batch  1180,	training loss: 4.534
epoch 1,	batch  1190,	training loss: 4.590
epoch 1,	batch  1200,	training loss: 4.596
epoch 1,	batch  1210,	training loss: 4.441
epoch 1,	batch  1220,	training loss: 4.449
epoch 1,	batch  1230,	training loss: 4.602
epoch 1,	batch  1240,	training loss: 4.473
epoch 1,	batch  1250,	training loss: 4.506
epoch 1,	batch  1260,	training loss: 4.260
epoch 1,	batch  1270,	training loss: 4.439
epoch 1,	batch  1280,	training loss: 4.374
epoch 1,	batch  1290,	training loss: 4.495
epoch 1,	batch  1300,	training loss: 4.435
epoch 1,	batch  1310,	training loss: 4.389
epoch 1,	batch  1320,	training loss: 4.469
epoch 1,	batch  1330,	training loss: 4.472
epoch 1,	batch  1340,	training loss: 4.394
epoch 1,	batch  1350,	training loss: 4.437
epoch 1,	batch  1360,	training loss: 4.433
epoch 1,	batch  1370,	training loss: 4.472
epoch 1,	batch  1380,	training loss: 4.414
epoch 1,	batch  1390,	training loss: 4.400
epoch 1,	batch  1400,	training loss: 4.512
epoch 1,	batch  1410,	training loss: 4.277
epoch 1,	batch  1420,	training loss: 4.523
epoch 1,	batch  1430,	training loss: 4.345
epoch 1,	batch  1440,	training loss: 4.263
epoch 1,	batch  1450,	training loss: 4.338
epoch 1,	batch  1460,	training loss: 4.380
epoch 1,	batch  1470,	training loss: 4.287
epoch 1,	batch  1480,	training loss: 4.434
epoch 1,	batch  1490,	training loss: 4.313
epoch 1,	batch  1500,	training loss: 4.379
epoch 1,	batch  1510,	training loss: 4.352
epoch 1,	batch  1520,	training loss: 4.319
epoch 1,	batch  1530,	training loss: 4.381
epoch 1,	batch  1540,	training loss: 4.441
epoch 1,	batch  1550,	training loss: 4.159
epoch 1,	batch  1560,	training loss: 4.263
epoch 1,	batch  1570,	training loss: 4.151
epoch 1,	batch  1580,	training loss: 4.236
epoch 1,	batch  1590,	training loss: 4.331
epoch 1,	batch  1600,	training loss: 4.363
epoch 1,	batch  1610,	training loss: 4.349
epoch 1,	batch  1620,	training loss: 4.158
epoch 1,	batch  1630,	training loss: 4.258
epoch 1,	batch  1640,	training loss: 4.185
epoch 1,	batch  1650,	training loss: 4.242
epoch 1,	batch  1660,	training loss: 4.080
epoch 1,	batch  1670,	training loss: 4.323
epoch 1,	batch  1680,	training loss: 4.213
epoch 1,	batch  1690,	training loss: 4.289
epoch 1,	batch  1700,	training loss: 4.290
epoch 1,	batch  1710,	training loss: 4.250
epoch 1,	batch  1720,	training loss: 4.253
epoch 1,	batch  1730,	training loss: 3.938
epoch 1,	batch  1740,	training loss: 4.273
epoch 1,	batch  1750,	training loss: 4.184
epoch 1,	batch  1760,	training loss: 4.121
epoch 1,	batch  1770,	training loss: 4.165
epoch 1,	batch  1780,	training loss: 4.034
epoch 1,	batch  1790,	training loss: 4.003
epoch 1,	batch  1800,	training loss: 4.132
epoch 1,	batch  1810,	training loss: 4.140
epoch 1,	batch  1820,	training loss: 3.998
epoch 1,	batch  1830,	training loss: 3.981
epoch 1,	batch  1840,	training loss: 4.097
epoch 1,	batch  1850,	training loss: 4.010
epoch 1,	batch  1860,	training loss: 4.031
epoch 1,	batch  1870,	training loss: 4.207
epoch 1,	batch  1880,	training loss: 4.247
epoch 1,	batch  1890,	training loss: 4.092
epoch 1,	batch  1900,	training loss: 4.202
epoch 1,	batch  1910,	training loss: 4.069
epoch 1,	batch  1920,	training loss: 3.936
epoch 1,	batch  1930,	training loss: 4.058
epoch 1,	batch  1940,	training loss: 3.925
epoch 1,	batch  1950,	training loss: 3.934
epoch 1,	batch  1960,	training loss: 3.960
epoch 1,	batch  1970,	training loss: 4.070
epoch 1,	batch  1980,	training loss: 4.035
END OF EPOCH 1
Testing on validation set...
# correct:	5669/54904 = 10.325295060469182%
# off by 1:	10628/54904 = 19.357423867113507%

epoch 2,	batch    10,	training loss: 3.970
epoch 2,	batch    20,	training loss: 3.918
epoch 2,	batch    30,	training loss: 3.946
epoch 2,	batch    40,	training loss: 3.910
epoch 2,	batch    50,	training loss: 3.934
epoch 2,	batch    60,	training loss: 3.883
epoch 2,	batch    70,	training loss: 3.823
epoch 2,	batch    80,	training loss: 3.928
epoch 2,	batch    90,	training loss: 3.893
epoch 2,	batch   100,	training loss: 3.874
epoch 2,	batch   110,	training loss: 3.859
epoch 2,	batch   120,	training loss: 3.769
epoch 2,	batch   130,	training loss: 3.858
epoch 2,	batch   140,	training loss: 4.040
epoch 2,	batch   150,	training loss: 3.944
epoch 2,	batch   160,	training loss: 3.875
epoch 2,	batch   170,	training loss: 3.761
epoch 2,	batch   180,	training loss: 3.894
epoch 2,	batch   190,	training loss: 3.795
epoch 2,	batch   200,	training loss: 3.966
epoch 2,	batch   210,	training loss: 3.847
epoch 2,	batch   220,	training loss: 3.868
epoch 2,	batch   230,	training loss: 3.881
epoch 2,	batch   240,	training loss: 3.817
epoch 2,	batch   250,	training loss: 3.754
epoch 2,	batch   260,	training loss: 3.938
epoch 2,	batch   270,	training loss: 3.848
epoch 2,	batch   280,	training loss: 3.742
epoch 2,	batch   290,	training loss: 3.767
epoch 2,	batch   300,	training loss: 3.878
epoch 2,	batch   310,	training loss: 3.908
epoch 2,	batch   320,	training loss: 3.781
epoch 2,	batch   330,	training loss: 3.765
epoch 2,	batch   340,	training loss: 3.755
epoch 2,	batch   350,	training loss: 3.840
epoch 2,	batch   360,	training loss: 3.840
epoch 2,	batch   370,	training loss: 3.925
epoch 2,	batch   380,	training loss: 3.911
epoch 2,	batch   390,	training loss: 3.837
epoch 2,	batch   400,	training loss: 3.890
epoch 2,	batch   410,	training loss: 3.802
epoch 2,	batch   420,	training loss: 3.800
epoch 2,	batch   430,	training loss: 3.688
epoch 2,	batch   440,	training loss: 3.834
epoch 2,	batch   450,	training loss: 3.794
epoch 2,	batch   460,	training loss: 3.630
epoch 2,	batch   470,	training loss: 3.602
epoch 2,	batch   480,	training loss: 3.859
epoch 2,	batch   490,	training loss: 3.715
epoch 2,	batch   500,	training loss: 3.879
epoch 2,	batch   510,	training loss: 3.820
epoch 2,	batch   520,	training loss: 3.511
epoch 2,	batch   530,	training loss: 3.771
epoch 2,	batch   540,	training loss: 3.879
epoch 2,	batch   550,	training loss: 3.769
epoch 2,	batch   560,	training loss: 3.648
epoch 2,	batch   570,	training loss: 3.805
epoch 2,	batch   580,	training loss: 3.702
epoch 2,	batch   590,	training loss: 3.621
epoch 2,	batch   600,	training loss: 3.805
epoch 2,	batch   610,	training loss: 3.616
epoch 2,	batch   620,	training loss: 3.713
epoch 2,	batch   630,	training loss: 3.729
epoch 2,	batch   640,	training loss: 3.545
epoch 2,	batch   650,	training loss: 3.715
epoch 2,	batch   660,	training loss: 3.808
epoch 2,	batch   670,	training loss: 3.549
epoch 2,	batch   680,	training loss: 3.690
epoch 2,	batch   690,	training loss: 3.708
epoch 2,	batch   700,	training loss: 3.710
epoch 2,	batch   710,	training loss: 3.617
epoch 2,	batch   720,	training loss: 3.723
epoch 2,	batch   730,	training loss: 3.623
epoch 2,	batch   740,	training loss: 3.651
epoch 2,	batch   750,	training loss: 3.714
epoch 2,	batch   760,	training loss: 3.710
epoch 2,	batch   770,	training loss: 3.703
epoch 2,	batch   780,	training loss: 3.869
epoch 2,	batch   790,	training loss: 3.703
epoch 2,	batch   800,	training loss: 3.641
epoch 2,	batch   810,	training loss: 3.920
epoch 2,	batch   820,	training loss: 3.735
epoch 2,	batch   830,	training loss: 3.752
epoch 2,	batch   840,	training loss: 3.735
epoch 2,	batch   850,	training loss: 3.849
epoch 2,	batch   860,	training loss: 3.946
epoch 2,	batch   870,	training loss: 3.662
epoch 2,	batch   880,	training loss: 3.689
epoch 2,	batch   890,	training loss: 3.761
epoch 2,	batch   900,	training loss: 3.738
epoch 2,	batch   910,	training loss: 3.699
epoch 2,	batch   920,	training loss: 3.616
epoch 2,	batch   930,	training loss: 3.808
epoch 2,	batch   940,	training loss: 3.769
epoch 2,	batch   950,	training loss: 3.648
epoch 2,	batch   960,	training loss: 3.712
epoch 2,	batch   970,	training loss: 3.502
epoch 2,	batch   980,	training loss: 3.552
epoch 2,	batch   990,	training loss: 3.697
epoch 2,	batch  1000,	training loss: 3.604
epoch 2,	batch  1010,	training loss: 3.633
epoch 2,	batch  1020,	training loss: 3.578
epoch 2,	batch  1030,	training loss: 3.739
epoch 2,	batch  1040,	training loss: 3.775
epoch 2,	batch  1050,	training loss: 3.635
epoch 2,	batch  1060,	training loss: 3.435
epoch 2,	batch  1070,	training loss: 3.531
epoch 2,	batch  1080,	training loss: 3.654
epoch 2,	batch  1090,	training loss: 3.593
epoch 2,	batch  1100,	training loss: 3.631
epoch 2,	batch  1110,	training loss: 3.449
epoch 2,	batch  1120,	training loss: 3.710
epoch 2,	batch  1130,	training loss: 3.716
epoch 2,	batch  1140,	training loss: 3.675
epoch 2,	batch  1150,	training loss: 3.660
epoch 2,	batch  1160,	training loss: 3.654
epoch 2,	batch  1170,	training loss: 3.773
epoch 2,	batch  1180,	training loss: 3.653
epoch 2,	batch  1190,	training loss: 3.624
epoch 2,	batch  1200,	training loss: 3.721
epoch 2,	batch  1210,	training loss: 3.565
epoch 2,	batch  1220,	training loss: 3.553
epoch 2,	batch  1230,	training loss: 3.590
epoch 2,	batch  1240,	training loss: 3.590
epoch 2,	batch  1250,	training loss: 3.764
epoch 2,	batch  1260,	training loss: 3.708
epoch 2,	batch  1270,	training loss: 3.619
epoch 2,	batch  1280,	training loss: 3.548
epoch 2,	batch  1290,	training loss: 3.581
epoch 2,	batch  1300,	training loss: 3.620
epoch 2,	batch  1310,	training loss: 3.578
epoch 2,	batch  1320,	training loss: 3.373
epoch 2,	batch  1330,	training loss: 3.809
epoch 2,	batch  1340,	training loss: 3.576
epoch 2,	batch  1350,	training loss: 3.632
epoch 2,	batch  1360,	training loss: 3.525
epoch 2,	batch  1370,	training loss: 3.455
epoch 2,	batch  1380,	training loss: 3.527
epoch 2,	batch  1390,	training loss: 3.634
epoch 2,	batch  1400,	training loss: 3.642
epoch 2,	batch  1410,	training loss: 3.579
epoch 2,	batch  1420,	training loss: 3.578
epoch 2,	batch  1430,	training loss: 3.579
epoch 2,	batch  1440,	training loss: 3.721
epoch 2,	batch  1450,	training loss: 3.623
epoch 2,	batch  1460,	training loss: 3.635
epoch 2,	batch  1470,	training loss: 3.636
epoch 2,	batch  1480,	training loss: 3.636
epoch 2,	batch  1490,	training loss: 3.638
epoch 2,	batch  1500,	training loss: 3.514
epoch 2,	batch  1510,	training loss: 3.554
epoch 2,	batch  1520,	training loss: 3.619
epoch 2,	batch  1530,	training loss: 3.420
epoch 2,	batch  1540,	training loss: 3.646
epoch 2,	batch  1550,	training loss: 3.724
epoch 2,	batch  1560,	training loss: 3.604
epoch 2,	batch  1570,	training loss: 3.505
epoch 2,	batch  1580,	training loss: 3.370
epoch 2,	batch  1590,	training loss: 3.508
epoch 2,	batch  1600,	training loss: 3.376
epoch 2,	batch  1610,	training loss: 3.543
epoch 2,	batch  1620,	training loss: 3.700
epoch 2,	batch  1630,	training loss: 3.644
epoch 2,	batch  1640,	training loss: 3.607
epoch 2,	batch  1650,	training loss: 3.558
epoch 2,	batch  1660,	training loss: 3.575
epoch 2,	batch  1670,	training loss: 3.519
epoch 2,	batch  1680,	training loss: 3.625
epoch 2,	batch  1690,	training loss: 3.763
epoch 2,	batch  1700,	training loss: 3.663
epoch 2,	batch  1710,	training loss: 3.412
epoch 2,	batch  1720,	training loss: 3.584
epoch 2,	batch  1730,	training loss: 3.476
epoch 2,	batch  1740,	training loss: 3.494
epoch 2,	batch  1750,	training loss: 3.351
epoch 2,	batch  1760,	training loss: 3.597
epoch 2,	batch  1770,	training loss: 3.519
epoch 2,	batch  1780,	training loss: 3.587
epoch 2,	batch  1790,	training loss: 3.507
epoch 2,	batch  1800,	training loss: 3.509
epoch 2,	batch  1810,	training loss: 3.643
epoch 2,	batch  1820,	training loss: 3.648
epoch 2,	batch  1830,	training loss: 3.638
epoch 2,	batch  1840,	training loss: 3.387
epoch 2,	batch  1850,	training loss: 3.412
epoch 2,	batch  1860,	training loss: 3.685
epoch 2,	batch  1870,	training loss: 3.483
epoch 2,	batch  1880,	training loss: 3.532
epoch 2,	batch  1890,	training loss: 3.603
epoch 2,	batch  1900,	training loss: 3.536
epoch 2,	batch  1910,	training loss: 3.528
epoch 2,	batch  1920,	training loss: 3.331
epoch 2,	batch  1930,	training loss: 3.512
epoch 2,	batch  1940,	training loss: 3.685
epoch 2,	batch  1950,	training loss: 3.474
epoch 2,	batch  1960,	training loss: 3.312
epoch 2,	batch  1970,	training loss: 3.647
epoch 2,	batch  1980,	training loss: 3.501
END OF EPOCH 2
Testing on validation set...
# correct:	6102/54904 = 11.113944339210258%
# off by 1:	11798/54904 = 21.4884161445432%

epoch 3,	batch    10,	training loss: 3.276
epoch 3,	batch    20,	training loss: 3.326
epoch 3,	batch    30,	training loss: 3.393
epoch 3,	batch    40,	training loss: 3.233
epoch 3,	batch    50,	training loss: 3.231
epoch 3,	batch    60,	training loss: 3.292
epoch 3,	batch    70,	training loss: 3.137
epoch 3,	batch    80,	training loss: 3.130
epoch 3,	batch    90,	training loss: 3.155
epoch 3,	batch   100,	training loss: 3.316
epoch 3,	batch   110,	training loss: 3.180
epoch 3,	batch   120,	training loss: 3.257
epoch 3,	batch   130,	training loss: 3.160
epoch 3,	batch   140,	training loss: 3.103
epoch 3,	batch   150,	training loss: 3.238
epoch 3,	batch   160,	training loss: 3.152
epoch 3,	batch   170,	training loss: 3.352
epoch 3,	batch   180,	training loss: 3.096
epoch 3,	batch   190,	training loss: 3.246
epoch 3,	batch   200,	training loss: 3.106
epoch 3,	batch   210,	training loss: 3.199
epoch 3,	batch   220,	training loss: 3.287
epoch 3,	batch   230,	training loss: 3.177
epoch 3,	batch   240,	training loss: 3.249
epoch 3,	batch   250,	training loss: 3.249
epoch 3,	batch   260,	training loss: 3.261
epoch 3,	batch   270,	training loss: 3.102
epoch 3,	batch   280,	training loss: 3.184
epoch 3,	batch   290,	training loss: 3.096
epoch 3,	batch   300,	training loss: 3.305
epoch 3,	batch   310,	training loss: 3.322
epoch 3,	batch   320,	training loss: 3.313
epoch 3,	batch   330,	training loss: 3.185
epoch 3,	batch   340,	training loss: 3.245
epoch 3,	batch   350,	training loss: 3.204
epoch 3,	batch   360,	training loss: 3.249
epoch 3,	batch   370,	training loss: 3.202
epoch 3,	batch   380,	training loss: 3.215
epoch 3,	batch   390,	training loss: 3.122
epoch 3,	batch   400,	training loss: 3.219
epoch 3,	batch   410,	training loss: 3.188
epoch 3,	batch   420,	training loss: 3.279
epoch 3,	batch   430,	training loss: 3.076
epoch 3,	batch   440,	training loss: 3.220
epoch 3,	batch   450,	training loss: 3.191
epoch 3,	batch   460,	training loss: 3.356
epoch 3,	batch   470,	training loss: 3.095
epoch 3,	batch   480,	training loss: 3.293
epoch 3,	batch   490,	training loss: 3.159
epoch 3,	batch   500,	training loss: 3.268
epoch 3,	batch   510,	training loss: 3.100
epoch 3,	batch   520,	training loss: 3.237
epoch 3,	batch   530,	training loss: 3.027
epoch 3,	batch   540,	training loss: 3.329
epoch 3,	batch   550,	training loss: 3.345
epoch 3,	batch   560,	training loss: 3.255
epoch 3,	batch   570,	training loss: 3.233
epoch 3,	batch   580,	training loss: 3.144
epoch 3,	batch   590,	training loss: 3.375
epoch 3,	batch   600,	training loss: 3.218
epoch 3,	batch   610,	training loss: 3.160
epoch 3,	batch   620,	training loss: 3.223
epoch 3,	batch   630,	training loss: 3.147
epoch 3,	batch   640,	training loss: 3.100
epoch 3,	batch   650,	training loss: 3.120
epoch 3,	batch   660,	training loss: 3.121
epoch 3,	batch   670,	training loss: 3.499
epoch 3,	batch   680,	training loss: 3.098
epoch 3,	batch   690,	training loss: 3.285
epoch 3,	batch   700,	training loss: 3.107
epoch 3,	batch   710,	training loss: 3.229
epoch 3,	batch   720,	training loss: 3.130
epoch 3,	batch   730,	training loss: 3.126
epoch 3,	batch   740,	training loss: 3.088
epoch 3,	batch   750,	training loss: 3.288
epoch 3,	batch   760,	training loss: 3.184
epoch 3,	batch   770,	training loss: 3.220
epoch 3,	batch   780,	training loss: 3.266
epoch 3,	batch   790,	training loss: 3.212
epoch 3,	batch   800,	training loss: 3.081
epoch 3,	batch   810,	training loss: 3.095
epoch 3,	batch   820,	training loss: 3.258
epoch 3,	batch   830,	training loss: 3.225
epoch 3,	batch   840,	training loss: 3.188
epoch 3,	batch   850,	training loss: 3.206
epoch 3,	batch   860,	training loss: 3.368
epoch 3,	batch   870,	training loss: 3.172
epoch 3,	batch   880,	training loss: 3.309
epoch 3,	batch   890,	training loss: 3.287
epoch 3,	batch   900,	training loss: 3.106
epoch 3,	batch   910,	training loss: 3.220
epoch 3,	batch   920,	training loss: 3.154
epoch 3,	batch   930,	training loss: 3.254
epoch 3,	batch   940,	training loss: 3.161
epoch 3,	batch   950,	training loss: 3.343
epoch 3,	batch   960,	training loss: 3.362
epoch 3,	batch   970,	training loss: 3.148
epoch 3,	batch   980,	training loss: 3.120
epoch 3,	batch   990,	training loss: 3.203
epoch 3,	batch  1000,	training loss: 3.091
epoch 3,	batch  1010,	training loss: 3.386
epoch 3,	batch  1020,	training loss: 3.078
epoch 3,	batch  1030,	training loss: 3.229
epoch 3,	batch  1040,	training loss: 3.274
epoch 3,	batch  1050,	training loss: 3.240
epoch 3,	batch  1060,	training loss: 3.081
epoch 3,	batch  1070,	training loss: 3.224
epoch 3,	batch  1080,	training loss: 3.225
epoch 3,	batch  1090,	training loss: 3.314
epoch 3,	batch  1100,	training loss: 3.045
epoch 3,	batch  1110,	training loss: 3.161
epoch 3,	batch  1120,	training loss: 3.143
epoch 3,	batch  1130,	training loss: 3.338
epoch 3,	batch  1140,	training loss: 3.021
epoch 3,	batch  1150,	training loss: 3.120
epoch 3,	batch  1160,	training loss: 3.291
epoch 3,	batch  1170,	training loss: 3.185
epoch 3,	batch  1180,	training loss: 3.156
epoch 3,	batch  1190,	training loss: 3.260
epoch 3,	batch  1200,	training loss: 3.023
epoch 3,	batch  1210,	training loss: 2.988
epoch 3,	batch  1220,	training loss: 3.273
epoch 3,	batch  1230,	training loss: 3.144
epoch 3,	batch  1240,	training loss: 3.116
epoch 3,	batch  1250,	training loss: 3.061
epoch 3,	batch  1260,	training loss: 3.340
epoch 3,	batch  1270,	training loss: 3.385
epoch 3,	batch  1280,	training loss: 3.245
epoch 3,	batch  1290,	training loss: 3.179
epoch 3,	batch  1300,	training loss: 3.256
epoch 3,	batch  1310,	training loss: 3.092
epoch 3,	batch  1320,	training loss: 3.116
epoch 3,	batch  1330,	training loss: 3.214
epoch 3,	batch  1340,	training loss: 3.230
epoch 3,	batch  1350,	training loss: 3.154
epoch 3,	batch  1360,	training loss: 3.176
epoch 3,	batch  1370,	training loss: 3.204
epoch 3,	batch  1380,	training loss: 3.237
epoch 3,	batch  1390,	training loss: 3.130
epoch 3,	batch  1400,	training loss: 3.195
epoch 3,	batch  1410,	training loss: 3.170
epoch 3,	batch  1420,	training loss: 3.283
epoch 3,	batch  1430,	training loss: 3.246
epoch 3,	batch  1440,	training loss: 3.260
epoch 3,	batch  1450,	training loss: 3.047
epoch 3,	batch  1460,	training loss: 3.045
epoch 3,	batch  1470,	training loss: 3.067
epoch 3,	batch  1480,	training loss: 3.314
epoch 3,	batch  1490,	training loss: 3.065
epoch 3,	batch  1500,	training loss: 3.166
epoch 3,	batch  1510,	training loss: 3.157
epoch 3,	batch  1520,	training loss: 3.183
epoch 3,	batch  1530,	training loss: 2.991
epoch 3,	batch  1540,	training loss: 3.203
epoch 3,	batch  1550,	training loss: 3.192
epoch 3,	batch  1560,	training loss: 3.149
epoch 3,	batch  1570,	training loss: 3.081
epoch 3,	batch  1580,	training loss: 3.209
epoch 3,	batch  1590,	training loss: 3.256
epoch 3,	batch  1600,	training loss: 3.052
epoch 3,	batch  1610,	training loss: 3.171
epoch 3,	batch  1620,	training loss: 3.081
epoch 3,	batch  1630,	training loss: 3.310
epoch 3,	batch  1640,	training loss: 3.176
epoch 3,	batch  1650,	training loss: 3.117
epoch 3,	batch  1660,	training loss: 3.236
epoch 3,	batch  1670,	training loss: 3.185
epoch 3,	batch  1680,	training loss: 3.118
epoch 3,	batch  1690,	training loss: 3.026
epoch 3,	batch  1700,	training loss: 3.294
epoch 3,	batch  1710,	training loss: 3.142
epoch 3,	batch  1720,	training loss: 3.177
epoch 3,	batch  1730,	training loss: 3.150
epoch 3,	batch  1740,	training loss: 3.030
epoch 3,	batch  1750,	training loss: 3.109
epoch 3,	batch  1760,	training loss: 3.039
epoch 3,	batch  1770,	training loss: 3.098
epoch 3,	batch  1780,	training loss: 3.263
epoch 3,	batch  1790,	training loss: 3.160
epoch 3,	batch  1800,	training loss: 3.236
epoch 3,	batch  1810,	training loss: 3.031
epoch 3,	batch  1820,	training loss: 3.233
epoch 3,	batch  1830,	training loss: 3.121
epoch 3,	batch  1840,	training loss: 3.156
epoch 3,	batch  1850,	training loss: 3.199
epoch 3,	batch  1860,	training loss: 3.187
epoch 3,	batch  1870,	training loss: 3.024
epoch 3,	batch  1880,	training loss: 3.162
epoch 3,	batch  1890,	training loss: 3.207
epoch 3,	batch  1900,	training loss: 3.081
epoch 3,	batch  1910,	training loss: 3.157
epoch 3,	batch  1920,	training loss: 3.190
epoch 3,	batch  1930,	training loss: 3.154
epoch 3,	batch  1940,	training loss: 3.236
epoch 3,	batch  1950,	training loss: 3.030
epoch 3,	batch  1960,	training loss: 3.041
epoch 3,	batch  1970,	training loss: 3.186
epoch 3,	batch  1980,	training loss: 3.130
END OF EPOCH 3
Testing on validation set...
# correct:	6611/54904 = 12.041017047938219%
# off by 1:	12502/54904 = 22.77065423284278%

epoch 4,	batch    10,	training loss: 2.661
epoch 4,	batch    20,	training loss: 2.586
epoch 4,	batch    30,	training loss: 2.850
epoch 4,	batch    40,	training loss: 2.828
epoch 4,	batch    50,	training loss: 2.618
epoch 4,	batch    60,	training loss: 2.663
epoch 4,	batch    70,	training loss: 2.681
epoch 4,	batch    80,	training loss: 2.776
epoch 4,	batch    90,	training loss: 2.726
epoch 4,	batch   100,	training loss: 2.865
epoch 4,	batch   110,	training loss: 2.882
epoch 4,	batch   120,	training loss: 2.821
epoch 4,	batch   130,	training loss: 2.732
epoch 4,	batch   140,	training loss: 2.813
epoch 4,	batch   150,	training loss: 2.669
epoch 4,	batch   160,	training loss: 2.644
epoch 4,	batch   170,	training loss: 2.628
epoch 4,	batch   180,	training loss: 2.699
epoch 4,	batch   190,	training loss: 2.731
epoch 4,	batch   200,	training loss: 2.857
epoch 4,	batch   210,	training loss: 2.713
epoch 4,	batch   220,	training loss: 2.675
epoch 4,	batch   230,	training loss: 2.643
epoch 4,	batch   240,	training loss: 2.761
epoch 4,	batch   250,	training loss: 2.774
epoch 4,	batch   260,	training loss: 2.727
epoch 4,	batch   270,	training loss: 2.632
epoch 4,	batch   280,	training loss: 2.795
epoch 4,	batch   290,	training loss: 2.696
epoch 4,	batch   300,	training loss: 2.812
epoch 4,	batch   310,	training loss: 2.688
epoch 4,	batch   320,	training loss: 2.846
epoch 4,	batch   330,	training loss: 2.712
epoch 4,	batch   340,	training loss: 2.726
epoch 4,	batch   350,	training loss: 2.767
epoch 4,	batch   360,	training loss: 2.787
epoch 4,	batch   370,	training loss: 2.767
epoch 4,	batch   380,	training loss: 2.718
epoch 4,	batch   390,	training loss: 2.844
epoch 4,	batch   400,	training loss: 2.859
epoch 4,	batch   410,	training loss: 2.906
epoch 4,	batch   420,	training loss: 2.884
epoch 4,	batch   430,	training loss: 2.721
epoch 4,	batch   440,	training loss: 2.777
epoch 4,	batch   450,	training loss: 2.752
epoch 4,	batch   460,	training loss: 2.858
epoch 4,	batch   470,	training loss: 2.781
epoch 4,	batch   480,	training loss: 2.832
epoch 4,	batch   490,	training loss: 2.720
epoch 4,	batch   500,	training loss: 2.848
epoch 4,	batch   510,	training loss: 2.783
epoch 4,	batch   520,	training loss: 2.768
epoch 4,	batch   530,	training loss: 2.782
epoch 4,	batch   540,	training loss: 2.716
epoch 4,	batch   550,	training loss: 2.870
epoch 4,	batch   560,	training loss: 2.817
epoch 4,	batch   570,	training loss: 2.611
epoch 4,	batch   580,	training loss: 2.818
epoch 4,	batch   590,	training loss: 2.795
epoch 4,	batch   600,	training loss: 2.917
epoch 4,	batch   610,	training loss: 2.718
epoch 4,	batch   620,	training loss: 2.837
epoch 4,	batch   630,	training loss: 2.768
epoch 4,	batch   640,	training loss: 2.796
epoch 4,	batch   650,	training loss: 2.789
epoch 4,	batch   660,	training loss: 2.793
epoch 4,	batch   670,	training loss: 2.850
epoch 4,	batch   680,	training loss: 2.754
epoch 4,	batch   690,	training loss: 2.913
epoch 4,	batch   700,	training loss: 2.888
epoch 4,	batch   710,	training loss: 2.891
epoch 4,	batch   720,	training loss: 2.830
epoch 4,	batch   730,	training loss: 2.805
epoch 4,	batch   740,	training loss: 2.764
epoch 4,	batch   750,	training loss: 2.791
epoch 4,	batch   760,	training loss: 2.782
epoch 4,	batch   770,	training loss: 2.621
epoch 4,	batch   780,	training loss: 2.716
epoch 4,	batch   790,	training loss: 2.718
epoch 4,	batch   800,	training loss: 2.754
epoch 4,	batch   810,	training loss: 2.695
epoch 4,	batch   820,	training loss: 2.772
epoch 4,	batch   830,	training loss: 2.797
epoch 4,	batch   840,	training loss: 2.719
epoch 4,	batch   850,	training loss: 2.848
epoch 4,	batch   860,	training loss: 2.834
epoch 4,	batch   870,	training loss: 2.848
epoch 4,	batch   880,	training loss: 2.788
epoch 4,	batch   890,	training loss: 2.774
epoch 4,	batch   900,	training loss: 2.920
epoch 4,	batch   910,	training loss: 2.835
epoch 4,	batch   920,	training loss: 2.772
epoch 4,	batch   930,	training loss: 2.934
epoch 4,	batch   940,	training loss: 2.778
epoch 4,	batch   950,	training loss: 2.722
epoch 4,	batch   960,	training loss: 2.694
epoch 4,	batch   970,	training loss: 2.795
epoch 4,	batch   980,	training loss: 2.702
epoch 4,	batch   990,	training loss: 2.751
epoch 4,	batch  1000,	training loss: 2.789
epoch 4,	batch  1010,	training loss: 2.794
epoch 4,	batch  1020,	training loss: 2.771
epoch 4,	batch  1030,	training loss: 2.737
epoch 4,	batch  1040,	training loss: 2.817
epoch 4,	batch  1050,	training loss: 2.747
epoch 4,	batch  1060,	training loss: 2.832
epoch 4,	batch  1070,	training loss: 2.807
epoch 4,	batch  1080,	training loss: 2.836
epoch 4,	batch  1090,	training loss: 2.669
epoch 4,	batch  1100,	training loss: 2.796
epoch 4,	batch  1110,	training loss: 2.896
epoch 4,	batch  1120,	training loss: 2.743
epoch 4,	batch  1130,	training loss: 2.775
epoch 4,	batch  1140,	training loss: 2.681
epoch 4,	batch  1150,	training loss: 2.755
epoch 4,	batch  1160,	training loss: 2.861
epoch 4,	batch  1170,	training loss: 2.779
epoch 4,	batch  1180,	training loss: 2.855
epoch 4,	batch  1190,	training loss: 2.648
epoch 4,	batch  1200,	training loss: 2.835
epoch 4,	batch  1210,	training loss: 2.795
epoch 4,	batch  1220,	training loss: 2.785
epoch 4,	batch  1230,	training loss: 2.670
epoch 4,	batch  1240,	training loss: 2.794
epoch 4,	batch  1250,	training loss: 2.756
epoch 4,	batch  1260,	training loss: 2.841
epoch 4,	batch  1270,	training loss: 2.581
epoch 4,	batch  1280,	training loss: 2.834
epoch 4,	batch  1290,	training loss: 2.715
epoch 4,	batch  1300,	training loss: 2.958
epoch 4,	batch  1310,	training loss: 2.778
epoch 4,	batch  1320,	training loss: 2.937
epoch 4,	batch  1330,	training loss: 2.892
epoch 4,	batch  1340,	training loss: 2.809
epoch 4,	batch  1350,	training loss: 2.708
epoch 4,	batch  1360,	training loss: 2.581
epoch 4,	batch  1370,	training loss: 2.733
epoch 4,	batch  1380,	training loss: 2.961
epoch 4,	batch  1390,	training loss: 2.888
epoch 4,	batch  1400,	training loss: 2.797
epoch 4,	batch  1410,	training loss: 2.829
epoch 4,	batch  1420,	training loss: 2.939
epoch 4,	batch  1430,	training loss: 2.887
epoch 4,	batch  1440,	training loss: 2.816
epoch 4,	batch  1450,	training loss: 2.813
epoch 4,	batch  1460,	training loss: 2.746
epoch 4,	batch  1470,	training loss: 2.833
epoch 4,	batch  1480,	training loss: 2.855
epoch 4,	batch  1490,	training loss: 2.806
epoch 4,	batch  1500,	training loss: 2.789
epoch 4,	batch  1510,	training loss: 2.843
epoch 4,	batch  1520,	training loss: 2.821
epoch 4,	batch  1530,	training loss: 2.742
epoch 4,	batch  1540,	training loss: 2.921
epoch 4,	batch  1550,	training loss: 2.748
epoch 4,	batch  1560,	training loss: 2.859
epoch 4,	batch  1570,	training loss: 2.822
epoch 4,	batch  1580,	training loss: 2.953
epoch 4,	batch  1590,	training loss: 2.748
epoch 4,	batch  1600,	training loss: 2.798
epoch 4,	batch  1610,	training loss: 2.756
epoch 4,	batch  1620,	training loss: 2.837
epoch 4,	batch  1630,	training loss: 2.883
epoch 4,	batch  1640,	training loss: 2.846
epoch 4,	batch  1650,	training loss: 2.867
epoch 4,	batch  1660,	training loss: 2.903
epoch 4,	batch  1670,	training loss: 2.766
epoch 4,	batch  1680,	training loss: 2.737
epoch 4,	batch  1690,	training loss: 2.941
epoch 4,	batch  1700,	training loss: 2.805
epoch 4,	batch  1710,	training loss: 2.892
epoch 4,	batch  1720,	training loss: 2.767
epoch 4,	batch  1730,	training loss: 2.830
epoch 4,	batch  1740,	training loss: 2.753
epoch 4,	batch  1750,	training loss: 2.871
epoch 4,	batch  1760,	training loss: 2.791
epoch 4,	batch  1770,	training loss: 2.746
epoch 4,	batch  1780,	training loss: 2.695
epoch 4,	batch  1790,	training loss: 2.728
epoch 4,	batch  1800,	training loss: 2.832
epoch 4,	batch  1810,	training loss: 2.679
epoch 4,	batch  1820,	training loss: 2.701
epoch 4,	batch  1830,	training loss: 2.837
epoch 4,	batch  1840,	training loss: 2.945
epoch 4,	batch  1850,	training loss: 2.891
epoch 4,	batch  1860,	training loss: 2.876
epoch 4,	batch  1870,	training loss: 2.819
epoch 4,	batch  1880,	training loss: 2.885
epoch 4,	batch  1890,	training loss: 2.894
epoch 4,	batch  1900,	training loss: 2.876
epoch 4,	batch  1910,	training loss: 2.787
epoch 4,	batch  1920,	training loss: 2.959
epoch 4,	batch  1930,	training loss: 2.888
epoch 4,	batch  1940,	training loss: 2.842
epoch 4,	batch  1950,	training loss: 2.887
epoch 4,	batch  1960,	training loss: 2.855
epoch 4,	batch  1970,	training loss: 2.779
epoch 4,	batch  1980,	training loss: 2.878
END OF EPOCH 4
Testing on validation set...
# correct:	6869/54904 = 12.51092816552528%
# off by 1:	12651/54904 = 23.042037010053914%

epoch 5,	batch    10,	training loss: 2.364
epoch 5,	batch    20,	training loss: 2.422
epoch 5,	batch    30,	training loss: 2.454
epoch 5,	batch    40,	training loss: 2.204
epoch 5,	batch    50,	training loss: 2.360
epoch 5,	batch    60,	training loss: 2.331
epoch 5,	batch    70,	training loss: 2.284
epoch 5,	batch    80,	training loss: 2.357
epoch 5,	batch    90,	training loss: 2.328
epoch 5,	batch   100,	training loss: 2.222
epoch 5,	batch   110,	training loss: 2.266
epoch 5,	batch   120,	training loss: 2.328
epoch 5,	batch   130,	training loss: 2.397
epoch 5,	batch   140,	training loss: 2.319
epoch 5,	batch   150,	training loss: 2.476
epoch 5,	batch   160,	training loss: 2.362
epoch 5,	batch   170,	training loss: 2.398
epoch 5,	batch   180,	training loss: 2.404
epoch 5,	batch   190,	training loss: 2.413
epoch 5,	batch   200,	training loss: 2.252
epoch 5,	batch   210,	training loss: 2.301
epoch 5,	batch   220,	training loss: 2.369
epoch 5,	batch   230,	training loss: 2.411
epoch 5,	batch   240,	training loss: 2.507
epoch 5,	batch   250,	training loss: 2.315
epoch 5,	batch   260,	training loss: 2.509
epoch 5,	batch   270,	training loss: 2.318
epoch 5,	batch   280,	training loss: 2.407
epoch 5,	batch   290,	training loss: 2.321
epoch 5,	batch   300,	training loss: 2.365
epoch 5,	batch   310,	training loss: 2.314
epoch 5,	batch   320,	training loss: 2.326
epoch 5,	batch   330,	training loss: 2.355
epoch 5,	batch   340,	training loss: 2.512
epoch 5,	batch   350,	training loss: 2.424
epoch 5,	batch   360,	training loss: 2.393
epoch 5,	batch   370,	training loss: 2.185
epoch 5,	batch   380,	training loss: 2.305
epoch 5,	batch   390,	training loss: 2.361
epoch 5,	batch   400,	training loss: 2.383
epoch 5,	batch   410,	training loss: 2.262
epoch 5,	batch   420,	training loss: 2.484
epoch 5,	batch   430,	training loss: 2.573
epoch 5,	batch   440,	training loss: 2.481
epoch 5,	batch   450,	training loss: 2.320
epoch 5,	batch   460,	training loss: 2.423
epoch 5,	batch   470,	training loss: 2.303
epoch 5,	batch   480,	training loss: 2.264
epoch 5,	batch   490,	training loss: 2.311
epoch 5,	batch   500,	training loss: 2.345
epoch 5,	batch   510,	training loss: 2.284
epoch 5,	batch   520,	training loss: 2.416
epoch 5,	batch   530,	training loss: 2.350
epoch 5,	batch   540,	training loss: 2.481
epoch 5,	batch   550,	training loss: 2.398
epoch 5,	batch   560,	training loss: 2.319
epoch 5,	batch   570,	training loss: 2.371
epoch 5,	batch   580,	training loss: 2.318
epoch 5,	batch   590,	training loss: 2.331
epoch 5,	batch   600,	training loss: 2.425
epoch 5,	batch   610,	training loss: 2.487
epoch 5,	batch   620,	training loss: 2.409
epoch 5,	batch   630,	training loss: 2.482
epoch 5,	batch   640,	training loss: 2.463
epoch 5,	batch   650,	training loss: 2.483
epoch 5,	batch   660,	training loss: 2.343
epoch 5,	batch   670,	training loss: 2.426
epoch 5,	batch   680,	training loss: 2.362
epoch 5,	batch   690,	training loss: 2.300
epoch 5,	batch   700,	training loss: 2.515
epoch 5,	batch   710,	training loss: 2.478
epoch 5,	batch   720,	training loss: 2.464
epoch 5,	batch   730,	training loss: 2.454
epoch 5,	batch   740,	training loss: 2.317
epoch 5,	batch   750,	training loss: 2.404
epoch 5,	batch   760,	training loss: 2.351
epoch 5,	batch   770,	training loss: 2.338
epoch 5,	batch   780,	training loss: 2.440
epoch 5,	batch   790,	training loss: 2.348
epoch 5,	batch   800,	training loss: 2.367
epoch 5,	batch   810,	training loss: 2.379
epoch 5,	batch   820,	training loss: 2.324
epoch 5,	batch   830,	training loss: 2.359
epoch 5,	batch   840,	training loss: 2.368
epoch 5,	batch   850,	training loss: 2.369
epoch 5,	batch   860,	training loss: 2.517
epoch 5,	batch   870,	training loss: 2.412
epoch 5,	batch   880,	training loss: 2.477
epoch 5,	batch   890,	training loss: 2.443
epoch 5,	batch   900,	training loss: 2.462
epoch 5,	batch   910,	training loss: 2.365
epoch 5,	batch   920,	training loss: 2.424
epoch 5,	batch   930,	training loss: 2.523
epoch 5,	batch   940,	training loss: 2.499
epoch 5,	batch   950,	training loss: 2.398
epoch 5,	batch   960,	training loss: 2.362
epoch 5,	batch   970,	training loss: 2.429
epoch 5,	batch   980,	training loss: 2.496
epoch 5,	batch   990,	training loss: 2.416
epoch 5,	batch  1000,	training loss: 2.355
epoch 5,	batch  1010,	training loss: 2.507
epoch 5,	batch  1020,	training loss: 2.369
epoch 5,	batch  1030,	training loss: 2.294
epoch 5,	batch  1040,	training loss: 2.308
epoch 5,	batch  1050,	training loss: 2.473
epoch 5,	batch  1060,	training loss: 2.473
epoch 5,	batch  1070,	training loss: 2.385
epoch 5,	batch  1080,	training loss: 2.333
epoch 5,	batch  1090,	training loss: 2.441
epoch 5,	batch  1100,	training loss: 2.430
epoch 5,	batch  1110,	training loss: 2.515
epoch 5,	batch  1120,	training loss: 2.455
epoch 5,	batch  1130,	training loss: 2.420
epoch 5,	batch  1140,	training loss: 2.434
epoch 5,	batch  1150,	training loss: 2.522
epoch 5,	batch  1160,	training loss: 2.437
epoch 5,	batch  1170,	training loss: 2.359
epoch 5,	batch  1180,	training loss: 2.378
epoch 5,	batch  1190,	training loss: 2.436
epoch 5,	batch  1200,	training loss: 2.470
epoch 5,	batch  1210,	training loss: 2.367
epoch 5,	batch  1220,	training loss: 2.422
epoch 5,	batch  1230,	training loss: 2.384
epoch 5,	batch  1240,	training loss: 2.541
epoch 5,	batch  1250,	training loss: 2.350
epoch 5,	batch  1260,	training loss: 2.593
epoch 5,	batch  1270,	training loss: 2.511
epoch 5,	batch  1280,	training loss: 2.632
epoch 5,	batch  1290,	training loss: 2.444
epoch 5,	batch  1300,	training loss: 2.444
epoch 5,	batch  1310,	training loss: 2.561
epoch 5,	batch  1320,	training loss: 2.412
epoch 5,	batch  1330,	training loss: 2.508
epoch 5,	batch  1340,	training loss: 2.406
epoch 5,	batch  1350,	training loss: 2.348
epoch 5,	batch  1360,	training loss: 2.265
epoch 5,	batch  1370,	training loss: 2.406
epoch 5,	batch  1380,	training loss: 2.406
epoch 5,	batch  1390,	training loss: 2.280
epoch 5,	batch  1400,	training loss: 2.479
epoch 5,	batch  1410,	training loss: 2.463
epoch 5,	batch  1420,	training loss: 2.439
epoch 5,	batch  1430,	training loss: 2.428
epoch 5,	batch  1440,	training loss: 2.539
epoch 5,	batch  1450,	training loss: 2.500
epoch 5,	batch  1460,	training loss: 2.430
epoch 5,	batch  1470,	training loss: 2.366
epoch 5,	batch  1480,	training loss: 2.564
epoch 5,	batch  1490,	training loss: 2.487
epoch 5,	batch  1500,	training loss: 2.557
epoch 5,	batch  1510,	training loss: 2.379
epoch 5,	batch  1520,	training loss: 2.485
epoch 5,	batch  1530,	training loss: 2.576
epoch 5,	batch  1540,	training loss: 2.527
epoch 5,	batch  1550,	training loss: 2.498
epoch 5,	batch  1560,	training loss: 2.412
epoch 5,	batch  1570,	training loss: 2.448
epoch 5,	batch  1580,	training loss: 2.440
epoch 5,	batch  1590,	training loss: 2.394
epoch 5,	batch  1600,	training loss: 2.380
epoch 5,	batch  1610,	training loss: 2.509
epoch 5,	batch  1620,	training loss: 2.483
epoch 5,	batch  1630,	training loss: 2.501
epoch 5,	batch  1640,	training loss: 2.486
epoch 5,	batch  1650,	training loss: 2.493
epoch 5,	batch  1660,	training loss: 2.530
epoch 5,	batch  1670,	training loss: 2.557
epoch 5,	batch  1680,	training loss: 2.497
epoch 5,	batch  1690,	training loss: 2.550
epoch 5,	batch  1700,	training loss: 2.301
epoch 5,	batch  1710,	training loss: 2.477
epoch 5,	batch  1720,	training loss: 2.606
epoch 5,	batch  1730,	training loss: 2.539
epoch 5,	batch  1740,	training loss: 2.468
epoch 5,	batch  1750,	training loss: 2.498
epoch 5,	batch  1760,	training loss: 2.623
epoch 5,	batch  1770,	training loss: 2.520
epoch 5,	batch  1780,	training loss: 2.582
epoch 5,	batch  1790,	training loss: 2.383
epoch 5,	batch  1800,	training loss: 2.483
epoch 5,	batch  1810,	training loss: 2.585
epoch 5,	batch  1820,	training loss: 2.452
epoch 5,	batch  1830,	training loss: 2.478
epoch 5,	batch  1840,	training loss: 2.389
epoch 5,	batch  1850,	training loss: 2.472
epoch 5,	batch  1860,	training loss: 2.401
epoch 5,	batch  1870,	training loss: 2.526
epoch 5,	batch  1880,	training loss: 2.553
epoch 5,	batch  1890,	training loss: 2.575
epoch 5,	batch  1900,	training loss: 2.377
epoch 5,	batch  1910,	training loss: 2.357
epoch 5,	batch  1920,	training loss: 2.549
epoch 5,	batch  1930,	training loss: 2.545
epoch 5,	batch  1940,	training loss: 2.473
epoch 5,	batch  1950,	training loss: 2.457
epoch 5,	batch  1960,	training loss: 2.550
epoch 5,	batch  1970,	training loss: 2.515
epoch 5,	batch  1980,	training loss: 2.577
END OF EPOCH 5
Testing on validation set...
# correct:	6588/54904 = 11.999125746757977%
# off by 1:	11372/54904 = 20.712516392248286%

epoch 6,	batch    10,	training loss: 1.966
epoch 6,	batch    20,	training loss: 2.028
epoch 6,	batch    30,	training loss: 1.899
epoch 6,	batch    40,	training loss: 2.044
epoch 6,	batch    50,	training loss: 1.930
epoch 6,	batch    60,	training loss: 1.976
epoch 6,	batch    70,	training loss: 1.924
epoch 6,	batch    80,	training loss: 1.890
epoch 6,	batch    90,	training loss: 1.888
epoch 6,	batch   100,	training loss: 1.858
epoch 6,	batch   110,	training loss: 1.935
epoch 6,	batch   120,	training loss: 1.890
epoch 6,	batch   130,	training loss: 1.929
epoch 6,	batch   140,	training loss: 1.936
epoch 6,	batch   150,	training loss: 2.030
epoch 6,	batch   160,	training loss: 2.136
epoch 6,	batch   170,	training loss: 1.980
epoch 6,	batch   180,	training loss: 2.030
epoch 6,	batch   190,	training loss: 1.944
epoch 6,	batch   200,	training loss: 1.918
epoch 6,	batch   210,	training loss: 1.992
epoch 6,	batch   220,	training loss: 1.989
epoch 6,	batch   230,	training loss: 1.898
epoch 6,	batch   240,	training loss: 1.957
epoch 6,	batch   250,	training loss: 1.925
epoch 6,	batch   260,	training loss: 1.998
epoch 6,	batch   270,	training loss: 1.940
epoch 6,	batch   280,	training loss: 1.863
epoch 6,	batch   290,	training loss: 1.930
epoch 6,	batch   300,	training loss: 1.964
epoch 6,	batch   310,	training loss: 1.949
epoch 6,	batch   320,	training loss: 1.871
epoch 6,	batch   330,	training loss: 1.972
epoch 6,	batch   340,	training loss: 1.967
epoch 6,	batch   350,	training loss: 1.999
epoch 6,	batch   360,	training loss: 1.820
epoch 6,	batch   370,	training loss: 2.065
epoch 6,	batch   380,	training loss: 1.814
epoch 6,	batch   390,	training loss: 2.029
epoch 6,	batch   400,	training loss: 2.041
epoch 6,	batch   410,	training loss: 1.861
epoch 6,	batch   420,	training loss: 1.996
epoch 6,	batch   430,	training loss: 2.040
epoch 6,	batch   440,	training loss: 2.006
epoch 6,	batch   450,	training loss: 1.955
epoch 6,	batch   460,	training loss: 1.936
epoch 6,	batch   470,	training loss: 2.044
epoch 6,	batch   480,	training loss: 2.096
epoch 6,	batch   490,	training loss: 2.013
epoch 6,	batch   500,	training loss: 1.914
epoch 6,	batch   510,	training loss: 2.049
epoch 6,	batch   520,	training loss: 2.129
epoch 6,	batch   530,	training loss: 2.047
epoch 6,	batch   540,	training loss: 2.081
epoch 6,	batch   550,	training loss: 2.045
epoch 6,	batch   560,	training loss: 1.841
epoch 6,	batch   570,	training loss: 1.976
epoch 6,	batch   580,	training loss: 2.044
epoch 6,	batch   590,	training loss: 1.947
epoch 6,	batch   600,	training loss: 1.976
epoch 6,	batch   610,	training loss: 1.995
epoch 6,	batch   620,	training loss: 2.011
epoch 6,	batch   630,	training loss: 2.036
epoch 6,	batch   640,	training loss: 2.166
epoch 6,	batch   650,	training loss: 2.016
epoch 6,	batch   660,	training loss: 1.956
epoch 6,	batch   670,	training loss: 1.897
epoch 6,	batch   680,	training loss: 2.007
epoch 6,	batch   690,	training loss: 2.041
epoch 6,	batch   700,	training loss: 2.121
epoch 6,	batch   710,	training loss: 2.144
epoch 6,	batch   720,	training loss: 2.083
epoch 6,	batch   730,	training loss: 2.014
epoch 6,	batch   740,	training loss: 2.031
epoch 6,	batch   750,	training loss: 1.906
epoch 6,	batch   760,	training loss: 2.080
epoch 6,	batch   770,	training loss: 2.005
epoch 6,	batch   780,	training loss: 2.201
epoch 6,	batch   790,	training loss: 1.947
epoch 6,	batch   800,	training loss: 1.949
epoch 6,	batch   810,	training loss: 2.135
epoch 6,	batch   820,	training loss: 2.157
epoch 6,	batch   830,	training loss: 1.962
epoch 6,	batch   840,	training loss: 1.979
epoch 6,	batch   850,	training loss: 1.946
epoch 6,	batch   860,	training loss: 2.034
epoch 6,	batch   870,	training loss: 2.128
epoch 6,	batch   880,	training loss: 2.026
epoch 6,	batch   890,	training loss: 2.080
epoch 6,	batch   900,	training loss: 2.200
epoch 6,	batch   910,	training loss: 2.128
epoch 6,	batch   920,	training loss: 2.056
epoch 6,	batch   930,	training loss: 2.080
epoch 6,	batch   940,	training loss: 2.140
epoch 6,	batch   950,	training loss: 2.110
epoch 6,	batch   960,	training loss: 2.145
epoch 6,	batch   970,	training loss: 2.038
epoch 6,	batch   980,	training loss: 2.199
epoch 6,	batch   990,	training loss: 2.091
epoch 6,	batch  1000,	training loss: 2.125
epoch 6,	batch  1010,	training loss: 1.948
epoch 6,	batch  1020,	training loss: 2.118
epoch 6,	batch  1030,	training loss: 2.226
epoch 6,	batch  1040,	training loss: 2.118
epoch 6,	batch  1050,	training loss: 2.208
epoch 6,	batch  1060,	training loss: 2.208
epoch 6,	batch  1070,	training loss: 2.044
epoch 6,	batch  1080,	training loss: 2.174
epoch 6,	batch  1090,	training loss: 2.073
epoch 6,	batch  1100,	training loss: 2.062
epoch 6,	batch  1110,	training loss: 2.177
epoch 6,	batch  1120,	training loss: 2.077
epoch 6,	batch  1130,	training loss: 2.093
epoch 6,	batch  1140,	training loss: 2.109
epoch 6,	batch  1150,	training loss: 2.105
epoch 6,	batch  1160,	training loss: 1.991
epoch 6,	batch  1170,	training loss: 2.131
epoch 6,	batch  1180,	training loss: 2.046
epoch 6,	batch  1190,	training loss: 2.093
epoch 6,	batch  1200,	training loss: 2.000
epoch 6,	batch  1210,	training loss: 2.165
epoch 6,	batch  1220,	training loss: 2.116
epoch 6,	batch  1230,	training loss: 2.159
epoch 6,	batch  1240,	training loss: 2.173
epoch 6,	batch  1250,	training loss: 2.147
epoch 6,	batch  1260,	training loss: 2.202
epoch 6,	batch  1270,	training loss: 2.182
epoch 6,	batch  1280,	training loss: 2.084
epoch 6,	batch  1290,	training loss: 2.176
epoch 6,	batch  1300,	training loss: 2.156
epoch 6,	batch  1310,	training loss: 2.169
epoch 6,	batch  1320,	training loss: 2.147
epoch 6,	batch  1330,	training loss: 2.130
epoch 6,	batch  1340,	training loss: 2.072
epoch 6,	batch  1350,	training loss: 2.186
epoch 6,	batch  1360,	training loss: 2.027
epoch 6,	batch  1370,	training loss: 2.244
epoch 6,	batch  1380,	training loss: 2.112
epoch 6,	batch  1390,	training loss: 2.065
epoch 6,	batch  1400,	training loss: 2.076
epoch 6,	batch  1410,	training loss: 2.132
epoch 6,	batch  1420,	training loss: 2.157
epoch 6,	batch  1430,	training loss: 2.146
epoch 6,	batch  1440,	training loss: 2.229
epoch 6,	batch  1450,	training loss: 2.158
epoch 6,	batch  1460,	training loss: 2.135
epoch 6,	batch  1470,	training loss: 2.098
epoch 6,	batch  1480,	training loss: 2.144
epoch 6,	batch  1490,	training loss: 2.073
epoch 6,	batch  1500,	training loss: 2.096
epoch 6,	batch  1510,	training loss: 2.283
epoch 6,	batch  1520,	training loss: 2.232
epoch 6,	batch  1530,	training loss: 2.222
epoch 6,	batch  1540,	training loss: 2.138
epoch 6,	batch  1550,	training loss: 2.132
epoch 6,	batch  1560,	training loss: 2.135
epoch 6,	batch  1570,	training loss: 2.233
epoch 6,	batch  1580,	training loss: 2.306
epoch 6,	batch  1590,	training loss: 2.123
epoch 6,	batch  1600,	training loss: 2.192
epoch 6,	batch  1610,	training loss: 2.082
epoch 6,	batch  1620,	training loss: 2.063
epoch 6,	batch  1630,	training loss: 2.166
epoch 6,	batch  1640,	training loss: 2.140
epoch 6,	batch  1650,	training loss: 2.144
epoch 6,	batch  1660,	training loss: 2.137
epoch 6,	batch  1670,	training loss: 2.104
epoch 6,	batch  1680,	training loss: 2.256
epoch 6,	batch  1690,	training loss: 2.289
epoch 6,	batch  1700,	training loss: 2.201
epoch 6,	batch  1710,	training loss: 2.243
epoch 6,	batch  1720,	training loss: 2.258
epoch 6,	batch  1730,	training loss: 2.299
epoch 6,	batch  1740,	training loss: 2.256
epoch 6,	batch  1750,	training loss: 2.237
epoch 6,	batch  1760,	training loss: 2.148
epoch 6,	batch  1770,	training loss: 2.222
epoch 6,	batch  1780,	training loss: 2.090
epoch 6,	batch  1790,	training loss: 2.179
epoch 6,	batch  1800,	training loss: 2.313
epoch 6,	batch  1810,	training loss: 2.291
epoch 6,	batch  1820,	training loss: 2.293
epoch 6,	batch  1830,	training loss: 2.419
epoch 6,	batch  1840,	training loss: 2.306
epoch 6,	batch  1850,	training loss: 2.212
epoch 6,	batch  1860,	training loss: 2.194
epoch 6,	batch  1870,	training loss: 2.115
epoch 6,	batch  1880,	training loss: 2.207
epoch 6,	batch  1890,	training loss: 2.223
epoch 6,	batch  1900,	training loss: 2.254
epoch 6,	batch  1910,	training loss: 2.202
epoch 6,	batch  1920,	training loss: 2.166
epoch 6,	batch  1930,	training loss: 2.113
epoch 6,	batch  1940,	training loss: 2.245
epoch 6,	batch  1950,	training loss: 2.330
epoch 6,	batch  1960,	training loss: 2.196
epoch 6,	batch  1970,	training loss: 2.131
epoch 6,	batch  1980,	training loss: 2.122
END OF EPOCH 6
Testing on validation set...
# correct:	6774/54904 = 12.337898878041672%
# off by 1:	11830/54904 = 21.546699694011366%

epoch 7,	batch    10,	training loss: 1.617
epoch 7,	batch    20,	training loss: 1.639
epoch 7,	batch    30,	training loss: 1.598
epoch 7,	batch    40,	training loss: 1.506
epoch 7,	batch    50,	training loss: 1.594
epoch 7,	batch    60,	training loss: 1.529
epoch 7,	batch    70,	training loss: 1.519
epoch 7,	batch    80,	training loss: 1.619
epoch 7,	batch    90,	training loss: 1.597
epoch 7,	batch   100,	training loss: 1.577
epoch 7,	batch   110,	training loss: 1.651
epoch 7,	batch   120,	training loss: 1.512
epoch 7,	batch   130,	training loss: 1.565
epoch 7,	batch   140,	training loss: 1.681
epoch 7,	batch   150,	training loss: 1.523
epoch 7,	batch   160,	training loss: 1.684
epoch 7,	batch   170,	training loss: 1.678
epoch 7,	batch   180,	training loss: 1.725
epoch 7,	batch   190,	training loss: 1.528
epoch 7,	batch   200,	training loss: 1.699
epoch 7,	batch   210,	training loss: 1.569
epoch 7,	batch   220,	training loss: 1.600
epoch 7,	batch   230,	training loss: 1.652
epoch 7,	batch   240,	training loss: 1.564
epoch 7,	batch   250,	training loss: 1.578
epoch 7,	batch   260,	training loss: 1.703
epoch 7,	batch   270,	training loss: 1.657
epoch 7,	batch   280,	training loss: 1.662
epoch 7,	batch   290,	training loss: 1.599
epoch 7,	batch   300,	training loss: 1.563
epoch 7,	batch   310,	training loss: 1.612
epoch 7,	batch   320,	training loss: 1.634
epoch 7,	batch   330,	training loss: 1.656
epoch 7,	batch   340,	training loss: 1.608
epoch 7,	batch   350,	training loss: 1.776
epoch 7,	batch   360,	training loss: 1.615
epoch 7,	batch   370,	training loss: 1.637
epoch 7,	batch   380,	training loss: 1.630
epoch 7,	batch   390,	training loss: 1.542
epoch 7,	batch   400,	training loss: 1.695
epoch 7,	batch   410,	training loss: 1.647
epoch 7,	batch   420,	training loss: 1.678
epoch 7,	batch   430,	training loss: 1.703
epoch 7,	batch   440,	training loss: 1.658
epoch 7,	batch   450,	training loss: 1.649
epoch 7,	batch   460,	training loss: 1.643
epoch 7,	batch   470,	training loss: 1.626
epoch 7,	batch   480,	training loss: 1.545
epoch 7,	batch   490,	training loss: 1.656
epoch 7,	batch   500,	training loss: 1.749
epoch 7,	batch   510,	training loss: 1.793
epoch 7,	batch   520,	training loss: 1.728
epoch 7,	batch   530,	training loss: 1.669
epoch 7,	batch   540,	training loss: 1.740
epoch 7,	batch   550,	training loss: 1.637
epoch 7,	batch   560,	training loss: 1.726
epoch 7,	batch   570,	training loss: 1.685
epoch 7,	batch   580,	training loss: 1.714
epoch 7,	batch   590,	training loss: 1.813
epoch 7,	batch   600,	training loss: 1.833
epoch 7,	batch   610,	training loss: 1.779
epoch 7,	batch   620,	training loss: 1.732
epoch 7,	batch   630,	training loss: 1.784
epoch 7,	batch   640,	training loss: 1.753
epoch 7,	batch   650,	training loss: 1.676
epoch 7,	batch   660,	training loss: 1.721
epoch 7,	batch   670,	training loss: 1.710
epoch 7,	batch   680,	training loss: 1.708
epoch 7,	batch   690,	training loss: 1.676
epoch 7,	batch   700,	training loss: 1.705
epoch 7,	batch   710,	training loss: 1.720
epoch 7,	batch   720,	training loss: 1.612
epoch 7,	batch   730,	training loss: 1.706
epoch 7,	batch   740,	training loss: 1.791
epoch 7,	batch   750,	training loss: 1.777
epoch 7,	batch   760,	training loss: 1.736
epoch 7,	batch   770,	training loss: 1.768
epoch 7,	batch   780,	training loss: 1.653
epoch 7,	batch   790,	training loss: 1.810
epoch 7,	batch   800,	training loss: 1.884
epoch 7,	batch   810,	training loss: 1.700
epoch 7,	batch   820,	training loss: 1.751
epoch 7,	batch   830,	training loss: 1.713
epoch 7,	batch   840,	training loss: 1.717
epoch 7,	batch   850,	training loss: 1.743
epoch 7,	batch   860,	training loss: 1.731
epoch 7,	batch   870,	training loss: 1.760
epoch 7,	batch   880,	training loss: 1.731
epoch 7,	batch   890,	training loss: 1.806
epoch 7,	batch   900,	training loss: 1.599
epoch 7,	batch   910,	training loss: 1.731
epoch 7,	batch   920,	training loss: 1.848
epoch 7,	batch   930,	training loss: 1.645
epoch 7,	batch   940,	training loss: 1.680
epoch 7,	batch   950,	training loss: 1.735
epoch 7,	batch   960,	training loss: 1.586
epoch 7,	batch   970,	training loss: 1.788
epoch 7,	batch   980,	training loss: 1.676
epoch 7,	batch   990,	training loss: 1.710
epoch 7,	batch  1000,	training loss: 1.753
epoch 7,	batch  1010,	training loss: 1.758
epoch 7,	batch  1020,	training loss: 1.869
epoch 7,	batch  1030,	training loss: 1.756
epoch 7,	batch  1040,	training loss: 1.662
epoch 7,	batch  1050,	training loss: 1.744
epoch 7,	batch  1060,	training loss: 1.679
epoch 7,	batch  1070,	training loss: 1.863
epoch 7,	batch  1080,	training loss: 1.799
epoch 7,	batch  1090,	training loss: 1.748
epoch 7,	batch  1100,	training loss: 1.788
epoch 7,	batch  1110,	training loss: 1.745
epoch 7,	batch  1120,	training loss: 1.872
epoch 7,	batch  1130,	training loss: 1.850
epoch 7,	batch  1140,	training loss: 1.755
epoch 7,	batch  1150,	training loss: 1.769
epoch 7,	batch  1160,	training loss: 1.659
epoch 7,	batch  1170,	training loss: 1.813
epoch 7,	batch  1180,	training loss: 1.814
epoch 7,	batch  1190,	training loss: 1.769
epoch 7,	batch  1200,	training loss: 1.786
epoch 7,	batch  1210,	training loss: 1.858
epoch 7,	batch  1220,	training loss: 1.878
epoch 7,	batch  1230,	training loss: 1.877
epoch 7,	batch  1240,	training loss: 1.730
epoch 7,	batch  1250,	training loss: 1.816
epoch 7,	batch  1260,	training loss: 1.740
epoch 7,	batch  1270,	training loss: 1.864
epoch 7,	batch  1280,	training loss: 1.843
epoch 7,	batch  1290,	training loss: 1.791
epoch 7,	batch  1300,	training loss: 1.747
epoch 7,	batch  1310,	training loss: 1.899
epoch 7,	batch  1320,	training loss: 1.837
epoch 7,	batch  1330,	training loss: 1.915
epoch 7,	batch  1340,	training loss: 1.935
epoch 7,	batch  1350,	training loss: 1.788
epoch 7,	batch  1360,	training loss: 1.717
epoch 7,	batch  1370,	training loss: 1.905
epoch 7,	batch  1380,	training loss: 1.851
epoch 7,	batch  1390,	training loss: 1.973
epoch 7,	batch  1400,	training loss: 1.902
epoch 7,	batch  1410,	training loss: 1.883
epoch 7,	batch  1420,	training loss: 1.735
epoch 7,	batch  1430,	training loss: 1.920
epoch 7,	batch  1440,	training loss: 1.904
epoch 7,	batch  1450,	training loss: 1.914
epoch 7,	batch  1460,	training loss: 1.883
epoch 7,	batch  1470,	training loss: 1.861
epoch 7,	batch  1480,	training loss: 1.765
epoch 7,	batch  1490,	training loss: 1.768
epoch 7,	batch  1500,	training loss: 1.764
epoch 7,	batch  1510,	training loss: 1.865
epoch 7,	batch  1520,	training loss: 1.782
epoch 7,	batch  1530,	training loss: 1.904
epoch 7,	batch  1540,	training loss: 1.919
epoch 7,	batch  1550,	training loss: 1.924
epoch 7,	batch  1560,	training loss: 1.968
epoch 7,	batch  1570,	training loss: 1.866
epoch 7,	batch  1580,	training loss: 1.761
epoch 7,	batch  1590,	training loss: 1.871
epoch 7,	batch  1600,	training loss: 1.818
epoch 7,	batch  1610,	training loss: 1.879
epoch 7,	batch  1620,	training loss: 1.857
epoch 7,	batch  1630,	training loss: 1.934
epoch 7,	batch  1640,	training loss: 1.816
epoch 7,	batch  1650,	training loss: 1.889
epoch 7,	batch  1660,	training loss: 1.825
epoch 7,	batch  1670,	training loss: 1.779
epoch 7,	batch  1680,	training loss: 1.783
epoch 7,	batch  1690,	training loss: 1.991
epoch 7,	batch  1700,	training loss: 1.920
epoch 7,	batch  1710,	training loss: 1.811
epoch 7,	batch  1720,	training loss: 1.868
epoch 7,	batch  1730,	training loss: 1.772
epoch 7,	batch  1740,	training loss: 1.883
epoch 7,	batch  1750,	training loss: 1.943
epoch 7,	batch  1760,	training loss: 1.929
epoch 7,	batch  1770,	training loss: 1.879
epoch 7,	batch  1780,	training loss: 1.867
epoch 7,	batch  1790,	training loss: 1.915
epoch 7,	batch  1800,	training loss: 2.050
epoch 7,	batch  1810,	training loss: 1.869
epoch 7,	batch  1820,	training loss: 1.879
epoch 7,	batch  1830,	training loss: 1.927
epoch 7,	batch  1840,	training loss: 1.846
epoch 7,	batch  1850,	training loss: 1.824
epoch 7,	batch  1860,	training loss: 1.846
epoch 7,	batch  1870,	training loss: 1.920
epoch 7,	batch  1880,	training loss: 1.875
epoch 7,	batch  1890,	training loss: 1.872
epoch 7,	batch  1900,	training loss: 1.814
epoch 7,	batch  1910,	training loss: 1.795
epoch 7,	batch  1920,	training loss: 1.909
epoch 7,	batch  1930,	training loss: 1.933
epoch 7,	batch  1940,	training loss: 1.993
epoch 7,	batch  1950,	training loss: 2.009
epoch 7,	batch  1960,	training loss: 1.772
epoch 7,	batch  1970,	training loss: 1.888
epoch 7,	batch  1980,	training loss: 1.900
END OF EPOCH 7
Testing on validation set...
# correct:	6417/54904 = 11.687673029287483%
# off by 1:	11205/54904 = 20.408349118461313%

epoch 8,	batch    10,	training loss: 1.394
epoch 8,	batch    20,	training loss: 1.359
epoch 8,	batch    30,	training loss: 1.347
epoch 8,	batch    40,	training loss: 1.347
epoch 8,	batch    50,	training loss: 1.186
epoch 8,	batch    60,	training loss: 1.357
epoch 8,	batch    70,	training loss: 1.197
epoch 8,	batch    80,	training loss: 1.241
epoch 8,	batch    90,	training loss: 1.229
epoch 8,	batch   100,	training loss: 1.147
epoch 8,	batch   110,	training loss: 1.278
epoch 8,	batch   120,	training loss: 1.258
epoch 8,	batch   130,	training loss: 1.268
epoch 8,	batch   140,	training loss: 1.339
epoch 8,	batch   150,	training loss: 1.356
epoch 8,	batch   160,	training loss: 1.271
epoch 8,	batch   170,	training loss: 1.240
epoch 8,	batch   180,	training loss: 1.288
epoch 8,	batch   190,	training loss: 1.309
epoch 8,	batch   200,	training loss: 1.254
epoch 8,	batch   210,	training loss: 1.314
epoch 8,	batch   220,	training loss: 1.275
epoch 8,	batch   230,	training loss: 1.305
epoch 8,	batch   240,	training loss: 1.316
epoch 8,	batch   250,	training loss: 1.359
epoch 8,	batch   260,	training loss: 1.317
epoch 8,	batch   270,	training loss: 1.272
epoch 8,	batch   280,	training loss: 1.278
epoch 8,	batch   290,	training loss: 1.380
epoch 8,	batch   300,	training loss: 1.264
epoch 8,	batch   310,	training loss: 1.331
epoch 8,	batch   320,	training loss: 1.256
epoch 8,	batch   330,	training loss: 1.293
epoch 8,	batch   340,	training loss: 1.308
epoch 8,	batch   350,	training loss: 1.278
epoch 8,	batch   360,	training loss: 1.223
epoch 8,	batch   370,	training loss: 1.308
epoch 8,	batch   380,	training loss: 1.374
epoch 8,	batch   390,	training loss: 1.398
epoch 8,	batch   400,	training loss: 1.459
epoch 8,	batch   410,	training loss: 1.423
epoch 8,	batch   420,	training loss: 1.371
epoch 8,	batch   430,	training loss: 1.215
epoch 8,	batch   440,	training loss: 1.383
epoch 8,	batch   450,	training loss: 1.313
epoch 8,	batch   460,	training loss: 1.319
epoch 8,	batch   470,	training loss: 1.380
epoch 8,	batch   480,	training loss: 1.376
epoch 8,	batch   490,	training loss: 1.295
epoch 8,	batch   500,	training loss: 1.338
epoch 8,	batch   510,	training loss: 1.343
epoch 8,	batch   520,	training loss: 1.366
epoch 8,	batch   530,	training loss: 1.351
epoch 8,	batch   540,	training loss: 1.397
epoch 8,	batch   550,	training loss: 1.377
epoch 8,	batch   560,	training loss: 1.385
epoch 8,	batch   570,	training loss: 1.359
epoch 8,	batch   580,	training loss: 1.401
epoch 8,	batch   590,	training loss: 1.394
epoch 8,	batch   600,	training loss: 1.529
epoch 8,	batch   610,	training loss: 1.369
epoch 8,	batch   620,	training loss: 1.337
epoch 8,	batch   630,	training loss: 1.456
epoch 8,	batch   640,	training loss: 1.483
epoch 8,	batch   650,	training loss: 1.355
epoch 8,	batch   660,	training loss: 1.400
epoch 8,	batch   670,	training loss: 1.443
epoch 8,	batch   680,	training loss: 1.490
epoch 8,	batch   690,	training loss: 1.451
epoch 8,	batch   700,	training loss: 1.348
epoch 8,	batch   710,	training loss: 1.412
epoch 8,	batch   720,	training loss: 1.522
epoch 8,	batch   730,	training loss: 1.465
epoch 8,	batch   740,	training loss: 1.440
epoch 8,	batch   750,	training loss: 1.417
epoch 8,	batch   760,	training loss: 1.458
epoch 8,	batch   770,	training loss: 1.388
epoch 8,	batch   780,	training loss: 1.365
epoch 8,	batch   790,	training loss: 1.399
epoch 8,	batch   800,	training loss: 1.388
epoch 8,	batch   810,	training loss: 1.386
epoch 8,	batch   820,	training loss: 1.430
epoch 8,	batch   830,	training loss: 1.467
epoch 8,	batch   840,	training loss: 1.338
epoch 8,	batch   850,	training loss: 1.457
epoch 8,	batch   860,	training loss: 1.515
epoch 8,	batch   870,	training loss: 1.515
epoch 8,	batch   880,	training loss: 1.457
epoch 8,	batch   890,	training loss: 1.491
epoch 8,	batch   900,	training loss: 1.463
epoch 8,	batch   910,	training loss: 1.459
epoch 8,	batch   920,	training loss: 1.456
epoch 8,	batch   930,	training loss: 1.439
epoch 8,	batch   940,	training loss: 1.583
epoch 8,	batch   950,	training loss: 1.467
epoch 8,	batch   960,	training loss: 1.491
epoch 8,	batch   970,	training loss: 1.496
epoch 8,	batch   980,	training loss: 1.369
epoch 8,	batch   990,	training loss: 1.453
epoch 8,	batch  1000,	training loss: 1.526
epoch 8,	batch  1010,	training loss: 1.614
epoch 8,	batch  1020,	training loss: 1.488
epoch 8,	batch  1030,	training loss: 1.421
epoch 8,	batch  1040,	training loss: 1.486
epoch 8,	batch  1050,	training loss: 1.438
epoch 8,	batch  1060,	training loss: 1.498
epoch 8,	batch  1070,	training loss: 1.500
epoch 8,	batch  1080,	training loss: 1.462
epoch 8,	batch  1090,	training loss: 1.516
epoch 8,	batch  1100,	training loss: 1.463
epoch 8,	batch  1110,	training loss: 1.463
epoch 8,	batch  1120,	training loss: 1.355
epoch 8,	batch  1130,	training loss: 1.557
epoch 8,	batch  1140,	training loss: 1.576
epoch 8,	batch  1150,	training loss: 1.537
epoch 8,	batch  1160,	training loss: 1.429
epoch 8,	batch  1170,	training loss: 1.521
epoch 8,	batch  1180,	training loss: 1.532
epoch 8,	batch  1190,	training loss: 1.528
epoch 8,	batch  1200,	training loss: 1.489
epoch 8,	batch  1210,	training loss: 1.455
epoch 8,	batch  1220,	training loss: 1.635
epoch 8,	batch  1230,	training loss: 1.554
epoch 8,	batch  1240,	training loss: 1.443
epoch 8,	batch  1250,	training loss: 1.519
epoch 8,	batch  1260,	training loss: 1.487
epoch 8,	batch  1270,	training loss: 1.613
epoch 8,	batch  1280,	training loss: 1.537
epoch 8,	batch  1290,	training loss: 1.523
epoch 8,	batch  1300,	training loss: 1.567
epoch 8,	batch  1310,	training loss: 1.450
epoch 8,	batch  1320,	training loss: 1.526
epoch 8,	batch  1330,	training loss: 1.558
epoch 8,	batch  1340,	training loss: 1.571
epoch 8,	batch  1350,	training loss: 1.560
epoch 8,	batch  1360,	training loss: 1.592
epoch 8,	batch  1370,	training loss: 1.572
epoch 8,	batch  1380,	training loss: 1.558
epoch 8,	batch  1390,	training loss: 1.470
epoch 8,	batch  1400,	training loss: 1.581
epoch 8,	batch  1410,	training loss: 1.556
epoch 8,	batch  1420,	training loss: 1.476
epoch 8,	batch  1430,	training loss: 1.621
epoch 8,	batch  1440,	training loss: 1.543
epoch 8,	batch  1450,	training loss: 1.475
epoch 8,	batch  1460,	training loss: 1.580
epoch 8,	batch  1470,	training loss: 1.486
epoch 8,	batch  1480,	training loss: 1.519
epoch 8,	batch  1490,	training loss: 1.509
epoch 8,	batch  1500,	training loss: 1.681
epoch 8,	batch  1510,	training loss: 1.685
epoch 8,	batch  1520,	training loss: 1.622
epoch 8,	batch  1530,	training loss: 1.619
epoch 8,	batch  1540,	training loss: 1.530
epoch 8,	batch  1550,	training loss: 1.465
epoch 8,	batch  1560,	training loss: 1.484
epoch 8,	batch  1570,	training loss: 1.530
epoch 8,	batch  1580,	training loss: 1.645
epoch 8,	batch  1590,	training loss: 1.574
epoch 8,	batch  1600,	training loss: 1.596
epoch 8,	batch  1610,	training loss: 1.471
epoch 8,	batch  1620,	training loss: 1.523
epoch 8,	batch  1630,	training loss: 1.601
epoch 8,	batch  1640,	training loss: 1.533
epoch 8,	batch  1650,	training loss: 1.571
epoch 8,	batch  1660,	training loss: 1.635
epoch 8,	batch  1670,	training loss: 1.729
epoch 8,	batch  1680,	training loss: 1.648
epoch 8,	batch  1690,	training loss: 1.648
epoch 8,	batch  1700,	training loss: 1.595
epoch 8,	batch  1710,	training loss: 1.447
epoch 8,	batch  1720,	training loss: 1.645
epoch 8,	batch  1730,	training loss: 1.603
epoch 8,	batch  1740,	training loss: 1.519
epoch 8,	batch  1750,	training loss: 1.489
epoch 8,	batch  1760,	training loss: 1.593
epoch 8,	batch  1770,	training loss: 1.574
epoch 8,	batch  1780,	training loss: 1.650
epoch 8,	batch  1790,	training loss: 1.740
epoch 8,	batch  1800,	training loss: 1.615
epoch 8,	batch  1810,	training loss: 1.496
epoch 8,	batch  1820,	training loss: 1.597
epoch 8,	batch  1830,	training loss: 1.564
epoch 8,	batch  1840,	training loss: 1.598
epoch 8,	batch  1850,	training loss: 1.643
epoch 8,	batch  1860,	training loss: 1.781
epoch 8,	batch  1870,	training loss: 1.729
epoch 8,	batch  1880,	training loss: 1.456
epoch 8,	batch  1890,	training loss: 1.589
epoch 8,	batch  1900,	training loss: 1.527
epoch 8,	batch  1910,	training loss: 1.585
epoch 8,	batch  1920,	training loss: 1.491
epoch 8,	batch  1930,	training loss: 1.694
epoch 8,	batch  1940,	training loss: 1.644
epoch 8,	batch  1950,	training loss: 1.565
epoch 8,	batch  1960,	training loss: 1.547
epoch 8,	batch  1970,	training loss: 1.690
epoch 8,	batch  1980,	training loss: 1.780
END OF EPOCH 8
Testing on validation set...
# correct:	5378/54904 = 9.79527903249308%
# off by 1:	10294/54904 = 18.74908931953956%

epoch 9,	batch    10,	training loss: 1.074
epoch 9,	batch    20,	training loss: 1.150
epoch 9,	batch    30,	training loss: 1.085
epoch 9,	batch    40,	training loss: 1.002
epoch 9,	batch    50,	training loss: 1.044
epoch 9,	batch    60,	training loss: 1.087
epoch 9,	batch    70,	training loss: 1.087
epoch 9,	batch    80,	training loss: 1.067
epoch 9,	batch    90,	training loss: 1.035
epoch 9,	batch   100,	training loss: 1.062
epoch 9,	batch   110,	training loss: 1.020
epoch 9,	batch   120,	training loss: 0.996
epoch 9,	batch   130,	training loss: 1.004
epoch 9,	batch   140,	training loss: 1.064
epoch 9,	batch   150,	training loss: 1.045
epoch 9,	batch   160,	training loss: 0.956
epoch 9,	batch   170,	training loss: 1.004
epoch 9,	batch   180,	training loss: 1.032
epoch 9,	batch   190,	training loss: 1.110
epoch 9,	batch   200,	training loss: 1.110
epoch 9,	batch   210,	training loss: 1.072
epoch 9,	batch   220,	training loss: 1.042
epoch 9,	batch   230,	training loss: 1.164
epoch 9,	batch   240,	training loss: 1.008
epoch 9,	batch   250,	training loss: 1.045
epoch 9,	batch   260,	training loss: 1.127
epoch 9,	batch   270,	training loss: 1.062
epoch 9,	batch   280,	training loss: 1.134
epoch 9,	batch   290,	training loss: 1.130
epoch 9,	batch   300,	training loss: 1.006
epoch 9,	batch   310,	training loss: 1.169
epoch 9,	batch   320,	training loss: 1.235
epoch 9,	batch   330,	training loss: 1.057
epoch 9,	batch   340,	training loss: 1.092
epoch 9,	batch   350,	training loss: 1.148
epoch 9,	batch   360,	training loss: 1.034
epoch 9,	batch   370,	training loss: 1.160
epoch 9,	batch   380,	training loss: 1.090
epoch 9,	batch   390,	training loss: 1.150
epoch 9,	batch   400,	training loss: 1.085
epoch 9,	batch   410,	training loss: 1.122
epoch 9,	batch   420,	training loss: 1.136
epoch 9,	batch   430,	training loss: 1.012
epoch 9,	batch   440,	training loss: 1.081
epoch 9,	batch   450,	training loss: 1.034
epoch 9,	batch   460,	training loss: 1.049
epoch 9,	batch   470,	training loss: 1.088
epoch 9,	batch   480,	training loss: 1.045
epoch 9,	batch   490,	training loss: 1.177
epoch 9,	batch   500,	training loss: 1.070
epoch 9,	batch   510,	training loss: 1.087
epoch 9,	batch   520,	training loss: 1.277
epoch 9,	batch   530,	training loss: 1.116
epoch 9,	batch   540,	training loss: 1.151
epoch 9,	batch   550,	training loss: 1.101
epoch 9,	batch   560,	training loss: 1.134
epoch 9,	batch   570,	training loss: 1.110
epoch 9,	batch   580,	training loss: 1.062
epoch 9,	batch   590,	training loss: 1.070
epoch 9,	batch   600,	training loss: 1.063
epoch 9,	batch   610,	training loss: 1.070
epoch 9,	batch   620,	training loss: 1.085
epoch 9,	batch   630,	training loss: 1.265
epoch 9,	batch   640,	training loss: 1.025
epoch 9,	batch   650,	training loss: 1.274
epoch 9,	batch   660,	training loss: 1.101
epoch 9,	batch   670,	training loss: 1.173
epoch 9,	batch   680,	training loss: 1.087
epoch 9,	batch   690,	training loss: 1.116
epoch 9,	batch   700,	training loss: 1.225
epoch 9,	batch   710,	training loss: 1.170
epoch 9,	batch   720,	training loss: 1.240
epoch 9,	batch   730,	training loss: 1.117
epoch 9,	batch   740,	training loss: 1.190
epoch 9,	batch   750,	training loss: 1.160
epoch 9,	batch   760,	training loss: 1.132
epoch 9,	batch   770,	training loss: 1.185
epoch 9,	batch   780,	training loss: 1.125
epoch 9,	batch   790,	training loss: 1.111
epoch 9,	batch   800,	training loss: 1.111
epoch 9,	batch   810,	training loss: 1.225
epoch 9,	batch   820,	training loss: 1.155
epoch 9,	batch   830,	training loss: 1.120
epoch 9,	batch   840,	training loss: 1.108
epoch 9,	batch   850,	training loss: 1.139
epoch 9,	batch   860,	training loss: 1.272
epoch 9,	batch   870,	training loss: 1.207
epoch 9,	batch   880,	training loss: 1.158
epoch 9,	batch   890,	training loss: 1.206
epoch 9,	batch   900,	training loss: 1.244
epoch 9,	batch   910,	training loss: 1.217
epoch 9,	batch   920,	training loss: 1.167
epoch 9,	batch   930,	training loss: 1.150
epoch 9,	batch   940,	training loss: 1.273
epoch 9,	batch   950,	training loss: 1.116
epoch 9,	batch   960,	training loss: 1.341
epoch 9,	batch   970,	training loss: 1.280
epoch 9,	batch   980,	training loss: 1.123
epoch 9,	batch   990,	training loss: 1.251
epoch 9,	batch  1000,	training loss: 1.184
epoch 9,	batch  1010,	training loss: 1.220
epoch 9,	batch  1020,	training loss: 1.165
epoch 9,	batch  1030,	training loss: 1.040
epoch 9,	batch  1040,	training loss: 1.271
epoch 9,	batch  1050,	training loss: 1.171
epoch 9,	batch  1060,	training loss: 1.168
epoch 9,	batch  1070,	training loss: 1.143
epoch 9,	batch  1080,	training loss: 1.289
epoch 9,	batch  1090,	training loss: 1.334
epoch 9,	batch  1100,	training loss: 1.335
epoch 9,	batch  1110,	training loss: 1.258
epoch 9,	batch  1120,	training loss: 1.249
epoch 9,	batch  1130,	training loss: 1.322
epoch 9,	batch  1140,	training loss: 1.152
epoch 9,	batch  1150,	training loss: 1.240
epoch 9,	batch  1160,	training loss: 1.257
epoch 9,	batch  1170,	training loss: 1.300
epoch 9,	batch  1180,	training loss: 1.191
epoch 9,	batch  1190,	training loss: 1.366
epoch 9,	batch  1200,	training loss: 1.308
epoch 9,	batch  1210,	training loss: 1.241
epoch 9,	batch  1220,	training loss: 1.160
epoch 9,	batch  1230,	training loss: 1.256
epoch 9,	batch  1240,	training loss: 1.305
epoch 9,	batch  1250,	training loss: 1.303
epoch 9,	batch  1260,	training loss: 1.293
epoch 9,	batch  1270,	training loss: 1.277
epoch 9,	batch  1280,	training loss: 1.283
epoch 9,	batch  1290,	training loss: 1.229
epoch 9,	batch  1300,	training loss: 1.236
epoch 9,	batch  1310,	training loss: 1.325
epoch 9,	batch  1320,	training loss: 1.351
epoch 9,	batch  1330,	training loss: 1.249
epoch 9,	batch  1340,	training loss: 1.212
epoch 9,	batch  1350,	training loss: 1.252
epoch 9,	batch  1360,	training loss: 1.313
epoch 9,	batch  1370,	training loss: 1.325
epoch 9,	batch  1380,	training loss: 1.279
epoch 9,	batch  1390,	training loss: 1.364
epoch 9,	batch  1400,	training loss: 1.225
epoch 9,	batch  1410,	training loss: 1.258
epoch 9,	batch  1420,	training loss: 1.198
epoch 9,	batch  1430,	training loss: 1.222
epoch 9,	batch  1440,	training loss: 1.218
epoch 9,	batch  1450,	training loss: 1.220
epoch 9,	batch  1460,	training loss: 1.327
epoch 9,	batch  1470,	training loss: 1.259
epoch 9,	batch  1480,	training loss: 1.323
epoch 9,	batch  1490,	training loss: 1.237
epoch 9,	batch  1500,	training loss: 1.225
epoch 9,	batch  1510,	training loss: 1.287
epoch 9,	batch  1520,	training loss: 1.285
epoch 9,	batch  1530,	training loss: 1.178
epoch 9,	batch  1540,	training loss: 1.257
epoch 9,	batch  1550,	training loss: 1.255
epoch 9,	batch  1560,	training loss: 1.230
epoch 9,	batch  1570,	training loss: 1.343
epoch 9,	batch  1580,	training loss: 1.218
epoch 9,	batch  1590,	training loss: 1.316
epoch 9,	batch  1600,	training loss: 1.346
epoch 9,	batch  1610,	training loss: 1.308
epoch 9,	batch  1620,	training loss: 1.302
epoch 9,	batch  1630,	training loss: 1.316
epoch 9,	batch  1640,	training loss: 1.226
epoch 9,	batch  1650,	training loss: 1.344
epoch 9,	batch  1660,	training loss: 1.302
epoch 9,	batch  1670,	training loss: 1.273
epoch 9,	batch  1680,	training loss: 1.315
epoch 9,	batch  1690,	training loss: 1.400
epoch 9,	batch  1700,	training loss: 1.351
epoch 9,	batch  1710,	training loss: 1.305
epoch 9,	batch  1720,	training loss: 1.278
epoch 9,	batch  1730,	training loss: 1.291
epoch 9,	batch  1740,	training loss: 1.344
epoch 9,	batch  1750,	training loss: 1.316
epoch 9,	batch  1760,	training loss: 1.382
epoch 9,	batch  1770,	training loss: 1.286
epoch 9,	batch  1780,	training loss: 1.347
epoch 9,	batch  1790,	training loss: 1.322
epoch 9,	batch  1800,	training loss: 1.295
epoch 9,	batch  1810,	training loss: 1.350
epoch 9,	batch  1820,	training loss: 1.239
epoch 9,	batch  1830,	training loss: 1.243
epoch 9,	batch  1840,	training loss: 1.290
epoch 9,	batch  1850,	training loss: 1.400
epoch 9,	batch  1860,	training loss: 1.209
epoch 9,	batch  1870,	training loss: 1.252
epoch 9,	batch  1880,	training loss: 1.352
epoch 9,	batch  1890,	training loss: 1.354
epoch 9,	batch  1900,	training loss: 1.285
epoch 9,	batch  1910,	training loss: 1.361
epoch 9,	batch  1920,	training loss: 1.338
epoch 9,	batch  1930,	training loss: 1.306
epoch 9,	batch  1940,	training loss: 1.325
epoch 9,	batch  1950,	training loss: 1.373
epoch 9,	batch  1960,	training loss: 1.295
epoch 9,	batch  1970,	training loss: 1.332
epoch 9,	batch  1980,	training loss: 1.404
END OF EPOCH 9
Testing on validation set...
# correct:	6584/54904 = 11.991840303074458%
# off by 1:	11796/54904 = 21.484773422701444%

epoch 10,	batch    10,	training loss: 0.963
epoch 10,	batch    20,	training loss: 0.813
epoch 10,	batch    30,	training loss: 0.804
epoch 10,	batch    40,	training loss: 0.812
epoch 10,	batch    50,	training loss: 0.825
epoch 10,	batch    60,	training loss: 0.876
epoch 10,	batch    70,	training loss: 0.894
epoch 10,	batch    80,	training loss: 0.879
epoch 10,	batch    90,	training loss: 0.825
epoch 10,	batch   100,	training loss: 0.763
epoch 10,	batch   110,	training loss: 0.882
epoch 10,	batch   120,	training loss: 0.852
epoch 10,	batch   130,	training loss: 0.798
epoch 10,	batch   140,	training loss: 0.885
epoch 10,	batch   150,	training loss: 0.826
epoch 10,	batch   160,	training loss: 0.855
epoch 10,	batch   170,	training loss: 0.885
epoch 10,	batch   180,	training loss: 0.898
epoch 10,	batch   190,	training loss: 0.808
epoch 10,	batch   200,	training loss: 0.843
epoch 10,	batch   210,	training loss: 0.853
epoch 10,	batch   220,	training loss: 0.858
epoch 10,	batch   230,	training loss: 0.850
epoch 10,	batch   240,	training loss: 0.792
epoch 10,	batch   250,	training loss: 0.864
epoch 10,	batch   260,	training loss: 0.861
epoch 10,	batch   270,	training loss: 0.936
epoch 10,	batch   280,	training loss: 0.815
epoch 10,	batch   290,	training loss: 0.817
epoch 10,	batch   300,	training loss: 0.829
epoch 10,	batch   310,	training loss: 0.853
epoch 10,	batch   320,	training loss: 0.829
epoch 10,	batch   330,	training loss: 0.877
epoch 10,	batch   340,	training loss: 0.936
epoch 10,	batch   350,	training loss: 0.926
epoch 10,	batch   360,	training loss: 0.934
epoch 10,	batch   370,	training loss: 0.929
epoch 10,	batch   380,	training loss: 0.872
epoch 10,	batch   390,	training loss: 0.905
epoch 10,	batch   400,	training loss: 0.833
epoch 10,	batch   410,	training loss: 0.884
epoch 10,	batch   420,	training loss: 0.856
epoch 10,	batch   430,	training loss: 0.908
epoch 10,	batch   440,	training loss: 0.888
epoch 10,	batch   450,	training loss: 0.876
epoch 10,	batch   460,	training loss: 0.935
epoch 10,	batch   470,	training loss: 0.847
epoch 10,	batch   480,	training loss: 0.941
epoch 10,	batch   490,	training loss: 0.973
epoch 10,	batch   500,	training loss: 0.804
epoch 10,	batch   510,	training loss: 0.903
epoch 10,	batch   520,	training loss: 0.882
epoch 10,	batch   530,	training loss: 0.865
epoch 10,	batch   540,	training loss: 0.828
epoch 10,	batch   550,	training loss: 0.962
epoch 10,	batch   560,	training loss: 0.927
epoch 10,	batch   570,	training loss: 0.915
epoch 10,	batch   580,	training loss: 0.903
epoch 10,	batch   590,	training loss: 0.984
epoch 10,	batch   600,	training loss: 0.866
epoch 10,	batch   610,	training loss: 0.899
epoch 10,	batch   620,	training loss: 0.938
epoch 10,	batch   630,	training loss: 0.996
epoch 10,	batch   640,	training loss: 0.926
epoch 10,	batch   650,	training loss: 0.876
epoch 10,	batch   660,	training loss: 0.944
epoch 10,	batch   670,	training loss: 0.943
epoch 10,	batch   680,	training loss: 0.931
epoch 10,	batch   690,	training loss: 0.959
epoch 10,	batch   700,	training loss: 0.953
epoch 10,	batch   710,	training loss: 0.972
epoch 10,	batch   720,	training loss: 0.994
epoch 10,	batch   730,	training loss: 0.958
epoch 10,	batch   740,	training loss: 1.001
epoch 10,	batch   750,	training loss: 0.949
epoch 10,	batch   760,	training loss: 0.935
epoch 10,	batch   770,	training loss: 0.968
epoch 10,	batch   780,	training loss: 0.953
epoch 10,	batch   790,	training loss: 0.981
epoch 10,	batch   800,	training loss: 1.007
epoch 10,	batch   810,	training loss: 1.003
epoch 10,	batch   820,	training loss: 0.965
epoch 10,	batch   830,	training loss: 0.913
epoch 10,	batch   840,	training loss: 0.972
epoch 10,	batch   850,	training loss: 0.970
epoch 10,	batch   860,	training loss: 1.000
epoch 10,	batch   870,	training loss: 0.947
epoch 10,	batch   880,	training loss: 0.986
epoch 10,	batch   890,	training loss: 0.992
epoch 10,	batch   900,	training loss: 1.044
epoch 10,	batch   910,	training loss: 1.030
epoch 10,	batch   920,	training loss: 0.898
epoch 10,	batch   930,	training loss: 1.046
epoch 10,	batch   940,	training loss: 0.973
epoch 10,	batch   950,	training loss: 0.927
epoch 10,	batch   960,	training loss: 0.982
epoch 10,	batch   970,	training loss: 1.050
epoch 10,	batch   980,	training loss: 0.978
epoch 10,	batch   990,	training loss: 0.970
epoch 10,	batch  1000,	training loss: 0.910
epoch 10,	batch  1010,	training loss: 1.021
epoch 10,	batch  1020,	training loss: 1.115
epoch 10,	batch  1030,	training loss: 0.972
epoch 10,	batch  1040,	training loss: 0.952
epoch 10,	batch  1050,	training loss: 0.997
epoch 10,	batch  1060,	training loss: 0.958
epoch 10,	batch  1070,	training loss: 1.017
epoch 10,	batch  1080,	training loss: 0.947
epoch 10,	batch  1090,	training loss: 0.966
epoch 10,	batch  1100,	training loss: 0.940
epoch 10,	batch  1110,	training loss: 1.048
epoch 10,	batch  1120,	training loss: 0.902
epoch 10,	batch  1130,	training loss: 1.135
epoch 10,	batch  1140,	training loss: 1.004
epoch 10,	batch  1150,	training loss: 1.142
epoch 10,	batch  1160,	training loss: 1.047
epoch 10,	batch  1170,	training loss: 1.044
epoch 10,	batch  1180,	training loss: 1.027
epoch 10,	batch  1190,	training loss: 0.931
epoch 10,	batch  1200,	training loss: 1.072
epoch 10,	batch  1210,	training loss: 0.981
epoch 10,	batch  1220,	training loss: 0.874
epoch 10,	batch  1230,	training loss: 0.973
epoch 10,	batch  1240,	training loss: 1.059
epoch 10,	batch  1250,	training loss: 1.026
epoch 10,	batch  1260,	training loss: 1.007
epoch 10,	batch  1270,	training loss: 1.046
epoch 10,	batch  1280,	training loss: 1.010
epoch 10,	batch  1290,	training loss: 0.990
epoch 10,	batch  1300,	training loss: 0.965
epoch 10,	batch  1310,	training loss: 1.015
epoch 10,	batch  1320,	training loss: 1.051
epoch 10,	batch  1330,	training loss: 1.003
epoch 10,	batch  1340,	training loss: 1.086
epoch 10,	batch  1350,	training loss: 0.991
epoch 10,	batch  1360,	training loss: 1.118
epoch 10,	batch  1370,	training loss: 1.010
epoch 10,	batch  1380,	training loss: 0.993
epoch 10,	batch  1390,	training loss: 1.052
epoch 10,	batch  1400,	training loss: 1.116
epoch 10,	batch  1410,	training loss: 1.000
epoch 10,	batch  1420,	training loss: 1.145
epoch 10,	batch  1430,	training loss: 1.099
epoch 10,	batch  1440,	training loss: 1.122
epoch 10,	batch  1450,	training loss: 1.129
epoch 10,	batch  1460,	training loss: 1.044
epoch 10,	batch  1470,	training loss: 1.046
epoch 10,	batch  1480,	training loss: 1.018
epoch 10,	batch  1490,	training loss: 1.125
epoch 10,	batch  1500,	training loss: 0.981
epoch 10,	batch  1510,	training loss: 0.968
epoch 10,	batch  1520,	training loss: 1.049
epoch 10,	batch  1530,	training loss: 1.084
epoch 10,	batch  1540,	training loss: 1.016
epoch 10,	batch  1550,	training loss: 1.122
epoch 10,	batch  1560,	training loss: 1.078
epoch 10,	batch  1570,	training loss: 1.079
epoch 10,	batch  1580,	training loss: 1.227
epoch 10,	batch  1590,	training loss: 1.035
epoch 10,	batch  1600,	training loss: 1.111
epoch 10,	batch  1610,	training loss: 1.093
epoch 10,	batch  1620,	training loss: 1.050
epoch 10,	batch  1630,	training loss: 1.155
epoch 10,	batch  1640,	training loss: 1.111
epoch 10,	batch  1650,	training loss: 1.134
epoch 10,	batch  1660,	training loss: 1.054
epoch 10,	batch  1670,	training loss: 1.051
epoch 10,	batch  1680,	training loss: 1.150
epoch 10,	batch  1690,	training loss: 1.067
epoch 10,	batch  1700,	training loss: 1.131
epoch 10,	batch  1710,	training loss: 1.138
epoch 10,	batch  1720,	training loss: 1.096
epoch 10,	batch  1730,	training loss: 1.120
epoch 10,	batch  1740,	training loss: 1.051
epoch 10,	batch  1750,	training loss: 1.113
epoch 10,	batch  1760,	training loss: 1.079
epoch 10,	batch  1770,	training loss: 1.128
epoch 10,	batch  1780,	training loss: 1.191
epoch 10,	batch  1790,	training loss: 1.172
epoch 10,	batch  1800,	training loss: 1.142
epoch 10,	batch  1810,	training loss: 1.034
epoch 10,	batch  1820,	training loss: 1.153
epoch 10,	batch  1830,	training loss: 1.080
epoch 10,	batch  1840,	training loss: 0.936
epoch 10,	batch  1850,	training loss: 1.061
epoch 10,	batch  1860,	training loss: 1.177
epoch 10,	batch  1870,	training loss: 1.105
epoch 10,	batch  1880,	training loss: 1.060
epoch 10,	batch  1890,	training loss: 1.012
epoch 10,	batch  1900,	training loss: 1.021
epoch 10,	batch  1910,	training loss: 1.129
epoch 10,	batch  1920,	training loss: 1.139
epoch 10,	batch  1930,	training loss: 1.245
epoch 10,	batch  1940,	training loss: 1.158
epoch 10,	batch  1950,	training loss: 1.115
epoch 10,	batch  1960,	training loss: 1.157
epoch 10,	batch  1970,	training loss: 1.061
epoch 10,	batch  1980,	training loss: 1.148
END OF EPOCH 10
Testing on validation set...
# correct:	6610/54904 = 12.03919568701734%
# off by 1:	11180/54904 = 20.362815095439313%

epoch 11,	batch    10,	training loss: 0.671
epoch 11,	batch    20,	training loss: 0.640
epoch 11,	batch    30,	training loss: 0.693
epoch 11,	batch    40,	training loss: 0.736
epoch 11,	batch    50,	training loss: 0.706
epoch 11,	batch    60,	training loss: 0.695
epoch 11,	batch    70,	training loss: 0.642
epoch 11,	batch    80,	training loss: 0.529
epoch 11,	batch    90,	training loss: 0.706
epoch 11,	batch   100,	training loss: 0.729
epoch 11,	batch   110,	training loss: 0.676
epoch 11,	batch   120,	training loss: 0.667
epoch 11,	batch   130,	training loss: 0.743
epoch 11,	batch   140,	training loss: 0.678
epoch 11,	batch   150,	training loss: 0.788
epoch 11,	batch   160,	training loss: 0.679
epoch 11,	batch   170,	training loss: 0.682
epoch 11,	batch   180,	training loss: 0.700
epoch 11,	batch   190,	training loss: 0.724
epoch 11,	batch   200,	training loss: 0.658
epoch 11,	batch   210,	training loss: 0.621
epoch 11,	batch   220,	training loss: 0.628
epoch 11,	batch   230,	training loss: 0.624
epoch 11,	batch   240,	training loss: 0.669
epoch 11,	batch   250,	training loss: 0.773
epoch 11,	batch   260,	training loss: 0.686
epoch 11,	batch   270,	training loss: 0.616
epoch 11,	batch   280,	training loss: 0.696
epoch 11,	batch   290,	training loss: 0.727
epoch 11,	batch   300,	training loss: 0.626
epoch 11,	batch   310,	training loss: 0.701
epoch 11,	batch   320,	training loss: 0.654
epoch 11,	batch   330,	training loss: 0.683
epoch 11,	batch   340,	training loss: 0.694
epoch 11,	batch   350,	training loss: 0.658
epoch 11,	batch   360,	training loss: 0.726
epoch 11,	batch   370,	training loss: 0.675
epoch 11,	batch   380,	training loss: 0.713
epoch 11,	batch   390,	training loss: 0.734
epoch 11,	batch   400,	training loss: 0.737
epoch 11,	batch   410,	training loss: 0.681
epoch 11,	batch   420,	training loss: 0.719
epoch 11,	batch   430,	training loss: 0.735
epoch 11,	batch   440,	training loss: 0.712
epoch 11,	batch   450,	training loss: 0.689
epoch 11,	batch   460,	training loss: 0.779
epoch 11,	batch   470,	training loss: 0.677
epoch 11,	batch   480,	training loss: 0.738
epoch 11,	batch   490,	training loss: 0.672
epoch 11,	batch   500,	training loss: 0.760
epoch 11,	batch   510,	training loss: 0.706
epoch 11,	batch   520,	training loss: 0.652
epoch 11,	batch   530,	training loss: 0.656
epoch 11,	batch   540,	training loss: 0.776
epoch 11,	batch   550,	training loss: 0.771
epoch 11,	batch   560,	training loss: 0.790
epoch 11,	batch   570,	training loss: 0.661
epoch 11,	batch   580,	training loss: 0.736
epoch 11,	batch   590,	training loss: 0.779
epoch 11,	batch   600,	training loss: 0.769
epoch 11,	batch   610,	training loss: 0.721
epoch 11,	batch   620,	training loss: 0.699
epoch 11,	batch   630,	training loss: 0.743
epoch 11,	batch   640,	training loss: 0.688
epoch 11,	batch   650,	training loss: 0.757
epoch 11,	batch   660,	training loss: 0.729
epoch 11,	batch   670,	training loss: 0.777
epoch 11,	batch   680,	training loss: 0.719
epoch 11,	batch   690,	training loss: 0.772
epoch 11,	batch   700,	training loss: 0.772
epoch 11,	batch   710,	training loss: 0.793
epoch 11,	batch   720,	training loss: 0.715
epoch 11,	batch   730,	training loss: 0.797
epoch 11,	batch   740,	training loss: 0.744
epoch 11,	batch   750,	training loss: 0.682
epoch 11,	batch   760,	training loss: 0.732
epoch 11,	batch   770,	training loss: 0.768
epoch 11,	batch   780,	training loss: 0.818
epoch 11,	batch   790,	training loss: 0.814
epoch 11,	batch   800,	training loss: 0.786
epoch 11,	batch   810,	training loss: 0.750
epoch 11,	batch   820,	training loss: 0.772
epoch 11,	batch   830,	training loss: 0.781
epoch 11,	batch   840,	training loss: 0.824
epoch 11,	batch   850,	training loss: 0.792
epoch 11,	batch   860,	training loss: 0.812
epoch 11,	batch   870,	training loss: 0.878
epoch 11,	batch   880,	training loss: 0.809
epoch 11,	batch   890,	training loss: 0.884
epoch 11,	batch   900,	training loss: 0.856
epoch 11,	batch   910,	training loss: 0.750
epoch 11,	batch   920,	training loss: 0.769
epoch 11,	batch   930,	training loss: 0.811
epoch 11,	batch   940,	training loss: 0.720
epoch 11,	batch   950,	training loss: 0.825
epoch 11,	batch   960,	training loss: 0.831
epoch 11,	batch   970,	training loss: 0.863
epoch 11,	batch   980,	training loss: 0.759
epoch 11,	batch   990,	training loss: 0.793
epoch 11,	batch  1000,	training loss: 0.785
epoch 11,	batch  1010,	training loss: 0.855
epoch 11,	batch  1020,	training loss: 0.789
epoch 11,	batch  1030,	training loss: 0.826
epoch 11,	batch  1040,	training loss: 0.711
epoch 11,	batch  1050,	training loss: 0.849
epoch 11,	batch  1060,	training loss: 0.798
epoch 11,	batch  1070,	training loss: 0.834
epoch 11,	batch  1080,	training loss: 0.849
epoch 11,	batch  1090,	training loss: 0.749
epoch 11,	batch  1100,	training loss: 0.811
epoch 11,	batch  1110,	training loss: 0.825
epoch 11,	batch  1120,	training loss: 0.830
epoch 11,	batch  1130,	training loss: 0.909
epoch 11,	batch  1140,	training loss: 0.882
epoch 11,	batch  1150,	training loss: 0.811
epoch 11,	batch  1160,	training loss: 0.801
epoch 11,	batch  1170,	training loss: 0.767
epoch 11,	batch  1180,	training loss: 0.778
epoch 11,	batch  1190,	training loss: 0.834
epoch 11,	batch  1200,	training loss: 0.757
epoch 11,	batch  1210,	training loss: 0.776
epoch 11,	batch  1220,	training loss: 0.761
epoch 11,	batch  1230,	training loss: 0.840
epoch 11,	batch  1240,	training loss: 0.828
epoch 11,	batch  1250,	training loss: 0.816
epoch 11,	batch  1260,	training loss: 0.923
epoch 11,	batch  1270,	training loss: 0.915
epoch 11,	batch  1280,	training loss: 0.986
epoch 11,	batch  1290,	training loss: 0.931
epoch 11,	batch  1300,	training loss: 0.866
epoch 11,	batch  1310,	training loss: 0.891
epoch 11,	batch  1320,	training loss: 0.797
epoch 11,	batch  1330,	training loss: 0.896
epoch 11,	batch  1340,	training loss: 0.855
epoch 11,	batch  1350,	training loss: 0.874
epoch 11,	batch  1360,	training loss: 0.934
epoch 11,	batch  1370,	training loss: 0.978
epoch 11,	batch  1380,	training loss: 0.908
epoch 11,	batch  1390,	training loss: 0.770
epoch 11,	batch  1400,	training loss: 0.851
epoch 11,	batch  1410,	training loss: 0.847
epoch 11,	batch  1420,	training loss: 0.807
epoch 11,	batch  1430,	training loss: 0.847
epoch 11,	batch  1440,	training loss: 0.813
epoch 11,	batch  1450,	training loss: 0.771
epoch 11,	batch  1460,	training loss: 0.890
epoch 11,	batch  1470,	training loss: 0.822
epoch 11,	batch  1480,	training loss: 0.905
epoch 11,	batch  1490,	training loss: 0.885
epoch 11,	batch  1500,	training loss: 0.828
epoch 11,	batch  1510,	training loss: 0.889
epoch 11,	batch  1520,	training loss: 0.931
epoch 11,	batch  1530,	training loss: 0.822
epoch 11,	batch  1540,	training loss: 0.961
epoch 11,	batch  1550,	training loss: 0.979
epoch 11,	batch  1560,	training loss: 0.923
epoch 11,	batch  1570,	training loss: 0.879
epoch 11,	batch  1580,	training loss: 0.918
epoch 11,	batch  1590,	training loss: 1.022
epoch 11,	batch  1600,	training loss: 0.871
epoch 11,	batch  1610,	training loss: 0.828
epoch 11,	batch  1620,	training loss: 0.955
epoch 11,	batch  1630,	training loss: 0.911
epoch 11,	batch  1640,	training loss: 0.927
epoch 11,	batch  1650,	training loss: 0.870
epoch 11,	batch  1660,	training loss: 0.905
epoch 11,	batch  1670,	training loss: 0.957
epoch 11,	batch  1680,	training loss: 0.939
epoch 11,	batch  1690,	training loss: 0.934
epoch 11,	batch  1700,	training loss: 0.824
epoch 11,	batch  1710,	training loss: 0.982
epoch 11,	batch  1720,	training loss: 0.826
epoch 11,	batch  1730,	training loss: 0.965
epoch 11,	batch  1740,	training loss: 0.872
epoch 11,	batch  1750,	training loss: 0.837
epoch 11,	batch  1760,	training loss: 0.944
epoch 11,	batch  1770,	training loss: 0.949
epoch 11,	batch  1780,	training loss: 0.985
epoch 11,	batch  1790,	training loss: 0.903
epoch 11,	batch  1800,	training loss: 1.034
epoch 11,	batch  1810,	training loss: 0.903
epoch 11,	batch  1820,	training loss: 0.847
epoch 11,	batch  1830,	training loss: 0.982
epoch 11,	batch  1840,	training loss: 1.018
epoch 11,	batch  1850,	training loss: 0.891
epoch 11,	batch  1860,	training loss: 0.921
epoch 11,	batch  1870,	training loss: 0.935
epoch 11,	batch  1880,	training loss: 0.923
epoch 11,	batch  1890,	training loss: 0.904
epoch 11,	batch  1900,	training loss: 0.912
epoch 11,	batch  1910,	training loss: 0.936
epoch 11,	batch  1920,	training loss: 0.927
epoch 11,	batch  1930,	training loss: 0.867
epoch 11,	batch  1940,	training loss: 0.959
epoch 11,	batch  1950,	training loss: 0.852
epoch 11,	batch  1960,	training loss: 0.795
epoch 11,	batch  1970,	training loss: 0.980
epoch 11,	batch  1980,	training loss: 0.859
END OF EPOCH 11
Testing on validation set...
# correct:	5706/54904 = 10.392685414541745%
# off by 1:	10199/54904 = 18.576060032055953%

epoch 12,	batch    10,	training loss: 0.670
epoch 12,	batch    20,	training loss: 0.655
epoch 12,	batch    30,	training loss: 0.663
epoch 12,	batch    40,	training loss: 0.611
epoch 12,	batch    50,	training loss: 0.622
epoch 12,	batch    60,	training loss: 0.513
epoch 12,	batch    70,	training loss: 0.604
epoch 12,	batch    80,	training loss: 0.535
epoch 12,	batch    90,	training loss: 0.636
epoch 12,	batch   100,	training loss: 0.553
epoch 12,	batch   110,	training loss: 0.530
epoch 12,	batch   120,	training loss: 0.556
epoch 12,	batch   130,	training loss: 0.526
epoch 12,	batch   140,	training loss: 0.580
epoch 12,	batch   150,	training loss: 0.515
epoch 12,	batch   160,	training loss: 0.552
epoch 12,	batch   170,	training loss: 0.577
epoch 12,	batch   180,	training loss: 0.534
epoch 12,	batch   190,	training loss: 0.539
epoch 12,	batch   200,	training loss: 0.492
epoch 12,	batch   210,	training loss: 0.584
epoch 12,	batch   220,	training loss: 0.535
epoch 12,	batch   230,	training loss: 0.584
epoch 12,	batch   240,	training loss: 0.503
epoch 12,	batch   250,	training loss: 0.549
epoch 12,	batch   260,	training loss: 0.566
epoch 12,	batch   270,	training loss: 0.507
epoch 12,	batch   280,	training loss: 0.586
epoch 12,	batch   290,	training loss: 0.577
epoch 12,	batch   300,	training loss: 0.584
epoch 12,	batch   310,	training loss: 0.608
epoch 12,	batch   320,	training loss: 0.547
epoch 12,	batch   330,	training loss: 0.662
epoch 12,	batch   340,	training loss: 0.565
epoch 12,	batch   350,	training loss: 0.580
epoch 12,	batch   360,	training loss: 0.548
epoch 12,	batch   370,	training loss: 0.522
epoch 12,	batch   380,	training loss: 0.565
epoch 12,	batch   390,	training loss: 0.598
epoch 12,	batch   400,	training loss: 0.515
epoch 12,	batch   410,	training loss: 0.571
epoch 12,	batch   420,	training loss: 0.554
epoch 12,	batch   430,	training loss: 0.596
epoch 12,	batch   440,	training loss: 0.658
epoch 12,	batch   450,	training loss: 0.617
epoch 12,	batch   460,	training loss: 0.621
epoch 12,	batch   470,	training loss: 0.561
epoch 12,	batch   480,	training loss: 0.591
epoch 12,	batch   490,	training loss: 0.636
epoch 12,	batch   500,	training loss: 0.567
epoch 12,	batch   510,	training loss: 0.562
epoch 12,	batch   520,	training loss: 0.555
epoch 12,	batch   530,	training loss: 0.597
epoch 12,	batch   540,	training loss: 0.656
epoch 12,	batch   550,	training loss: 0.688
epoch 12,	batch   560,	training loss: 0.607
epoch 12,	batch   570,	training loss: 0.613
epoch 12,	batch   580,	training loss: 0.550
epoch 12,	batch   590,	training loss: 0.618
epoch 12,	batch   600,	training loss: 0.502
epoch 12,	batch   610,	training loss: 0.607
epoch 12,	batch   620,	training loss: 0.566
epoch 12,	batch   630,	training loss: 0.633
epoch 12,	batch   640,	training loss: 0.631
epoch 12,	batch   650,	training loss: 0.578
epoch 12,	batch   660,	training loss: 0.660
epoch 12,	batch   670,	training loss: 0.690
epoch 12,	batch   680,	training loss: 0.624
epoch 12,	batch   690,	training loss: 0.681
epoch 12,	batch   700,	training loss: 0.590
epoch 12,	batch   710,	training loss: 0.612
epoch 12,	batch   720,	training loss: 0.597
epoch 12,	batch   730,	training loss: 0.664
epoch 12,	batch   740,	training loss: 0.626
epoch 12,	batch   750,	training loss: 0.680
epoch 12,	batch   760,	training loss: 0.643
epoch 12,	batch   770,	training loss: 0.632
epoch 12,	batch   780,	training loss: 0.656
epoch 12,	batch   790,	training loss: 0.627
epoch 12,	batch   800,	training loss: 0.572
epoch 12,	batch   810,	training loss: 0.589
epoch 12,	batch   820,	training loss: 0.614
epoch 12,	batch   830,	training loss: 0.621
epoch 12,	batch   840,	training loss: 0.700
epoch 12,	batch   850,	training loss: 0.683
epoch 12,	batch   860,	training loss: 0.642
epoch 12,	batch   870,	training loss: 0.638
epoch 12,	batch   880,	training loss: 0.690
epoch 12,	batch   890,	training loss: 0.692
epoch 12,	batch   900,	training loss: 0.637
epoch 12,	batch   910,	training loss: 0.655
epoch 12,	batch   920,	training loss: 0.600
epoch 12,	batch   930,	training loss: 0.701
epoch 12,	batch   940,	training loss: 0.687
epoch 12,	batch   950,	training loss: 0.598
epoch 12,	batch   960,	training loss: 0.687
epoch 12,	batch   970,	training loss: 0.598
epoch 12,	batch   980,	training loss: 0.604
epoch 12,	batch   990,	training loss: 0.623
epoch 12,	batch  1000,	training loss: 0.654
epoch 12,	batch  1010,	training loss: 0.679
epoch 12,	batch  1020,	training loss: 0.624
epoch 12,	batch  1030,	training loss: 0.603
epoch 12,	batch  1040,	training loss: 0.632
epoch 12,	batch  1050,	training loss: 0.676
epoch 12,	batch  1060,	training loss: 0.684
epoch 12,	batch  1070,	training loss: 0.682
epoch 12,	batch  1080,	training loss: 0.606
epoch 12,	batch  1090,	training loss: 0.624
epoch 12,	batch  1100,	training loss: 0.752
epoch 12,	batch  1110,	training loss: 0.653
epoch 12,	batch  1120,	training loss: 0.724
epoch 12,	batch  1130,	training loss: 0.726
epoch 12,	batch  1140,	training loss: 0.685
epoch 12,	batch  1150,	training loss: 0.672
epoch 12,	batch  1160,	training loss: 0.592
epoch 12,	batch  1170,	training loss: 0.699
epoch 12,	batch  1180,	training loss: 0.606
epoch 12,	batch  1190,	training loss: 0.670
epoch 12,	batch  1200,	training loss: 0.732
epoch 12,	batch  1210,	training loss: 0.654
epoch 12,	batch  1220,	training loss: 0.696
epoch 12,	batch  1230,	training loss: 0.743
epoch 12,	batch  1240,	training loss: 0.772
epoch 12,	batch  1250,	training loss: 0.687
epoch 12,	batch  1260,	training loss: 0.637
epoch 12,	batch  1270,	training loss: 0.683
epoch 12,	batch  1280,	training loss: 0.657
epoch 12,	batch  1290,	training loss: 0.713
epoch 12,	batch  1300,	training loss: 0.689
epoch 12,	batch  1310,	training loss: 0.711
epoch 12,	batch  1320,	training loss: 0.627
epoch 12,	batch  1330,	training loss: 0.739
epoch 12,	batch  1340,	training loss: 0.725
epoch 12,	batch  1350,	training loss: 0.758
epoch 12,	batch  1360,	training loss: 0.706
epoch 12,	batch  1370,	training loss: 0.663
epoch 12,	batch  1380,	training loss: 0.694
epoch 12,	batch  1390,	training loss: 0.633
epoch 12,	batch  1400,	training loss: 0.808
epoch 12,	batch  1410,	training loss: 0.723
epoch 12,	batch  1420,	training loss: 0.747
epoch 12,	batch  1430,	training loss: 0.702
epoch 12,	batch  1440,	training loss: 0.706
epoch 12,	batch  1450,	training loss: 0.662
epoch 12,	batch  1460,	training loss: 0.693
epoch 12,	batch  1470,	training loss: 0.705
epoch 12,	batch  1480,	training loss: 0.694
epoch 12,	batch  1490,	training loss: 0.676
epoch 12,	batch  1500,	training loss: 0.691
epoch 12,	batch  1510,	training loss: 0.665
epoch 12,	batch  1520,	training loss: 0.785
epoch 12,	batch  1530,	training loss: 0.736
epoch 12,	batch  1540,	training loss: 0.609
epoch 12,	batch  1550,	training loss: 0.724
epoch 12,	batch  1560,	training loss: 0.662
epoch 12,	batch  1570,	training loss: 0.767
epoch 12,	batch  1580,	training loss: 0.653
epoch 12,	batch  1590,	training loss: 0.678
epoch 12,	batch  1600,	training loss: 0.626
epoch 12,	batch  1610,	training loss: 0.761
epoch 12,	batch  1620,	training loss: 0.688
epoch 12,	batch  1630,	training loss: 0.769
epoch 12,	batch  1640,	training loss: 0.718
epoch 12,	batch  1650,	training loss: 0.664
epoch 12,	batch  1660,	training loss: 0.710
epoch 12,	batch  1670,	training loss: 0.689
epoch 12,	batch  1680,	training loss: 0.778
epoch 12,	batch  1690,	training loss: 0.774
epoch 12,	batch  1700,	training loss: 0.808
epoch 12,	batch  1710,	training loss: 0.797
epoch 12,	batch  1720,	training loss: 0.787
epoch 12,	batch  1730,	training loss: 0.763
epoch 12,	batch  1740,	training loss: 0.720
epoch 12,	batch  1750,	training loss: 0.744
epoch 12,	batch  1760,	training loss: 0.781
epoch 12,	batch  1770,	training loss: 0.797
epoch 12,	batch  1780,	training loss: 0.724
epoch 12,	batch  1790,	training loss: 0.722
epoch 12,	batch  1800,	training loss: 0.737
epoch 12,	batch  1810,	training loss: 0.770
epoch 12,	batch  1820,	training loss: 0.687
epoch 12,	batch  1830,	training loss: 0.702
epoch 12,	batch  1840,	training loss: 0.762
epoch 12,	batch  1850,	training loss: 0.749
epoch 12,	batch  1860,	training loss: 0.700
epoch 12,	batch  1870,	training loss: 0.752
epoch 12,	batch  1880,	training loss: 0.730
epoch 12,	batch  1890,	training loss: 0.802
epoch 12,	batch  1900,	training loss: 0.793
epoch 12,	batch  1910,	training loss: 0.796
epoch 12,	batch  1920,	training loss: 0.776
epoch 12,	batch  1930,	training loss: 0.871
epoch 12,	batch  1940,	training loss: 0.760
epoch 12,	batch  1950,	training loss: 0.758
epoch 12,	batch  1960,	training loss: 0.711
epoch 12,	batch  1970,	training loss: 0.762
epoch 12,	batch  1980,	training loss: 0.830
END OF EPOCH 12
Testing on validation set...
# correct:	5813/54904 = 10.587571033075914%
# off by 1:	10236/54904 = 18.643450386128514%

epoch 13,	batch    10,	training loss: 0.564
epoch 13,	batch    20,	training loss: 0.556
epoch 13,	batch    30,	training loss: 0.535
epoch 13,	batch    40,	training loss: 0.498
epoch 13,	batch    50,	training loss: 0.480
epoch 13,	batch    60,	training loss: 0.469
epoch 13,	batch    70,	training loss: 0.461
epoch 13,	batch    80,	training loss: 0.530
epoch 13,	batch    90,	training loss: 0.475
epoch 13,	batch   100,	training loss: 0.475
epoch 13,	batch   110,	training loss: 0.582
epoch 13,	batch   120,	training loss: 0.394
epoch 13,	batch   130,	training loss: 0.423
epoch 13,	batch   140,	training loss: 0.490
epoch 13,	batch   150,	training loss: 0.507
epoch 13,	batch   160,	training loss: 0.484
epoch 13,	batch   170,	training loss: 0.476
epoch 13,	batch   180,	training loss: 0.446
epoch 13,	batch   190,	training loss: 0.475
epoch 13,	batch   200,	training loss: 0.514
epoch 13,	batch   210,	training loss: 0.520
epoch 13,	batch   220,	training loss: 0.438
epoch 13,	batch   230,	training loss: 0.527
epoch 13,	batch   240,	training loss: 0.435
epoch 13,	batch   250,	training loss: 0.434
epoch 13,	batch   260,	training loss: 0.440
epoch 13,	batch   270,	training loss: 0.456
epoch 13,	batch   280,	training loss: 0.466
epoch 13,	batch   290,	training loss: 0.527
epoch 13,	batch   300,	training loss: 0.574
epoch 13,	batch   310,	training loss: 0.501
epoch 13,	batch   320,	training loss: 0.425
epoch 13,	batch   330,	training loss: 0.493
epoch 13,	batch   340,	training loss: 0.523
epoch 13,	batch   350,	training loss: 0.500
epoch 13,	batch   360,	training loss: 0.474
epoch 13,	batch   370,	training loss: 0.487
epoch 13,	batch   380,	training loss: 0.430
epoch 13,	batch   390,	training loss: 0.525
epoch 13,	batch   400,	training loss: 0.419
epoch 13,	batch   410,	training loss: 0.446
epoch 13,	batch   420,	training loss: 0.405
epoch 13,	batch   430,	training loss: 0.444
epoch 13,	batch   440,	training loss: 0.459
epoch 13,	batch   450,	training loss: 0.546
epoch 13,	batch   460,	training loss: 0.518
epoch 13,	batch   470,	training loss: 0.470
epoch 13,	batch   480,	training loss: 0.458
epoch 13,	batch   490,	training loss: 0.456
epoch 13,	batch   500,	training loss: 0.445
epoch 13,	batch   510,	training loss: 0.440
epoch 13,	batch   520,	training loss: 0.484
epoch 13,	batch   530,	training loss: 0.470
epoch 13,	batch   540,	training loss: 0.519
epoch 13,	batch   550,	training loss: 0.495
epoch 13,	batch   560,	training loss: 0.510
epoch 13,	batch   570,	training loss: 0.491
epoch 13,	batch   580,	training loss: 0.402
epoch 13,	batch   590,	training loss: 0.507
epoch 13,	batch   600,	training loss: 0.472
epoch 13,	batch   610,	training loss: 0.560
epoch 13,	batch   620,	training loss: 0.505
epoch 13,	batch   630,	training loss: 0.516
epoch 13,	batch   640,	training loss: 0.497
epoch 13,	batch   650,	training loss: 0.542
epoch 13,	batch   660,	training loss: 0.519
epoch 13,	batch   670,	training loss: 0.494
epoch 13,	batch   680,	training loss: 0.544
epoch 13,	batch   690,	training loss: 0.506
epoch 13,	batch   700,	training loss: 0.538
epoch 13,	batch   710,	training loss: 0.536
epoch 13,	batch   720,	training loss: 0.530
epoch 13,	batch   730,	training loss: 0.486
epoch 13,	batch   740,	training loss: 0.467
epoch 13,	batch   750,	training loss: 0.475
epoch 13,	batch   760,	training loss: 0.603
epoch 13,	batch   770,	training loss: 0.567
epoch 13,	batch   780,	training loss: 0.554
epoch 13,	batch   790,	training loss: 0.489
epoch 13,	batch   800,	training loss: 0.537
epoch 13,	batch   810,	training loss: 0.616
epoch 13,	batch   820,	training loss: 0.518
epoch 13,	batch   830,	training loss: 0.530
epoch 13,	batch   840,	training loss: 0.563
epoch 13,	batch   850,	training loss: 0.544
epoch 13,	batch   860,	training loss: 0.560
epoch 13,	batch   870,	training loss: 0.541
epoch 13,	batch   880,	training loss: 0.603
epoch 13,	batch   890,	training loss: 0.560
epoch 13,	batch   900,	training loss: 0.497
epoch 13,	batch   910,	training loss: 0.569
epoch 13,	batch   920,	training loss: 0.598
epoch 13,	batch   930,	training loss: 0.596
epoch 13,	batch   940,	training loss: 0.632
epoch 13,	batch   950,	training loss: 0.581
epoch 13,	batch   960,	training loss: 0.542
epoch 13,	batch   970,	training loss: 0.575
epoch 13,	batch   980,	training loss: 0.516
epoch 13,	batch   990,	training loss: 0.549
epoch 13,	batch  1000,	training loss: 0.574
epoch 13,	batch  1010,	training loss: 0.457
epoch 13,	batch  1020,	training loss: 0.500
epoch 13,	batch  1030,	training loss: 0.545
epoch 13,	batch  1040,	training loss: 0.653
epoch 13,	batch  1050,	training loss: 0.621
epoch 13,	batch  1060,	training loss: 0.538
epoch 13,	batch  1070,	training loss: 0.548
epoch 13,	batch  1080,	training loss: 0.522
epoch 13,	batch  1090,	training loss: 0.527
epoch 13,	batch  1100,	training loss: 0.536
epoch 13,	batch  1110,	training loss: 0.603
epoch 13,	batch  1120,	training loss: 0.496
epoch 13,	batch  1130,	training loss: 0.635
epoch 13,	batch  1140,	training loss: 0.520
epoch 13,	batch  1150,	training loss: 0.582
epoch 13,	batch  1160,	training loss: 0.588
epoch 13,	batch  1170,	training loss: 0.646
epoch 13,	batch  1180,	training loss: 0.479
epoch 13,	batch  1190,	training loss: 0.542
epoch 13,	batch  1200,	training loss: 0.504
epoch 13,	batch  1210,	training loss: 0.591
epoch 13,	batch  1220,	training loss: 0.574
epoch 13,	batch  1230,	training loss: 0.514
epoch 13,	batch  1240,	training loss: 0.602
epoch 13,	batch  1250,	training loss: 0.445
epoch 13,	batch  1260,	training loss: 0.580
epoch 13,	batch  1270,	training loss: 0.567
epoch 13,	batch  1280,	training loss: 0.585
epoch 13,	batch  1290,	training loss: 0.579
epoch 13,	batch  1300,	training loss: 0.558
epoch 13,	batch  1310,	training loss: 0.596
epoch 13,	batch  1320,	training loss: 0.577
epoch 13,	batch  1330,	training loss: 0.537
epoch 13,	batch  1340,	training loss: 0.508
epoch 13,	batch  1350,	training loss: 0.574
epoch 13,	batch  1360,	training loss: 0.583
epoch 13,	batch  1370,	training loss: 0.618
epoch 13,	batch  1380,	training loss: 0.541
epoch 13,	batch  1390,	training loss: 0.535
epoch 13,	batch  1400,	training loss: 0.611
epoch 13,	batch  1410,	training loss: 0.566
epoch 13,	batch  1420,	training loss: 0.612
epoch 13,	batch  1430,	training loss: 0.570
epoch 13,	batch  1440,	training loss: 0.628
epoch 13,	batch  1450,	training loss: 0.653
epoch 13,	batch  1460,	training loss: 0.569
epoch 13,	batch  1470,	training loss: 0.541
epoch 13,	batch  1480,	training loss: 0.604
epoch 13,	batch  1490,	training loss: 0.522
epoch 13,	batch  1500,	training loss: 0.611
epoch 13,	batch  1510,	training loss: 0.599
epoch 13,	batch  1520,	training loss: 0.539
epoch 13,	batch  1530,	training loss: 0.545
epoch 13,	batch  1540,	training loss: 0.532
epoch 13,	batch  1550,	training loss: 0.558
epoch 13,	batch  1560,	training loss: 0.604
epoch 13,	batch  1570,	training loss: 0.648
epoch 13,	batch  1580,	training loss: 0.608
epoch 13,	batch  1590,	training loss: 0.568
epoch 13,	batch  1600,	training loss: 0.629
epoch 13,	batch  1610,	training loss: 0.606
epoch 13,	batch  1620,	training loss: 0.599
epoch 13,	batch  1630,	training loss: 0.581
epoch 13,	batch  1640,	training loss: 0.557
epoch 13,	batch  1650,	training loss: 0.570
epoch 13,	batch  1660,	training loss: 0.589
epoch 13,	batch  1670,	training loss: 0.600
epoch 13,	batch  1680,	training loss: 0.605
epoch 13,	batch  1690,	training loss: 0.635
epoch 13,	batch  1700,	training loss: 0.609
epoch 13,	batch  1710,	training loss: 0.615
epoch 13,	batch  1720,	training loss: 0.637
epoch 13,	batch  1730,	training loss: 0.518
epoch 13,	batch  1740,	training loss: 0.594
epoch 13,	batch  1750,	training loss: 0.641
epoch 13,	batch  1760,	training loss: 0.643
epoch 13,	batch  1770,	training loss: 0.618
epoch 13,	batch  1780,	training loss: 0.645
epoch 13,	batch  1790,	training loss: 0.587
epoch 13,	batch  1800,	training loss: 0.584
epoch 13,	batch  1810,	training loss: 0.578
epoch 13,	batch  1820,	training loss: 0.695
epoch 13,	batch  1830,	training loss: 0.696
epoch 13,	batch  1840,	training loss: 0.685
epoch 13,	batch  1850,	training loss: 0.614
epoch 13,	batch  1860,	training loss: 0.592
epoch 13,	batch  1870,	training loss: 0.667
epoch 13,	batch  1880,	training loss: 0.676
epoch 13,	batch  1890,	training loss: 0.688
epoch 13,	batch  1900,	training loss: 0.642
epoch 13,	batch  1910,	training loss: 0.768
epoch 13,	batch  1920,	training loss: 0.727
epoch 13,	batch  1930,	training loss: 0.718
epoch 13,	batch  1940,	training loss: 0.567
epoch 13,	batch  1950,	training loss: 0.584
epoch 13,	batch  1960,	training loss: 0.592
epoch 13,	batch  1970,	training loss: 0.628
epoch 13,	batch  1980,	training loss: 0.604
END OF EPOCH 13
Testing on validation set...
# correct:	6247/54904 = 11.37804167273787%
# off by 1:	11315/54904 = 20.608698819758125%

epoch 14,	batch    10,	training loss: 0.398
epoch 14,	batch    20,	training loss: 0.374
epoch 14,	batch    30,	training loss: 0.359
epoch 14,	batch    40,	training loss: 0.416
epoch 14,	batch    50,	training loss: 0.331
epoch 14,	batch    60,	training loss: 0.338
epoch 14,	batch    70,	training loss: 0.359
epoch 14,	batch    80,	training loss: 0.358
epoch 14,	batch    90,	training loss: 0.369
epoch 14,	batch   100,	training loss: 0.335
epoch 14,	batch   110,	training loss: 0.375
epoch 14,	batch   120,	training loss: 0.424
epoch 14,	batch   130,	training loss: 0.374
epoch 14,	batch   140,	training loss: 0.447
epoch 14,	batch   150,	training loss: 0.373
epoch 14,	batch   160,	training loss: 0.337
epoch 14,	batch   170,	training loss: 0.416
epoch 14,	batch   180,	training loss: 0.387
epoch 14,	batch   190,	training loss: 0.334
epoch 14,	batch   200,	training loss: 0.389
epoch 14,	batch   210,	training loss: 0.442
epoch 14,	batch   220,	training loss: 0.435
epoch 14,	batch   230,	training loss: 0.374
epoch 14,	batch   240,	training loss: 0.424
epoch 14,	batch   250,	training loss: 0.416
epoch 14,	batch   260,	training loss: 0.388
epoch 14,	batch   270,	training loss: 0.397
epoch 14,	batch   280,	training loss: 0.371
epoch 14,	batch   290,	training loss: 0.375
epoch 14,	batch   300,	training loss: 0.357
epoch 14,	batch   310,	training loss: 0.388
epoch 14,	batch   320,	training loss: 0.314
epoch 14,	batch   330,	training loss: 0.381
epoch 14,	batch   340,	training loss: 0.362
epoch 14,	batch   350,	training loss: 0.387
epoch 14,	batch   360,	training loss: 0.428
epoch 14,	batch   370,	training loss: 0.369
epoch 14,	batch   380,	training loss: 0.348
epoch 14,	batch   390,	training loss: 0.393
epoch 14,	batch   400,	training loss: 0.441
epoch 14,	batch   410,	training loss: 0.403
epoch 14,	batch   420,	training loss: 0.523
epoch 14,	batch   430,	training loss: 0.359
epoch 14,	batch   440,	training loss: 0.428
epoch 14,	batch   450,	training loss: 0.401
epoch 14,	batch   460,	training loss: 0.441
epoch 14,	batch   470,	training loss: 0.422
epoch 14,	batch   480,	training loss: 0.338
epoch 14,	batch   490,	training loss: 0.359
epoch 14,	batch   500,	training loss: 0.395
epoch 14,	batch   510,	training loss: 0.441
epoch 14,	batch   520,	training loss: 0.370
epoch 14,	batch   530,	training loss: 0.397
epoch 14,	batch   540,	training loss: 0.360
epoch 14,	batch   550,	training loss: 0.420
epoch 14,	batch   560,	training loss: 0.401
epoch 14,	batch   570,	training loss: 0.407
epoch 14,	batch   580,	training loss: 0.372
epoch 14,	batch   590,	training loss: 0.406
epoch 14,	batch   600,	training loss: 0.395
epoch 14,	batch   610,	training loss: 0.299
epoch 14,	batch   620,	training loss: 0.429
epoch 14,	batch   630,	training loss: 0.464
epoch 14,	batch   640,	training loss: 0.341
epoch 14,	batch   650,	training loss: 0.437
epoch 14,	batch   660,	training loss: 0.427
epoch 14,	batch   670,	training loss: 0.449
epoch 14,	batch   680,	training loss: 0.457
epoch 14,	batch   690,	training loss: 0.427
epoch 14,	batch   700,	training loss: 0.477
epoch 14,	batch   710,	training loss: 0.506
epoch 14,	batch   720,	training loss: 0.340
epoch 14,	batch   730,	training loss: 0.393
epoch 14,	batch   740,	training loss: 0.413
epoch 14,	batch   750,	training loss: 0.331
epoch 14,	batch   760,	training loss: 0.367
epoch 14,	batch   770,	training loss: 0.523
epoch 14,	batch   780,	training loss: 0.393
epoch 14,	batch   790,	training loss: 0.490
epoch 14,	batch   800,	training loss: 0.403
epoch 14,	batch   810,	training loss: 0.357
epoch 14,	batch   820,	training loss: 0.429
epoch 14,	batch   830,	training loss: 0.398
epoch 14,	batch   840,	training loss: 0.375
epoch 14,	batch   850,	training loss: 0.474
epoch 14,	batch   860,	training loss: 0.407
epoch 14,	batch   870,	training loss: 0.431
epoch 14,	batch   880,	training loss: 0.460
epoch 14,	batch   890,	training loss: 0.411
epoch 14,	batch   900,	training loss: 0.434
epoch 14,	batch   910,	training loss: 0.420
epoch 14,	batch   920,	training loss: 0.467
epoch 14,	batch   930,	training loss: 0.488
epoch 14,	batch   940,	training loss: 0.476
epoch 14,	batch   950,	training loss: 0.449
epoch 14,	batch   960,	training loss: 0.447
epoch 14,	batch   970,	training loss: 0.442
epoch 14,	batch   980,	training loss: 0.438
epoch 14,	batch   990,	training loss: 0.524
epoch 14,	batch  1000,	training loss: 0.444
epoch 14,	batch  1010,	training loss: 0.449
epoch 14,	batch  1020,	training loss: 0.471
epoch 14,	batch  1030,	training loss: 0.483
epoch 14,	batch  1040,	training loss: 0.479
epoch 14,	batch  1050,	training loss: 0.488
epoch 14,	batch  1060,	training loss: 0.424
epoch 14,	batch  1070,	training loss: 0.436
epoch 14,	batch  1080,	training loss: 0.493
epoch 14,	batch  1090,	training loss: 0.449
epoch 14,	batch  1100,	training loss: 0.509
epoch 14,	batch  1110,	training loss: 0.416
epoch 14,	batch  1120,	training loss: 0.525
epoch 14,	batch  1130,	training loss: 0.488
epoch 14,	batch  1140,	training loss: 0.460
epoch 14,	batch  1150,	training loss: 0.472
epoch 14,	batch  1160,	training loss: 0.487
epoch 14,	batch  1170,	training loss: 0.415
epoch 14,	batch  1180,	training loss: 0.438
epoch 14,	batch  1190,	training loss: 0.525
epoch 14,	batch  1200,	training loss: 0.457
epoch 14,	batch  1210,	training loss: 0.625
epoch 14,	batch  1220,	training loss: 0.440
epoch 14,	batch  1230,	training loss: 0.454
epoch 14,	batch  1240,	training loss: 0.482
epoch 14,	batch  1250,	training loss: 0.447
epoch 14,	batch  1260,	training loss: 0.483
epoch 14,	batch  1270,	training loss: 0.425
epoch 14,	batch  1280,	training loss: 0.433
epoch 14,	batch  1290,	training loss: 0.489
epoch 14,	batch  1300,	training loss: 0.519
epoch 14,	batch  1310,	training loss: 0.471
epoch 14,	batch  1320,	training loss: 0.425
epoch 14,	batch  1330,	training loss: 0.465
epoch 14,	batch  1340,	training loss: 0.521
epoch 14,	batch  1350,	training loss: 0.424
epoch 14,	batch  1360,	training loss: 0.407
epoch 14,	batch  1370,	training loss: 0.422
epoch 14,	batch  1380,	training loss: 0.522
epoch 14,	batch  1390,	training loss: 0.455
epoch 14,	batch  1400,	training loss: 0.476
epoch 14,	batch  1410,	training loss: 0.416
epoch 14,	batch  1420,	training loss: 0.558
epoch 14,	batch  1430,	training loss: 0.526
epoch 14,	batch  1440,	training loss: 0.544
epoch 14,	batch  1450,	training loss: 0.494
epoch 14,	batch  1460,	training loss: 0.455
epoch 14,	batch  1470,	training loss: 0.554
epoch 14,	batch  1480,	training loss: 0.476
epoch 14,	batch  1490,	training loss: 0.505
epoch 14,	batch  1500,	training loss: 0.451
epoch 14,	batch  1510,	training loss: 0.541
epoch 14,	batch  1520,	training loss: 0.437
epoch 14,	batch  1530,	training loss: 0.506
epoch 14,	batch  1540,	training loss: 0.388
epoch 14,	batch  1550,	training loss: 0.456
epoch 14,	batch  1560,	training loss: 0.516
epoch 14,	batch  1570,	training loss: 0.548
epoch 14,	batch  1580,	training loss: 0.458
epoch 14,	batch  1590,	training loss: 0.513
epoch 14,	batch  1600,	training loss: 0.554
epoch 14,	batch  1610,	training loss: 0.532
epoch 14,	batch  1620,	training loss: 0.485
epoch 14,	batch  1630,	training loss: 0.489
epoch 14,	batch  1640,	training loss: 0.442
epoch 14,	batch  1650,	training loss: 0.500
epoch 14,	batch  1660,	training loss: 0.461
epoch 14,	batch  1670,	training loss: 0.476
epoch 14,	batch  1680,	training loss: 0.537
epoch 14,	batch  1690,	training loss: 0.604
epoch 14,	batch  1700,	training loss: 0.572
epoch 14,	batch  1710,	training loss: 0.488
epoch 14,	batch  1720,	training loss: 0.516
epoch 14,	batch  1730,	training loss: 0.588
epoch 14,	batch  1740,	training loss: 0.525
epoch 14,	batch  1750,	training loss: 0.543
epoch 14,	batch  1760,	training loss: 0.576
epoch 14,	batch  1770,	training loss: 0.642
epoch 14,	batch  1780,	training loss: 0.539
epoch 14,	batch  1790,	training loss: 0.467
epoch 14,	batch  1800,	training loss: 0.512
epoch 14,	batch  1810,	training loss: 0.516
epoch 14,	batch  1820,	training loss: 0.414
epoch 14,	batch  1830,	training loss: 0.489
epoch 14,	batch  1840,	training loss: 0.656
epoch 14,	batch  1850,	training loss: 0.482
epoch 14,	batch  1860,	training loss: 0.480
epoch 14,	batch  1870,	training loss: 0.511
epoch 14,	batch  1880,	training loss: 0.513
epoch 14,	batch  1890,	training loss: 0.595
epoch 14,	batch  1900,	training loss: 0.468
epoch 14,	batch  1910,	training loss: 0.455
epoch 14,	batch  1920,	training loss: 0.547
epoch 14,	batch  1930,	training loss: 0.520
epoch 14,	batch  1940,	training loss: 0.466
epoch 14,	batch  1950,	training loss: 0.402
epoch 14,	batch  1960,	training loss: 0.494
epoch 14,	batch  1970,	training loss: 0.569
epoch 14,	batch  1980,	training loss: 0.547
END OF EPOCH 14
Testing on validation set...
# correct:	6041/54904 = 11.002841323036574%
# off by 1:	11106/54904 = 20.228034387294187%

epoch 15,	batch    10,	training loss: 0.415
epoch 15,	batch    20,	training loss: 0.428
epoch 15,	batch    30,	training loss: 0.337
epoch 15,	batch    40,	training loss: 0.370
epoch 15,	batch    50,	training loss: 0.314
epoch 15,	batch    60,	training loss: 0.328
epoch 15,	batch    70,	training loss: 0.297
epoch 15,	batch    80,	training loss: 0.301
epoch 15,	batch    90,	training loss: 0.284
epoch 15,	batch   100,	training loss: 0.314
epoch 15,	batch   110,	training loss: 0.278
epoch 15,	batch   120,	training loss: 0.285
epoch 15,	batch   130,	training loss: 0.336
epoch 15,	batch   140,	training loss: 0.250
epoch 15,	batch   150,	training loss: 0.302
epoch 15,	batch   160,	training loss: 0.349
epoch 15,	batch   170,	training loss: 0.330
epoch 15,	batch   180,	training loss: 0.361
epoch 15,	batch   190,	training loss: 0.350
epoch 15,	batch   200,	training loss: 0.307
epoch 15,	batch   210,	training loss: 0.279
epoch 15,	batch   220,	training loss: 0.336
epoch 15,	batch   230,	training loss: 0.299
epoch 15,	batch   240,	training loss: 0.308
epoch 15,	batch   250,	training loss: 0.274
epoch 15,	batch   260,	training loss: 0.319
epoch 15,	batch   270,	training loss: 0.304
epoch 15,	batch   280,	training loss: 0.319
epoch 15,	batch   290,	training loss: 0.348
epoch 15,	batch   300,	training loss: 0.333
epoch 15,	batch   310,	training loss: 0.366
epoch 15,	batch   320,	training loss: 0.284
epoch 15,	batch   330,	training loss: 0.334
epoch 15,	batch   340,	training loss: 0.321
epoch 15,	batch   350,	training loss: 0.326
epoch 15,	batch   360,	training loss: 0.390
epoch 15,	batch   370,	training loss: 0.347
epoch 15,	batch   380,	training loss: 0.309
epoch 15,	batch   390,	training loss: 0.371
epoch 15,	batch   400,	training loss: 0.316
epoch 15,	batch   410,	training loss: 0.336
epoch 15,	batch   420,	training loss: 0.346
epoch 15,	batch   430,	training loss: 0.297
epoch 15,	batch   440,	training loss: 0.378
epoch 15,	batch   450,	training loss: 0.306
epoch 15,	batch   460,	training loss: 0.321
epoch 15,	batch   470,	training loss: 0.355
epoch 15,	batch   480,	training loss: 0.400
epoch 15,	batch   490,	training loss: 0.452
epoch 15,	batch   500,	training loss: 0.331
epoch 15,	batch   510,	training loss: 0.369
epoch 15,	batch   520,	training loss: 0.326
epoch 15,	batch   530,	training loss: 0.333
epoch 15,	batch   540,	training loss: 0.373
epoch 15,	batch   550,	training loss: 0.366
epoch 15,	batch   560,	training loss: 0.333
epoch 15,	batch   570,	training loss: 0.353
epoch 15,	batch   580,	training loss: 0.305
epoch 15,	batch   590,	training loss: 0.417
epoch 15,	batch   600,	training loss: 0.315
epoch 15,	batch   610,	training loss: 0.306
epoch 15,	batch   620,	training loss: 0.429
epoch 15,	batch   630,	training loss: 0.355
epoch 15,	batch   640,	training loss: 0.355
epoch 15,	batch   650,	training loss: 0.354
epoch 15,	batch   660,	training loss: 0.359
epoch 15,	batch   670,	training loss: 0.304
epoch 15,	batch   680,	training loss: 0.314
epoch 15,	batch   690,	training loss: 0.383
epoch 15,	batch   700,	training loss: 0.323
epoch 15,	batch   710,	training loss: 0.387
epoch 15,	batch   720,	training loss: 0.329
epoch 15,	batch   730,	training loss: 0.346
epoch 15,	batch   740,	training loss: 0.396
epoch 15,	batch   750,	training loss: 0.398
epoch 15,	batch   760,	training loss: 0.333
epoch 15,	batch   770,	training loss: 0.386
epoch 15,	batch   780,	training loss: 0.364
epoch 15,	batch   790,	training loss: 0.307
epoch 15,	batch   800,	training loss: 0.418
epoch 15,	batch   810,	training loss: 0.369
epoch 15,	batch   820,	training loss: 0.406
epoch 15,	batch   830,	training loss: 0.334
epoch 15,	batch   840,	training loss: 0.332
epoch 15,	batch   850,	training loss: 0.402
epoch 15,	batch   860,	training loss: 0.350
epoch 15,	batch   870,	training loss: 0.320
epoch 15,	batch   880,	training loss: 0.442
epoch 15,	batch   890,	training loss: 0.382
epoch 15,	batch   900,	training loss: 0.412
epoch 15,	batch   910,	training loss: 0.420
epoch 15,	batch   920,	training loss: 0.353
epoch 15,	batch   930,	training loss: 0.392
epoch 15,	batch   940,	training loss: 0.396
epoch 15,	batch   950,	training loss: 0.391
epoch 15,	batch   960,	training loss: 0.342
epoch 15,	batch   970,	training loss: 0.378
epoch 15,	batch   980,	training loss: 0.380
epoch 15,	batch   990,	training loss: 0.369
epoch 15,	batch  1000,	training loss: 0.336
epoch 15,	batch  1010,	training loss: 0.355
epoch 15,	batch  1020,	training loss: 0.427
epoch 15,	batch  1030,	training loss: 0.325
epoch 15,	batch  1040,	training loss: 0.362
epoch 15,	batch  1050,	training loss: 0.372
epoch 15,	batch  1060,	training loss: 0.392
epoch 15,	batch  1070,	training loss: 0.387
epoch 15,	batch  1080,	training loss: 0.376
epoch 15,	batch  1090,	training loss: 0.362
epoch 15,	batch  1100,	training loss: 0.389
epoch 15,	batch  1110,	training loss: 0.361
epoch 15,	batch  1120,	training loss: 0.364
epoch 15,	batch  1130,	training loss: 0.415
epoch 15,	batch  1140,	training loss: 0.450
epoch 15,	batch  1150,	training loss: 0.390
epoch 15,	batch  1160,	training loss: 0.351
epoch 15,	batch  1170,	training loss: 0.393
epoch 15,	batch  1180,	training loss: 0.484
epoch 15,	batch  1190,	training loss: 0.465
epoch 15,	batch  1200,	training loss: 0.339
epoch 15,	batch  1210,	training loss: 0.397
epoch 15,	batch  1220,	training loss: 0.412
epoch 15,	batch  1230,	training loss: 0.391
epoch 15,	batch  1240,	training loss: 0.374
epoch 15,	batch  1250,	training loss: 0.391
epoch 15,	batch  1260,	training loss: 0.390
epoch 15,	batch  1270,	training loss: 0.394
epoch 15,	batch  1280,	training loss: 0.425
epoch 15,	batch  1290,	training loss: 0.389
epoch 15,	batch  1300,	training loss: 0.399
epoch 15,	batch  1310,	training loss: 0.407
epoch 15,	batch  1320,	training loss: 0.485
epoch 15,	batch  1330,	training loss: 0.394
epoch 15,	batch  1340,	training loss: 0.433
epoch 15,	batch  1350,	training loss: 0.424
epoch 15,	batch  1360,	training loss: 0.348
epoch 15,	batch  1370,	training loss: 0.349
epoch 15,	batch  1380,	training loss: 0.389
epoch 15,	batch  1390,	training loss: 0.389
epoch 15,	batch  1400,	training loss: 0.383
epoch 15,	batch  1410,	training loss: 0.454
epoch 15,	batch  1420,	training loss: 0.400
epoch 15,	batch  1430,	training loss: 0.395
epoch 15,	batch  1440,	training loss: 0.436
epoch 15,	batch  1450,	training loss: 0.379
epoch 15,	batch  1460,	training loss: 0.391
epoch 15,	batch  1470,	training loss: 0.371
epoch 15,	batch  1480,	training loss: 0.436
epoch 15,	batch  1490,	training loss: 0.380
epoch 15,	batch  1500,	training loss: 0.434
epoch 15,	batch  1510,	training loss: 0.427
epoch 15,	batch  1520,	training loss: 0.342
epoch 15,	batch  1530,	training loss: 0.406
epoch 15,	batch  1540,	training loss: 0.376
epoch 15,	batch  1550,	training loss: 0.460
epoch 15,	batch  1560,	training loss: 0.373
epoch 15,	batch  1570,	training loss: 0.419
epoch 15,	batch  1580,	training loss: 0.346
epoch 15,	batch  1590,	training loss: 0.421
epoch 15,	batch  1600,	training loss: 0.384
epoch 15,	batch  1610,	training loss: 0.429
epoch 15,	batch  1620,	training loss: 0.372
epoch 15,	batch  1630,	training loss: 0.373
epoch 15,	batch  1640,	training loss: 0.400
epoch 15,	batch  1650,	training loss: 0.409
epoch 15,	batch  1660,	training loss: 0.418
epoch 15,	batch  1670,	training loss: 0.442
epoch 15,	batch  1680,	training loss: 0.424
epoch 15,	batch  1690,	training loss: 0.408
epoch 15,	batch  1700,	training loss: 0.373
epoch 15,	batch  1710,	training loss: 0.372
epoch 15,	batch  1720,	training loss: 0.382
epoch 15,	batch  1730,	training loss: 0.432
epoch 15,	batch  1740,	training loss: 0.434
epoch 15,	batch  1750,	training loss: 0.357
epoch 15,	batch  1760,	training loss: 0.415
epoch 15,	batch  1770,	training loss: 0.383
epoch 15,	batch  1780,	training loss: 0.486
epoch 15,	batch  1790,	training loss: 0.417
epoch 15,	batch  1800,	training loss: 0.420
epoch 15,	batch  1810,	training loss: 0.458
epoch 15,	batch  1820,	training loss: 0.397
epoch 15,	batch  1830,	training loss: 0.434
epoch 15,	batch  1840,	training loss: 0.370
epoch 15,	batch  1850,	training loss: 0.420
epoch 15,	batch  1860,	training loss: 0.408
epoch 15,	batch  1870,	training loss: 0.441
epoch 15,	batch  1880,	training loss: 0.440
epoch 15,	batch  1890,	training loss: 0.413
epoch 15,	batch  1900,	training loss: 0.481
epoch 15,	batch  1910,	training loss: 0.452
epoch 15,	batch  1920,	training loss: 0.498
epoch 15,	batch  1930,	training loss: 0.420
epoch 15,	batch  1940,	training loss: 0.409
epoch 15,	batch  1950,	training loss: 0.449
epoch 15,	batch  1960,	training loss: 0.360
epoch 15,	batch  1970,	training loss: 0.449
epoch 15,	batch  1980,	training loss: 0.449
END OF EPOCH 15
Testing on validation set...
# correct:	6301/54904 = 11.476395162465394%
# off by 1:	10771/54904 = 19.61787847879936%

epoch 16,	batch    10,	training loss: 0.331
epoch 16,	batch    20,	training loss: 0.333
epoch 16,	batch    30,	training loss: 0.286
epoch 16,	batch    40,	training loss: 0.351
epoch 16,	batch    50,	training loss: 0.293
epoch 16,	batch    60,	training loss: 0.262
epoch 16,	batch    70,	training loss: 0.307
epoch 16,	batch    80,	training loss: 0.326
epoch 16,	batch    90,	training loss: 0.246
epoch 16,	batch   100,	training loss: 0.310
epoch 16,	batch   110,	training loss: 0.291
epoch 16,	batch   120,	training loss: 0.288
epoch 16,	batch   130,	training loss: 0.291
epoch 16,	batch   140,	training loss: 0.257
epoch 16,	batch   150,	training loss: 0.244
epoch 16,	batch   160,	training loss: 0.230
epoch 16,	batch   170,	training loss: 0.260
epoch 16,	batch   180,	training loss: 0.293
epoch 16,	batch   190,	training loss: 0.264
epoch 16,	batch   200,	training loss: 0.239
epoch 16,	batch   210,	training loss: 0.245
epoch 16,	batch   220,	training loss: 0.267
epoch 16,	batch   230,	training loss: 0.293
epoch 16,	batch   240,	training loss: 0.259
epoch 16,	batch   250,	training loss: 0.262
epoch 16,	batch   260,	training loss: 0.292
epoch 16,	batch   270,	training loss: 0.357
epoch 16,	batch   280,	training loss: 0.264
epoch 16,	batch   290,	training loss: 0.305
epoch 16,	batch   300,	training loss: 0.286
epoch 16,	batch   310,	training loss: 0.268
epoch 16,	batch   320,	training loss: 0.256
epoch 16,	batch   330,	training loss: 0.326
epoch 16,	batch   340,	training loss: 0.230
epoch 16,	batch   350,	training loss: 0.262
epoch 16,	batch   360,	training loss: 0.316
epoch 16,	batch   370,	training loss: 0.325
epoch 16,	batch   380,	training loss: 0.274
epoch 16,	batch   390,	training loss: 0.262
epoch 16,	batch   400,	training loss: 0.277
epoch 16,	batch   410,	training loss: 0.254
epoch 16,	batch   420,	training loss: 0.274
epoch 16,	batch   430,	training loss: 0.264
epoch 16,	batch   440,	training loss: 0.261
epoch 16,	batch   450,	training loss: 0.288
epoch 16,	batch   460,	training loss: 0.380
epoch 16,	batch   470,	training loss: 0.291
epoch 16,	batch   480,	training loss: 0.291
epoch 16,	batch   490,	training loss: 0.310
epoch 16,	batch   500,	training loss: 0.316
epoch 16,	batch   510,	training loss: 0.280
epoch 16,	batch   520,	training loss: 0.352
epoch 16,	batch   530,	training loss: 0.337
epoch 16,	batch   540,	training loss: 0.349
epoch 16,	batch   550,	training loss: 0.383
epoch 16,	batch   560,	training loss: 0.347
epoch 16,	batch   570,	training loss: 0.423
epoch 16,	batch   580,	training loss: 0.333
epoch 16,	batch   590,	training loss: 0.283
epoch 16,	batch   600,	training loss: 0.313
epoch 16,	batch   610,	training loss: 0.304
epoch 16,	batch   620,	training loss: 0.255
epoch 16,	batch   630,	training loss: 0.375
epoch 16,	batch   640,	training loss: 0.273
epoch 16,	batch   650,	training loss: 0.270
epoch 16,	batch   660,	training loss: 0.316
epoch 16,	batch   670,	training loss: 0.315
epoch 16,	batch   680,	training loss: 0.315
epoch 16,	batch   690,	training loss: 0.296
epoch 16,	batch   700,	training loss: 0.302
epoch 16,	batch   710,	training loss: 0.334
epoch 16,	batch   720,	training loss: 0.314
epoch 16,	batch   730,	training loss: 0.308
epoch 16,	batch   740,	training loss: 0.291
epoch 16,	batch   750,	training loss: 0.298
epoch 16,	batch   760,	training loss: 0.273
epoch 16,	batch   770,	training loss: 0.321
epoch 16,	batch   780,	training loss: 0.280
epoch 16,	batch   790,	training loss: 0.242
epoch 16,	batch   800,	training loss: 0.300
epoch 16,	batch   810,	training loss: 0.292
epoch 16,	batch   820,	training loss: 0.355
epoch 16,	batch   830,	training loss: 0.341
epoch 16,	batch   840,	training loss: 0.271
epoch 16,	batch   850,	training loss: 0.323
epoch 16,	batch   860,	training loss: 0.323
epoch 16,	batch   870,	training loss: 0.392
epoch 16,	batch   880,	training loss: 0.298
epoch 16,	batch   890,	training loss: 0.285
epoch 16,	batch   900,	training loss: 0.362
epoch 16,	batch   910,	training loss: 0.367
epoch 16,	batch   920,	training loss: 0.302
epoch 16,	batch   930,	training loss: 0.298
epoch 16,	batch   940,	training loss: 0.281
epoch 16,	batch   950,	training loss: 0.321
epoch 16,	batch   960,	training loss: 0.288
epoch 16,	batch   970,	training loss: 0.306
epoch 16,	batch   980,	training loss: 0.398
epoch 16,	batch   990,	training loss: 0.382
epoch 16,	batch  1000,	training loss: 0.313
epoch 16,	batch  1010,	training loss: 0.306
epoch 16,	batch  1020,	training loss: 0.260
epoch 16,	batch  1030,	training loss: 0.280
epoch 16,	batch  1040,	training loss: 0.310
epoch 16,	batch  1050,	training loss: 0.363
epoch 16,	batch  1060,	training loss: 0.355
epoch 16,	batch  1070,	training loss: 0.344
epoch 16,	batch  1080,	training loss: 0.333
epoch 16,	batch  1090,	training loss: 0.343
epoch 16,	batch  1100,	training loss: 0.316
epoch 16,	batch  1110,	training loss: 0.296
epoch 16,	batch  1120,	training loss: 0.360
epoch 16,	batch  1130,	training loss: 0.423
epoch 16,	batch  1140,	training loss: 0.342
epoch 16,	batch  1150,	training loss: 0.420
epoch 16,	batch  1160,	training loss: 0.299
epoch 16,	batch  1170,	training loss: 0.328
epoch 16,	batch  1180,	training loss: 0.337
epoch 16,	batch  1190,	training loss: 0.343
epoch 16,	batch  1200,	training loss: 0.324
epoch 16,	batch  1210,	training loss: 0.298
epoch 16,	batch  1220,	training loss: 0.336
epoch 16,	batch  1230,	training loss: 0.373
epoch 16,	batch  1240,	training loss: 0.369
epoch 16,	batch  1250,	training loss: 0.387
epoch 16,	batch  1260,	training loss: 0.452
epoch 16,	batch  1270,	training loss: 0.440
epoch 16,	batch  1280,	training loss: 0.444
epoch 16,	batch  1290,	training loss: 0.337
epoch 16,	batch  1300,	training loss: 0.325
epoch 16,	batch  1310,	training loss: 0.319
epoch 16,	batch  1320,	training loss: 0.299
epoch 16,	batch  1330,	training loss: 0.391
epoch 16,	batch  1340,	training loss: 0.395
epoch 16,	batch  1350,	training loss: 0.374
epoch 16,	batch  1360,	training loss: 0.394
epoch 16,	batch  1370,	training loss: 0.361
epoch 16,	batch  1380,	training loss: 0.357
epoch 16,	batch  1390,	training loss: 0.302
epoch 16,	batch  1400,	training loss: 0.299
epoch 16,	batch  1410,	training loss: 0.309
epoch 16,	batch  1420,	training loss: 0.279
epoch 16,	batch  1430,	training loss: 0.314
epoch 16,	batch  1440,	training loss: 0.454
epoch 16,	batch  1450,	training loss: 0.296
epoch 16,	batch  1460,	training loss: 0.318
epoch 16,	batch  1470,	training loss: 0.349
epoch 16,	batch  1480,	training loss: 0.306
epoch 16,	batch  1490,	training loss: 0.334
epoch 16,	batch  1500,	training loss: 0.367
epoch 16,	batch  1510,	training loss: 0.295
epoch 16,	batch  1520,	training loss: 0.317
epoch 16,	batch  1530,	training loss: 0.318
epoch 16,	batch  1540,	training loss: 0.399
epoch 16,	batch  1550,	training loss: 0.372
epoch 16,	batch  1560,	training loss: 0.313
epoch 16,	batch  1570,	training loss: 0.377
epoch 16,	batch  1580,	training loss: 0.376
epoch 16,	batch  1590,	training loss: 0.431
epoch 16,	batch  1600,	training loss: 0.297
epoch 16,	batch  1610,	training loss: 0.390
epoch 16,	batch  1620,	training loss: 0.298
epoch 16,	batch  1630,	training loss: 0.332
epoch 16,	batch  1640,	training loss: 0.385
epoch 16,	batch  1650,	training loss: 0.343
epoch 16,	batch  1660,	training loss: 0.323
epoch 16,	batch  1670,	training loss: 0.324
epoch 16,	batch  1680,	training loss: 0.352
epoch 16,	batch  1690,	training loss: 0.344
epoch 16,	batch  1700,	training loss: 0.370
epoch 16,	batch  1710,	training loss: 0.408
epoch 16,	batch  1720,	training loss: 0.314
epoch 16,	batch  1730,	training loss: 0.338
epoch 16,	batch  1740,	training loss: 0.399
epoch 16,	batch  1750,	training loss: 0.396
epoch 16,	batch  1760,	training loss: 0.246
epoch 16,	batch  1770,	training loss: 0.356
epoch 16,	batch  1780,	training loss: 0.351
epoch 16,	batch  1790,	training loss: 0.408
epoch 16,	batch  1800,	training loss: 0.360
epoch 16,	batch  1810,	training loss: 0.367
epoch 16,	batch  1820,	training loss: 0.369
epoch 16,	batch  1830,	training loss: 0.346
epoch 16,	batch  1840,	training loss: 0.332
epoch 16,	batch  1850,	training loss: 0.414
epoch 16,	batch  1860,	training loss: 0.416
epoch 16,	batch  1870,	training loss: 0.348
epoch 16,	batch  1880,	training loss: 0.327
epoch 16,	batch  1890,	training loss: 0.332
epoch 16,	batch  1900,	training loss: 0.392
epoch 16,	batch  1910,	training loss: 0.389
epoch 16,	batch  1920,	training loss: 0.323
epoch 16,	batch  1930,	training loss: 0.351
epoch 16,	batch  1940,	training loss: 0.348
epoch 16,	batch  1950,	training loss: 0.423
epoch 16,	batch  1960,	training loss: 0.334
epoch 16,	batch  1970,	training loss: 0.364
epoch 16,	batch  1980,	training loss: 0.391
END OF EPOCH 16
Testing on validation set...
# correct:	5906/54904 = 10.756957598717761%
# off by 1:	11257/54904 = 20.503059886347078%

epoch 17,	batch    10,	training loss: 0.319
epoch 17,	batch    20,	training loss: 0.232
epoch 17,	batch    30,	training loss: 0.240
epoch 17,	batch    40,	training loss: 0.244
epoch 17,	batch    50,	training loss: 0.239
epoch 17,	batch    60,	training loss: 0.221
epoch 17,	batch    70,	training loss: 0.298
epoch 17,	batch    80,	training loss: 0.193
epoch 17,	batch    90,	training loss: 0.265
epoch 17,	batch   100,	training loss: 0.284
epoch 17,	batch   110,	training loss: 0.224
epoch 17,	batch   120,	training loss: 0.160
epoch 17,	batch   130,	training loss: 0.289
epoch 17,	batch   140,	training loss: 0.234
epoch 17,	batch   150,	training loss: 0.228
epoch 17,	batch   160,	training loss: 0.249
epoch 17,	batch   170,	training loss: 0.295
epoch 17,	batch   180,	training loss: 0.222
epoch 17,	batch   190,	training loss: 0.277
epoch 17,	batch   200,	training loss: 0.290
epoch 17,	batch   210,	training loss: 0.319
epoch 17,	batch   220,	training loss: 0.205
epoch 17,	batch   230,	training loss: 0.256
epoch 17,	batch   240,	training loss: 0.192
epoch 17,	batch   250,	training loss: 0.256
epoch 17,	batch   260,	training loss: 0.208
epoch 17,	batch   270,	training loss: 0.241
epoch 17,	batch   280,	training loss: 0.259
epoch 17,	batch   290,	training loss: 0.240
epoch 17,	batch   300,	training loss: 0.264
epoch 17,	batch   310,	training loss: 0.229
epoch 17,	batch   320,	training loss: 0.209
epoch 17,	batch   330,	training loss: 0.266
epoch 17,	batch   340,	training loss: 0.297
epoch 17,	batch   350,	training loss: 0.236
epoch 17,	batch   360,	training loss: 0.260
epoch 17,	batch   370,	training loss: 0.174
epoch 17,	batch   380,	training loss: 0.211
epoch 17,	batch   390,	training loss: 0.203
epoch 17,	batch   400,	training loss: 0.269
epoch 17,	batch   410,	training loss: 0.251
epoch 17,	batch   420,	training loss: 0.216
epoch 17,	batch   430,	training loss: 0.250
epoch 17,	batch   440,	training loss: 0.251
epoch 17,	batch   450,	training loss: 0.227
epoch 17,	batch   460,	training loss: 0.268
epoch 17,	batch   470,	training loss: 0.276
epoch 17,	batch   480,	training loss: 0.275
epoch 17,	batch   490,	training loss: 0.254
epoch 17,	batch   500,	training loss: 0.226
epoch 17,	batch   510,	training loss: 0.235
epoch 17,	batch   520,	training loss: 0.268
epoch 17,	batch   530,	training loss: 0.218
epoch 17,	batch   540,	training loss: 0.205
epoch 17,	batch   550,	training loss: 0.252
epoch 17,	batch   560,	training loss: 0.228
epoch 17,	batch   570,	training loss: 0.269
epoch 17,	batch   580,	training loss: 0.269
epoch 17,	batch   590,	training loss: 0.254
epoch 17,	batch   600,	training loss: 0.228
epoch 17,	batch   610,	training loss: 0.205
epoch 17,	batch   620,	training loss: 0.254
epoch 17,	batch   630,	training loss: 0.260
epoch 17,	batch   640,	training loss: 0.332
epoch 17,	batch   650,	training loss: 0.313
epoch 17,	batch   660,	training loss: 0.257
epoch 17,	batch   670,	training loss: 0.287
epoch 17,	batch   680,	training loss: 0.229
epoch 17,	batch   690,	training loss: 0.227
epoch 17,	batch   700,	training loss: 0.268
epoch 17,	batch   710,	training loss: 0.215
epoch 17,	batch   720,	training loss: 0.240
epoch 17,	batch   730,	training loss: 0.309
epoch 17,	batch   740,	training loss: 0.251
epoch 17,	batch   750,	training loss: 0.233
epoch 17,	batch   760,	training loss: 0.236
epoch 17,	batch   770,	training loss: 0.283
epoch 17,	batch   780,	training loss: 0.255
epoch 17,	batch   790,	training loss: 0.270
epoch 17,	batch   800,	training loss: 0.268
epoch 17,	batch   810,	training loss: 0.267
epoch 17,	batch   820,	training loss: 0.252
epoch 17,	batch   830,	training loss: 0.242
epoch 17,	batch   840,	training loss: 0.227
epoch 17,	batch   850,	training loss: 0.246
epoch 17,	batch   860,	training loss: 0.275
epoch 17,	batch   870,	training loss: 0.243
epoch 17,	batch   880,	training loss: 0.258
epoch 17,	batch   890,	training loss: 0.229
epoch 17,	batch   900,	training loss: 0.276
epoch 17,	batch   910,	training loss: 0.305
epoch 17,	batch   920,	training loss: 0.239
epoch 17,	batch   930,	training loss: 0.248
epoch 17,	batch   940,	training loss: 0.252
epoch 17,	batch   950,	training loss: 0.264
epoch 17,	batch   960,	training loss: 0.236
epoch 17,	batch   970,	training loss: 0.315
epoch 17,	batch   980,	training loss: 0.311
epoch 17,	batch   990,	training loss: 0.285
epoch 17,	batch  1000,	training loss: 0.263
epoch 17,	batch  1010,	training loss: 0.249
epoch 17,	batch  1020,	training loss: 0.275
epoch 17,	batch  1030,	training loss: 0.256
epoch 17,	batch  1040,	training loss: 0.256
epoch 17,	batch  1050,	training loss: 0.296
epoch 17,	batch  1060,	training loss: 0.309
epoch 17,	batch  1070,	training loss: 0.284
epoch 17,	batch  1080,	training loss: 0.244
epoch 17,	batch  1090,	training loss: 0.266
epoch 17,	batch  1100,	training loss: 0.273
epoch 17,	batch  1110,	training loss: 0.321
epoch 17,	batch  1120,	training loss: 0.322
epoch 17,	batch  1130,	training loss: 0.272
epoch 17,	batch  1140,	training loss: 0.300
epoch 17,	batch  1150,	training loss: 0.279
epoch 17,	batch  1160,	training loss: 0.248
epoch 17,	batch  1170,	training loss: 0.284
epoch 17,	batch  1180,	training loss: 0.297
epoch 17,	batch  1190,	training loss: 0.230
epoch 17,	batch  1200,	training loss: 0.274
epoch 17,	batch  1210,	training loss: 0.269
epoch 17,	batch  1220,	training loss: 0.373
epoch 17,	batch  1230,	training loss: 0.298
epoch 17,	batch  1240,	training loss: 0.281
epoch 17,	batch  1250,	training loss: 0.296
epoch 17,	batch  1260,	training loss: 0.277
epoch 17,	batch  1270,	training loss: 0.254
epoch 17,	batch  1280,	training loss: 0.276
epoch 17,	batch  1290,	training loss: 0.228
epoch 17,	batch  1300,	training loss: 0.264
epoch 17,	batch  1310,	training loss: 0.312
epoch 17,	batch  1320,	training loss: 0.247
epoch 17,	batch  1330,	training loss: 0.258
epoch 17,	batch  1340,	training loss: 0.256
epoch 17,	batch  1350,	training loss: 0.310
epoch 17,	batch  1360,	training loss: 0.246
epoch 17,	batch  1370,	training loss: 0.302
epoch 17,	batch  1380,	training loss: 0.323
epoch 17,	batch  1390,	training loss: 0.243
epoch 17,	batch  1400,	training loss: 0.299
epoch 17,	batch  1410,	training loss: 0.257
epoch 17,	batch  1420,	training loss: 0.362
epoch 17,	batch  1430,	training loss: 0.229
epoch 17,	batch  1440,	training loss: 0.315
epoch 17,	batch  1450,	training loss: 0.298
epoch 17,	batch  1460,	training loss: 0.273
epoch 17,	batch  1470,	training loss: 0.261
epoch 17,	batch  1480,	training loss: 0.284
epoch 17,	batch  1490,	training loss: 0.299
epoch 17,	batch  1500,	training loss: 0.255
epoch 17,	batch  1510,	training loss: 0.260
epoch 17,	batch  1520,	training loss: 0.302
epoch 17,	batch  1530,	training loss: 0.364
epoch 17,	batch  1540,	training loss: 0.340
epoch 17,	batch  1550,	training loss: 0.345
epoch 17,	batch  1560,	training loss: 0.360
epoch 17,	batch  1570,	training loss: 0.362
epoch 17,	batch  1580,	training loss: 0.308
epoch 17,	batch  1590,	training loss: 0.322
epoch 17,	batch  1600,	training loss: 0.390
epoch 17,	batch  1610,	training loss: 0.328
epoch 17,	batch  1620,	training loss: 0.319
epoch 17,	batch  1630,	training loss: 0.284
epoch 17,	batch  1640,	training loss: 0.345
epoch 17,	batch  1650,	training loss: 0.326
epoch 17,	batch  1660,	training loss: 0.315
epoch 17,	batch  1670,	training loss: 0.341
epoch 17,	batch  1680,	training loss: 0.321
epoch 17,	batch  1690,	training loss: 0.282
epoch 17,	batch  1700,	training loss: 0.341
epoch 17,	batch  1710,	training loss: 0.308
epoch 17,	batch  1720,	training loss: 0.280
epoch 17,	batch  1730,	training loss: 0.274
epoch 17,	batch  1740,	training loss: 0.298
epoch 17,	batch  1750,	training loss: 0.365
epoch 17,	batch  1760,	training loss: 0.306
epoch 17,	batch  1770,	training loss: 0.299
epoch 17,	batch  1780,	training loss: 0.318
epoch 17,	batch  1790,	training loss: 0.323
epoch 17,	batch  1800,	training loss: 0.274
epoch 17,	batch  1810,	training loss: 0.301
epoch 17,	batch  1820,	training loss: 0.284
epoch 17,	batch  1830,	training loss: 0.340
epoch 17,	batch  1840,	training loss: 0.325
epoch 17,	batch  1850,	training loss: 0.304
epoch 17,	batch  1860,	training loss: 0.360
epoch 17,	batch  1870,	training loss: 0.260
epoch 17,	batch  1880,	training loss: 0.354
epoch 17,	batch  1890,	training loss: 0.310
epoch 17,	batch  1900,	training loss: 0.267
epoch 17,	batch  1910,	training loss: 0.306
epoch 17,	batch  1920,	training loss: 0.307
epoch 17,	batch  1930,	training loss: 0.377
epoch 17,	batch  1940,	training loss: 0.274
epoch 17,	batch  1950,	training loss: 0.316
epoch 17,	batch  1960,	training loss: 0.301
epoch 17,	batch  1970,	training loss: 0.268
epoch 17,	batch  1980,	training loss: 0.285
END OF EPOCH 17
Testing on validation set...
# correct:	6328/54904 = 11.525571907329157%
# off by 1:	11346/54904 = 20.665161008305407%

epoch 18,	batch    10,	training loss: 0.212
epoch 18,	batch    20,	training loss: 0.203
epoch 18,	batch    30,	training loss: 0.182
epoch 18,	batch    40,	training loss: 0.176
epoch 18,	batch    50,	training loss: 0.191
epoch 18,	batch    60,	training loss: 0.237
epoch 18,	batch    70,	training loss: 0.186
epoch 18,	batch    80,	training loss: 0.201
epoch 18,	batch    90,	training loss: 0.198
epoch 18,	batch   100,	training loss: 0.199
epoch 18,	batch   110,	training loss: 0.214
epoch 18,	batch   120,	training loss: 0.231
epoch 18,	batch   130,	training loss: 0.188
epoch 18,	batch   140,	training loss: 0.221
epoch 18,	batch   150,	training loss: 0.217
epoch 18,	batch   160,	training loss: 0.193
epoch 18,	batch   170,	training loss: 0.184
epoch 18,	batch   180,	training loss: 0.160
epoch 18,	batch   190,	training loss: 0.185
epoch 18,	batch   200,	training loss: 0.172
epoch 18,	batch   210,	training loss: 0.198
epoch 18,	batch   220,	training loss: 0.181
epoch 18,	batch   230,	training loss: 0.220
epoch 18,	batch   240,	training loss: 0.187
epoch 18,	batch   250,	training loss: 0.163
epoch 18,	batch   260,	training loss: 0.219
epoch 18,	batch   270,	training loss: 0.193
epoch 18,	batch   280,	training loss: 0.167
epoch 18,	batch   290,	training loss: 0.148
epoch 18,	batch   300,	training loss: 0.235
epoch 18,	batch   310,	training loss: 0.159
epoch 18,	batch   320,	training loss: 0.173
epoch 18,	batch   330,	training loss: 0.192
epoch 18,	batch   340,	training loss: 0.178
epoch 18,	batch   350,	training loss: 0.258
epoch 18,	batch   360,	training loss: 0.249
epoch 18,	batch   370,	training loss: 0.248
epoch 18,	batch   380,	training loss: 0.175
epoch 18,	batch   390,	training loss: 0.230
epoch 18,	batch   400,	training loss: 0.158
epoch 18,	batch   410,	training loss: 0.236
epoch 18,	batch   420,	training loss: 0.240
epoch 18,	batch   430,	training loss: 0.168
epoch 18,	batch   440,	training loss: 0.252
epoch 18,	batch   450,	training loss: 0.219
epoch 18,	batch   460,	training loss: 0.267
epoch 18,	batch   470,	training loss: 0.182
epoch 18,	batch   480,	training loss: 0.166
epoch 18,	batch   490,	training loss: 0.212
epoch 18,	batch   500,	training loss: 0.175
epoch 18,	batch   510,	training loss: 0.192
epoch 18,	batch   520,	training loss: 0.204
epoch 18,	batch   530,	training loss: 0.191
epoch 18,	batch   540,	training loss: 0.181
epoch 18,	batch   550,	training loss: 0.218
epoch 18,	batch   560,	training loss: 0.190
epoch 18,	batch   570,	training loss: 0.228
epoch 18,	batch   580,	training loss: 0.212
epoch 18,	batch   590,	training loss: 0.235
epoch 18,	batch   600,	training loss: 0.200
epoch 18,	batch   610,	training loss: 0.205
epoch 18,	batch   620,	training loss: 0.185
epoch 18,	batch   630,	training loss: 0.210
epoch 18,	batch   640,	training loss: 0.175
epoch 18,	batch   650,	training loss: 0.213
epoch 18,	batch   660,	training loss: 0.257
epoch 18,	batch   670,	training loss: 0.205
epoch 18,	batch   680,	training loss: 0.199
epoch 18,	batch   690,	training loss: 0.179
epoch 18,	batch   700,	training loss: 0.203
epoch 18,	batch   710,	training loss: 0.219
epoch 18,	batch   720,	training loss: 0.201
epoch 18,	batch   730,	training loss: 0.233
epoch 18,	batch   740,	training loss: 0.207
epoch 18,	batch   750,	training loss: 0.213
epoch 18,	batch   760,	training loss: 0.209
epoch 18,	batch   770,	training loss: 0.194
epoch 18,	batch   780,	training loss: 0.221
epoch 18,	batch   790,	training loss: 0.179
epoch 18,	batch   800,	training loss: 0.184
epoch 18,	batch   810,	training loss: 0.182
epoch 18,	batch   820,	training loss: 0.279
epoch 18,	batch   830,	training loss: 0.259
epoch 18,	batch   840,	training loss: 0.233
epoch 18,	batch   850,	training loss: 0.286
epoch 18,	batch   860,	training loss: 0.186
epoch 18,	batch   870,	training loss: 0.229
epoch 18,	batch   880,	training loss: 0.205
epoch 18,	batch   890,	training loss: 0.182
epoch 18,	batch   900,	training loss: 0.195
epoch 18,	batch   910,	training loss: 0.248
epoch 18,	batch   920,	training loss: 0.258
epoch 18,	batch   930,	training loss: 0.235
epoch 18,	batch   940,	training loss: 0.231
epoch 18,	batch   950,	training loss: 0.203
epoch 18,	batch   960,	training loss: 0.219
epoch 18,	batch   970,	training loss: 0.219
epoch 18,	batch   980,	training loss: 0.279
epoch 18,	batch   990,	training loss: 0.289
epoch 18,	batch  1000,	training loss: 0.285
epoch 18,	batch  1010,	training loss: 0.237
epoch 18,	batch  1020,	training loss: 0.210
epoch 18,	batch  1030,	training loss: 0.207
epoch 18,	batch  1040,	training loss: 0.200
epoch 18,	batch  1050,	training loss: 0.257
epoch 18,	batch  1060,	training loss: 0.208
epoch 18,	batch  1070,	training loss: 0.254
epoch 18,	batch  1080,	training loss: 0.250
epoch 18,	batch  1090,	training loss: 0.274
epoch 18,	batch  1100,	training loss: 0.207
epoch 18,	batch  1110,	training loss: 0.225
epoch 18,	batch  1120,	training loss: 0.358
epoch 18,	batch  1130,	training loss: 0.303
epoch 18,	batch  1140,	training loss: 0.213
epoch 18,	batch  1150,	training loss: 0.311
epoch 18,	batch  1160,	training loss: 0.268
epoch 18,	batch  1170,	training loss: 0.222
epoch 18,	batch  1180,	training loss: 0.292
epoch 18,	batch  1190,	training loss: 0.282
epoch 18,	batch  1200,	training loss: 0.270
epoch 18,	batch  1210,	training loss: 0.270
epoch 18,	batch  1220,	training loss: 0.280
epoch 18,	batch  1230,	training loss: 0.264
epoch 18,	batch  1240,	training loss: 0.249
epoch 18,	batch  1250,	training loss: 0.277
epoch 18,	batch  1260,	training loss: 0.258
epoch 18,	batch  1270,	training loss: 0.239
epoch 18,	batch  1280,	training loss: 0.247
epoch 18,	batch  1290,	training loss: 0.230
epoch 18,	batch  1300,	training loss: 0.255
epoch 18,	batch  1310,	training loss: 0.220
epoch 18,	batch  1320,	training loss: 0.286
epoch 18,	batch  1330,	training loss: 0.243
epoch 18,	batch  1340,	training loss: 0.215
epoch 18,	batch  1350,	training loss: 0.260
epoch 18,	batch  1360,	training loss: 0.278
epoch 18,	batch  1370,	training loss: 0.253
epoch 18,	batch  1380,	training loss: 0.266
epoch 18,	batch  1390,	training loss: 0.255
epoch 18,	batch  1400,	training loss: 0.233
epoch 18,	batch  1410,	training loss: 0.260
epoch 18,	batch  1420,	training loss: 0.251
epoch 18,	batch  1430,	training loss: 0.269
epoch 18,	batch  1440,	training loss: 0.275
epoch 18,	batch  1450,	training loss: 0.245
epoch 18,	batch  1460,	training loss: 0.271
epoch 18,	batch  1470,	training loss: 0.246
epoch 18,	batch  1480,	training loss: 0.223
epoch 18,	batch  1490,	training loss: 0.242
epoch 18,	batch  1500,	training loss: 0.257
epoch 18,	batch  1510,	training loss: 0.271
epoch 18,	batch  1520,	training loss: 0.208
epoch 18,	batch  1530,	training loss: 0.234
epoch 18,	batch  1540,	training loss: 0.250
epoch 18,	batch  1550,	training loss: 0.256
epoch 18,	batch  1560,	training loss: 0.264
epoch 18,	batch  1570,	training loss: 0.264
epoch 18,	batch  1580,	training loss: 0.237
epoch 18,	batch  1590,	training loss: 0.305
epoch 18,	batch  1600,	training loss: 0.294
epoch 18,	batch  1610,	training loss: 0.283
epoch 18,	batch  1620,	training loss: 0.308
epoch 18,	batch  1630,	training loss: 0.257
epoch 18,	batch  1640,	training loss: 0.259
epoch 18,	batch  1650,	training loss: 0.318
epoch 18,	batch  1660,	training loss: 0.245
epoch 18,	batch  1670,	training loss: 0.252
epoch 18,	batch  1680,	training loss: 0.302
epoch 18,	batch  1690,	training loss: 0.268
epoch 18,	batch  1700,	training loss: 0.271
epoch 18,	batch  1710,	training loss: 0.295
epoch 18,	batch  1720,	training loss: 0.306
epoch 18,	batch  1730,	training loss: 0.280
epoch 18,	batch  1740,	training loss: 0.241
epoch 18,	batch  1750,	training loss: 0.250
epoch 18,	batch  1760,	training loss: 0.263
epoch 18,	batch  1770,	training loss: 0.268
epoch 18,	batch  1780,	training loss: 0.275
epoch 18,	batch  1790,	training loss: 0.273
epoch 18,	batch  1800,	training loss: 0.307
epoch 18,	batch  1810,	training loss: 0.340
epoch 18,	batch  1820,	training loss: 0.270
epoch 18,	batch  1830,	training loss: 0.258
epoch 18,	batch  1840,	training loss: 0.285
epoch 18,	batch  1850,	training loss: 0.245
epoch 18,	batch  1860,	training loss: 0.246
epoch 18,	batch  1870,	training loss: 0.189
epoch 18,	batch  1880,	training loss: 0.287
epoch 18,	batch  1890,	training loss: 0.279
epoch 18,	batch  1900,	training loss: 0.229
epoch 18,	batch  1910,	training loss: 0.285
epoch 18,	batch  1920,	training loss: 0.258
epoch 18,	batch  1930,	training loss: 0.245
epoch 18,	batch  1940,	training loss: 0.270
epoch 18,	batch  1950,	training loss: 0.343
epoch 18,	batch  1960,	training loss: 0.289
epoch 18,	batch  1970,	training loss: 0.317
epoch 18,	batch  1980,	training loss: 0.295
END OF EPOCH 18
Testing on validation set...
# correct:	6268/54904 = 11.416290252076351%
# off by 1:	11002/54904 = 20.038612851522657%

epoch 19,	batch    10,	training loss: 0.211
epoch 19,	batch    20,	training loss: 0.165
epoch 19,	batch    30,	training loss: 0.179
epoch 19,	batch    40,	training loss: 0.191
epoch 19,	batch    50,	training loss: 0.158
epoch 19,	batch    60,	training loss: 0.167
epoch 19,	batch    70,	training loss: 0.176
epoch 19,	batch    80,	training loss: 0.204
epoch 19,	batch    90,	training loss: 0.152
epoch 19,	batch   100,	training loss: 0.159
epoch 19,	batch   110,	training loss: 0.149
epoch 19,	batch   120,	training loss: 0.169
epoch 19,	batch   130,	training loss: 0.184
epoch 19,	batch   140,	training loss: 0.169
epoch 19,	batch   150,	training loss: 0.155
epoch 19,	batch   160,	training loss: 0.146
epoch 19,	batch   170,	training loss: 0.170
epoch 19,	batch   180,	training loss: 0.191
epoch 19,	batch   190,	training loss: 0.166
epoch 19,	batch   200,	training loss: 0.222
epoch 19,	batch   210,	training loss: 0.218
epoch 19,	batch   220,	training loss: 0.156
epoch 19,	batch   230,	training loss: 0.184
epoch 19,	batch   240,	training loss: 0.196
epoch 19,	batch   250,	training loss: 0.193
epoch 19,	batch   260,	training loss: 0.153
epoch 19,	batch   270,	training loss: 0.151
epoch 19,	batch   280,	training loss: 0.153
epoch 19,	batch   290,	training loss: 0.171
epoch 19,	batch   300,	training loss: 0.236
epoch 19,	batch   310,	training loss: 0.219
epoch 19,	batch   320,	training loss: 0.186
epoch 19,	batch   330,	training loss: 0.140
epoch 19,	batch   340,	training loss: 0.122
epoch 19,	batch   350,	training loss: 0.187
epoch 19,	batch   360,	training loss: 0.130
epoch 19,	batch   370,	training loss: 0.180
epoch 19,	batch   380,	training loss: 0.163
epoch 19,	batch   390,	training loss: 0.153
epoch 19,	batch   400,	training loss: 0.170
epoch 19,	batch   410,	training loss: 0.169
epoch 19,	batch   420,	training loss: 0.156
epoch 19,	batch   430,	training loss: 0.138
epoch 19,	batch   440,	training loss: 0.183
epoch 19,	batch   450,	training loss: 0.175
epoch 19,	batch   460,	training loss: 0.151
epoch 19,	batch   470,	training loss: 0.161
epoch 19,	batch   480,	training loss: 0.164
epoch 19,	batch   490,	training loss: 0.168
epoch 19,	batch   500,	training loss: 0.131
epoch 19,	batch   510,	training loss: 0.212
epoch 19,	batch   520,	training loss: 0.217
epoch 19,	batch   530,	training loss: 0.194
epoch 19,	batch   540,	training loss: 0.158
epoch 19,	batch   550,	training loss: 0.156
epoch 19,	batch   560,	training loss: 0.146
epoch 19,	batch   570,	training loss: 0.175
epoch 19,	batch   580,	training loss: 0.196
epoch 19,	batch   590,	training loss: 0.195
epoch 19,	batch   600,	training loss: 0.176
epoch 19,	batch   610,	training loss: 0.236
epoch 19,	batch   620,	training loss: 0.143
epoch 19,	batch   630,	training loss: 0.206
epoch 19,	batch   640,	training loss: 0.156
epoch 19,	batch   650,	training loss: 0.190
epoch 19,	batch   660,	training loss: 0.222
epoch 19,	batch   670,	training loss: 0.140
epoch 19,	batch   680,	training loss: 0.160
epoch 19,	batch   690,	training loss: 0.204
epoch 19,	batch   700,	training loss: 0.176
epoch 19,	batch   710,	training loss: 0.194
epoch 19,	batch   720,	training loss: 0.173
epoch 19,	batch   730,	training loss: 0.201
epoch 19,	batch   740,	training loss: 0.200
epoch 19,	batch   750,	training loss: 0.188
epoch 19,	batch   760,	training loss: 0.178
epoch 19,	batch   770,	training loss: 0.187
epoch 19,	batch   780,	training loss: 0.207
epoch 19,	batch   790,	training loss: 0.247
epoch 19,	batch   800,	training loss: 0.186
epoch 19,	batch   810,	training loss: 0.201
epoch 19,	batch   820,	training loss: 0.202
epoch 19,	batch   830,	training loss: 0.172
epoch 19,	batch   840,	training loss: 0.180
epoch 19,	batch   850,	training loss: 0.134
epoch 19,	batch   860,	training loss: 0.259
epoch 19,	batch   870,	training loss: 0.201
epoch 19,	batch   880,	training loss: 0.156
epoch 19,	batch   890,	training loss: 0.210
epoch 19,	batch   900,	training loss: 0.199
epoch 19,	batch   910,	training loss: 0.143
epoch 19,	batch   920,	training loss: 0.215
epoch 19,	batch   930,	training loss: 0.205
epoch 19,	batch   940,	training loss: 0.206
epoch 19,	batch   950,	training loss: 0.190
epoch 19,	batch   960,	training loss: 0.196
epoch 19,	batch   970,	training loss: 0.204
epoch 19,	batch   980,	training loss: 0.194
epoch 19,	batch   990,	training loss: 0.191
epoch 19,	batch  1000,	training loss: 0.266
epoch 19,	batch  1010,	training loss: 0.199
epoch 19,	batch  1020,	training loss: 0.201
epoch 19,	batch  1030,	training loss: 0.194
epoch 19,	batch  1040,	training loss: 0.164
epoch 19,	batch  1050,	training loss: 0.190
epoch 19,	batch  1060,	training loss: 0.202
epoch 19,	batch  1070,	training loss: 0.174
epoch 19,	batch  1080,	training loss: 0.207
epoch 19,	batch  1090,	training loss: 0.148
epoch 19,	batch  1100,	training loss: 0.267
epoch 19,	batch  1110,	training loss: 0.218
epoch 19,	batch  1120,	training loss: 0.226
epoch 19,	batch  1130,	training loss: 0.223
epoch 19,	batch  1140,	training loss: 0.209
epoch 19,	batch  1150,	training loss: 0.171
epoch 19,	batch  1160,	training loss: 0.194
epoch 19,	batch  1170,	training loss: 0.217
epoch 19,	batch  1180,	training loss: 0.179
epoch 19,	batch  1190,	training loss: 0.141
epoch 19,	batch  1200,	training loss: 0.209
epoch 19,	batch  1210,	training loss: 0.199
epoch 19,	batch  1220,	training loss: 0.234
epoch 19,	batch  1230,	training loss: 0.182
epoch 19,	batch  1240,	training loss: 0.270
epoch 19,	batch  1250,	training loss: 0.174
epoch 19,	batch  1260,	training loss: 0.227
epoch 19,	batch  1270,	training loss: 0.236
epoch 19,	batch  1280,	training loss: 0.217
epoch 19,	batch  1290,	training loss: 0.246
epoch 19,	batch  1300,	training loss: 0.213
epoch 19,	batch  1310,	training loss: 0.226
epoch 19,	batch  1320,	training loss: 0.232
epoch 19,	batch  1330,	training loss: 0.206
epoch 19,	batch  1340,	training loss: 0.212
epoch 19,	batch  1350,	training loss: 0.231
epoch 19,	batch  1360,	training loss: 0.229
epoch 19,	batch  1370,	training loss: 0.247
epoch 19,	batch  1380,	training loss: 0.205
epoch 19,	batch  1390,	training loss: 0.284
epoch 19,	batch  1400,	training loss: 0.210
epoch 19,	batch  1410,	training loss: 0.217
epoch 19,	batch  1420,	training loss: 0.239
epoch 19,	batch  1430,	training loss: 0.242
epoch 19,	batch  1440,	training loss: 0.240
epoch 19,	batch  1450,	training loss: 0.242
epoch 19,	batch  1460,	training loss: 0.261
epoch 19,	batch  1470,	training loss: 0.192
epoch 19,	batch  1480,	training loss: 0.222
epoch 19,	batch  1490,	training loss: 0.281
epoch 19,	batch  1500,	training loss: 0.264
epoch 19,	batch  1510,	training loss: 0.187
epoch 19,	batch  1520,	training loss: 0.235
epoch 19,	batch  1530,	training loss: 0.214
epoch 19,	batch  1540,	training loss: 0.233
epoch 19,	batch  1550,	training loss: 0.258
epoch 19,	batch  1560,	training loss: 0.206
epoch 19,	batch  1570,	training loss: 0.308
epoch 19,	batch  1580,	training loss: 0.226
epoch 19,	batch  1590,	training loss: 0.193
epoch 19,	batch  1600,	training loss: 0.209
epoch 19,	batch  1610,	training loss: 0.217
epoch 19,	batch  1620,	training loss: 0.192
epoch 19,	batch  1630,	training loss: 0.215
epoch 19,	batch  1640,	training loss: 0.205
epoch 19,	batch  1650,	training loss: 0.238
epoch 19,	batch  1660,	training loss: 0.224
epoch 19,	batch  1670,	training loss: 0.215
epoch 19,	batch  1680,	training loss: 0.316
epoch 19,	batch  1690,	training loss: 0.237
epoch 19,	batch  1700,	training loss: 0.199
epoch 19,	batch  1710,	training loss: 0.186
epoch 19,	batch  1720,	training loss: 0.219
epoch 19,	batch  1730,	training loss: 0.269
epoch 19,	batch  1740,	training loss: 0.203
epoch 19,	batch  1750,	training loss: 0.268
epoch 19,	batch  1760,	training loss: 0.252
epoch 19,	batch  1770,	training loss: 0.213
epoch 19,	batch  1780,	training loss: 0.229
epoch 19,	batch  1790,	training loss: 0.313
epoch 19,	batch  1800,	training loss: 0.192
epoch 19,	batch  1810,	training loss: 0.220
epoch 19,	batch  1820,	training loss: 0.219
epoch 19,	batch  1830,	training loss: 0.213
epoch 19,	batch  1840,	training loss: 0.247
epoch 19,	batch  1850,	training loss: 0.214
epoch 19,	batch  1860,	training loss: 0.248
epoch 19,	batch  1870,	training loss: 0.205
epoch 19,	batch  1880,	training loss: 0.246
epoch 19,	batch  1890,	training loss: 0.256
epoch 19,	batch  1900,	training loss: 0.237
epoch 19,	batch  1910,	training loss: 0.230
epoch 19,	batch  1920,	training loss: 0.207
epoch 19,	batch  1930,	training loss: 0.259
epoch 19,	batch  1940,	training loss: 0.262
epoch 19,	batch  1950,	training loss: 0.316
epoch 19,	batch  1960,	training loss: 0.205
epoch 19,	batch  1970,	training loss: 0.194
epoch 19,	batch  1980,	training loss: 0.215
END OF EPOCH 19
Testing on validation set...
# correct:	6082/54904 = 11.077517120792656%
# off by 1:	10896/54904 = 19.84554859390937%

epoch 20,	batch    10,	training loss: 0.218
epoch 20,	batch    20,	training loss: 0.203
epoch 20,	batch    30,	training loss: 0.158
epoch 20,	batch    40,	training loss: 0.178
epoch 20,	batch    50,	training loss: 0.155
epoch 20,	batch    60,	training loss: 0.146
epoch 20,	batch    70,	training loss: 0.115
epoch 20,	batch    80,	training loss: 0.113
epoch 20,	batch    90,	training loss: 0.128
epoch 20,	batch   100,	training loss: 0.145
epoch 20,	batch   110,	training loss: 0.158
epoch 20,	batch   120,	training loss: 0.162
epoch 20,	batch   130,	training loss: 0.147
epoch 20,	batch   140,	training loss: 0.171
epoch 20,	batch   150,	training loss: 0.160
epoch 20,	batch   160,	training loss: 0.130
epoch 20,	batch   170,	training loss: 0.152
epoch 20,	batch   180,	training loss: 0.130
epoch 20,	batch   190,	training loss: 0.183
epoch 20,	batch   200,	training loss: 0.167
epoch 20,	batch   210,	training loss: 0.147
epoch 20,	batch   220,	training loss: 0.155
epoch 20,	batch   230,	training loss: 0.145
epoch 20,	batch   240,	training loss: 0.136
epoch 20,	batch   250,	training loss: 0.155
epoch 20,	batch   260,	training loss: 0.152
epoch 20,	batch   270,	training loss: 0.137
epoch 20,	batch   280,	training loss: 0.125
epoch 20,	batch   290,	training loss: 0.263
epoch 20,	batch   300,	training loss: 0.145
epoch 20,	batch   310,	training loss: 0.181
epoch 20,	batch   320,	training loss: 0.167
epoch 20,	batch   330,	training loss: 0.164
epoch 20,	batch   340,	training loss: 0.190
epoch 20,	batch   350,	training loss: 0.141
epoch 20,	batch   360,	training loss: 0.124
epoch 20,	batch   370,	training loss: 0.135
epoch 20,	batch   380,	training loss: 0.179
epoch 20,	batch   390,	training loss: 0.177
epoch 20,	batch   400,	training loss: 0.163
epoch 20,	batch   410,	training loss: 0.193
epoch 20,	batch   420,	training loss: 0.217
epoch 20,	batch   430,	training loss: 0.184
epoch 20,	batch   440,	training loss: 0.151
epoch 20,	batch   450,	training loss: 0.197
epoch 20,	batch   460,	training loss: 0.183
epoch 20,	batch   470,	training loss: 0.139
epoch 20,	batch   480,	training loss: 0.173
epoch 20,	batch   490,	training loss: 0.199
epoch 20,	batch   500,	training loss: 0.193
epoch 20,	batch   510,	training loss: 0.191
epoch 20,	batch   520,	training loss: 0.161
epoch 20,	batch   530,	training loss: 0.156
epoch 20,	batch   540,	training loss: 0.182
epoch 20,	batch   550,	training loss: 0.193
epoch 20,	batch   560,	training loss: 0.151
epoch 20,	batch   570,	training loss: 0.163
epoch 20,	batch   580,	training loss: 0.220
epoch 20,	batch   590,	training loss: 0.153
epoch 20,	batch   600,	training loss: 0.190
epoch 20,	batch   610,	training loss: 0.149
epoch 20,	batch   620,	training loss: 0.170
epoch 20,	batch   630,	training loss: 0.168
epoch 20,	batch   640,	training loss: 0.150
epoch 20,	batch   650,	training loss: 0.142
epoch 20,	batch   660,	training loss: 0.151
epoch 20,	batch   670,	training loss: 0.165
epoch 20,	batch   680,	training loss: 0.151
epoch 20,	batch   690,	training loss: 0.128
epoch 20,	batch   700,	training loss: 0.141
epoch 20,	batch   710,	training loss: 0.205
epoch 20,	batch   720,	training loss: 0.146
epoch 20,	batch   730,	training loss: 0.183
epoch 20,	batch   740,	training loss: 0.171
epoch 20,	batch   750,	training loss: 0.147
epoch 20,	batch   760,	training loss: 0.151
epoch 20,	batch   770,	training loss: 0.181
epoch 20,	batch   780,	training loss: 0.173
epoch 20,	batch   790,	training loss: 0.160
epoch 20,	batch   800,	training loss: 0.191
epoch 20,	batch   810,	training loss: 0.161
epoch 20,	batch   820,	training loss: 0.126
epoch 20,	batch   830,	training loss: 0.171
epoch 20,	batch   840,	training loss: 0.159
epoch 20,	batch   850,	training loss: 0.147
epoch 20,	batch   860,	training loss: 0.163
epoch 20,	batch   870,	training loss: 0.207
epoch 20,	batch   880,	training loss: 0.208
epoch 20,	batch   890,	training loss: 0.168
epoch 20,	batch   900,	training loss: 0.147
epoch 20,	batch   910,	training loss: 0.134
epoch 20,	batch   920,	training loss: 0.160
epoch 20,	batch   930,	training loss: 0.189
epoch 20,	batch   940,	training loss: 0.125
epoch 20,	batch   950,	training loss: 0.155
epoch 20,	batch   960,	training loss: 0.167
epoch 20,	batch   970,	training loss: 0.192
epoch 20,	batch   980,	training loss: 0.226
epoch 20,	batch   990,	training loss: 0.152
epoch 20,	batch  1000,	training loss: 0.143
epoch 20,	batch  1010,	training loss: 0.182
epoch 20,	batch  1020,	training loss: 0.182
epoch 20,	batch  1030,	training loss: 0.147
epoch 20,	batch  1040,	training loss: 0.153
epoch 20,	batch  1050,	training loss: 0.166
epoch 20,	batch  1060,	training loss: 0.176
epoch 20,	batch  1070,	training loss: 0.188
epoch 20,	batch  1080,	training loss: 0.250
epoch 20,	batch  1090,	training loss: 0.138
epoch 20,	batch  1100,	training loss: 0.178
epoch 20,	batch  1110,	training loss: 0.177
epoch 20,	batch  1120,	training loss: 0.129
epoch 20,	batch  1130,	training loss: 0.160
epoch 20,	batch  1140,	training loss: 0.195
epoch 20,	batch  1150,	training loss: 0.126
epoch 20,	batch  1160,	training loss: 0.163
epoch 20,	batch  1170,	training loss: 0.176
epoch 20,	batch  1180,	training loss: 0.224
epoch 20,	batch  1190,	training loss: 0.182
epoch 20,	batch  1200,	training loss: 0.140
epoch 20,	batch  1210,	training loss: 0.193
epoch 20,	batch  1220,	training loss: 0.169
epoch 20,	batch  1230,	training loss: 0.179
epoch 20,	batch  1240,	training loss: 0.178
epoch 20,	batch  1250,	training loss: 0.133
epoch 20,	batch  1260,	training loss: 0.177
epoch 20,	batch  1270,	training loss: 0.172
epoch 20,	batch  1280,	training loss: 0.194
epoch 20,	batch  1290,	training loss: 0.194
epoch 20,	batch  1300,	training loss: 0.194
epoch 20,	batch  1310,	training loss: 0.173
epoch 20,	batch  1320,	training loss: 0.165
epoch 20,	batch  1330,	training loss: 0.144
epoch 20,	batch  1340,	training loss: 0.160
epoch 20,	batch  1350,	training loss: 0.174
epoch 20,	batch  1360,	training loss: 0.207
epoch 20,	batch  1370,	training loss: 0.135
epoch 20,	batch  1380,	training loss: 0.140
epoch 20,	batch  1390,	training loss: 0.191
epoch 20,	batch  1400,	training loss: 0.192
epoch 20,	batch  1410,	training loss: 0.202
epoch 20,	batch  1420,	training loss: 0.143
epoch 20,	batch  1430,	training loss: 0.156
epoch 20,	batch  1440,	training loss: 0.138
epoch 20,	batch  1450,	training loss: 0.244
epoch 20,	batch  1460,	training loss: 0.224
epoch 20,	batch  1470,	training loss: 0.196
epoch 20,	batch  1480,	training loss: 0.151
epoch 20,	batch  1490,	training loss: 0.157
epoch 20,	batch  1500,	training loss: 0.185
epoch 20,	batch  1510,	training loss: 0.229
epoch 20,	batch  1520,	training loss: 0.167
epoch 20,	batch  1530,	training loss: 0.195
epoch 20,	batch  1540,	training loss: 0.196
epoch 20,	batch  1550,	training loss: 0.191
epoch 20,	batch  1560,	training loss: 0.244
epoch 20,	batch  1570,	training loss: 0.184
epoch 20,	batch  1580,	training loss: 0.170
epoch 20,	batch  1590,	training loss: 0.215
epoch 20,	batch  1600,	training loss: 0.196
epoch 20,	batch  1610,	training loss: 0.266
epoch 20,	batch  1620,	training loss: 0.180
epoch 20,	batch  1630,	training loss: 0.238
epoch 20,	batch  1640,	training loss: 0.170
epoch 20,	batch  1650,	training loss: 0.212
epoch 20,	batch  1660,	training loss: 0.215
epoch 20,	batch  1670,	training loss: 0.161
epoch 20,	batch  1680,	training loss: 0.180
epoch 20,	batch  1690,	training loss: 0.172
epoch 20,	batch  1700,	training loss: 0.209
epoch 20,	batch  1710,	training loss: 0.192
epoch 20,	batch  1720,	training loss: 0.184
epoch 20,	batch  1730,	training loss: 0.201
epoch 20,	batch  1740,	training loss: 0.180
epoch 20,	batch  1750,	training loss: 0.210
epoch 20,	batch  1760,	training loss: 0.275
epoch 20,	batch  1770,	training loss: 0.259
epoch 20,	batch  1780,	training loss: 0.219
epoch 20,	batch  1790,	training loss: 0.228
epoch 20,	batch  1800,	training loss: 0.227
epoch 20,	batch  1810,	training loss: 0.212
epoch 20,	batch  1820,	training loss: 0.263
epoch 20,	batch  1830,	training loss: 0.204
epoch 20,	batch  1840,	training loss: 0.327
epoch 20,	batch  1850,	training loss: 0.194
epoch 20,	batch  1860,	training loss: 0.226
epoch 20,	batch  1870,	training loss: 0.198
epoch 20,	batch  1880,	training loss: 0.192
epoch 20,	batch  1890,	training loss: 0.153
epoch 20,	batch  1900,	training loss: 0.178
epoch 20,	batch  1910,	training loss: 0.175
epoch 20,	batch  1920,	training loss: 0.214
epoch 20,	batch  1930,	training loss: 0.192
epoch 20,	batch  1940,	training loss: 0.185
epoch 20,	batch  1950,	training loss: 0.160
epoch 20,	batch  1960,	training loss: 0.243
epoch 20,	batch  1970,	training loss: 0.214
epoch 20,	batch  1980,	training loss: 0.248
END OF EPOCH 20
Testing on validation set...
# correct:	6511/54904 = 11.85888095585021%
# off by 1:	10985/54904 = 20.007649715867696%

epoch 21,	batch    10,	training loss: 0.155
epoch 21,	batch    20,	training loss: 0.187
epoch 21,	batch    30,	training loss: 0.174
epoch 21,	batch    40,	training loss: 0.128
epoch 21,	batch    50,	training loss: 0.148
epoch 21,	batch    60,	training loss: 0.108
epoch 21,	batch    70,	training loss: 0.117
epoch 21,	batch    80,	training loss: 0.131
epoch 21,	batch    90,	training loss: 0.158
epoch 21,	batch   100,	training loss: 0.122
epoch 21,	batch   110,	training loss: 0.126
epoch 21,	batch   120,	training loss: 0.119
epoch 21,	batch   130,	training loss: 0.106
epoch 21,	batch   140,	training loss: 0.123
epoch 21,	batch   150,	training loss: 0.150
epoch 21,	batch   160,	training loss: 0.080
epoch 21,	batch   170,	training loss: 0.128
epoch 21,	batch   180,	training loss: 0.166
epoch 21,	batch   190,	training loss: 0.118
epoch 21,	batch   200,	training loss: 0.125
epoch 21,	batch   210,	training loss: 0.174
epoch 21,	batch   220,	training loss: 0.128
epoch 21,	batch   230,	training loss: 0.149
epoch 21,	batch   240,	training loss: 0.168
epoch 21,	batch   250,	training loss: 0.198
epoch 21,	batch   260,	training loss: 0.152
epoch 21,	batch   270,	training loss: 0.091
epoch 21,	batch   280,	training loss: 0.137
epoch 21,	batch   290,	training loss: 0.089
epoch 21,	batch   300,	training loss: 0.116
epoch 21,	batch   310,	training loss: 0.171
epoch 21,	batch   320,	training loss: 0.130
epoch 21,	batch   330,	training loss: 0.131
epoch 21,	batch   340,	training loss: 0.133
epoch 21,	batch   350,	training loss: 0.156
epoch 21,	batch   360,	training loss: 0.128
epoch 21,	batch   370,	training loss: 0.098
epoch 21,	batch   380,	training loss: 0.140
epoch 21,	batch   390,	training loss: 0.124
epoch 21,	batch   400,	training loss: 0.144
epoch 21,	batch   410,	training loss: 0.158
epoch 21,	batch   420,	training loss: 0.128
epoch 21,	batch   430,	training loss: 0.119
epoch 21,	batch   440,	training loss: 0.131
epoch 21,	batch   450,	training loss: 0.104
epoch 21,	batch   460,	training loss: 0.088
epoch 21,	batch   470,	training loss: 0.156
epoch 21,	batch   480,	training loss: 0.139
epoch 21,	batch   490,	training loss: 0.112
epoch 21,	batch   500,	training loss: 0.127
epoch 21,	batch   510,	training loss: 0.164
epoch 21,	batch   520,	training loss: 0.120
epoch 21,	batch   530,	training loss: 0.142
epoch 21,	batch   540,	training loss: 0.124
epoch 21,	batch   550,	training loss: 0.119
epoch 21,	batch   560,	training loss: 0.103
epoch 21,	batch   570,	training loss: 0.153
epoch 21,	batch   580,	training loss: 0.123
epoch 21,	batch   590,	training loss: 0.137
epoch 21,	batch   600,	training loss: 0.127
epoch 21,	batch   610,	training loss: 0.128
epoch 21,	batch   620,	training loss: 0.111
epoch 21,	batch   630,	training loss: 0.153
epoch 21,	batch   640,	training loss: 0.147
epoch 21,	batch   650,	training loss: 0.139
epoch 21,	batch   660,	training loss: 0.129
epoch 21,	batch   670,	training loss: 0.128
epoch 21,	batch   680,	training loss: 0.117
epoch 21,	batch   690,	training loss: 0.124
epoch 21,	batch   700,	training loss: 0.150
epoch 21,	batch   710,	training loss: 0.141
epoch 21,	batch   720,	training loss: 0.145
epoch 21,	batch   730,	training loss: 0.158
epoch 21,	batch   740,	training loss: 0.174
epoch 21,	batch   750,	training loss: 0.139
epoch 21,	batch   760,	training loss: 0.112
epoch 21,	batch   770,	training loss: 0.140
epoch 21,	batch   780,	training loss: 0.142
epoch 21,	batch   790,	training loss: 0.187
epoch 21,	batch   800,	training loss: 0.136
epoch 21,	batch   810,	training loss: 0.141
epoch 21,	batch   820,	training loss: 0.216
epoch 21,	batch   830,	training loss: 0.117
epoch 21,	batch   840,	training loss: 0.168
epoch 21,	batch   850,	training loss: 0.182
epoch 21,	batch   860,	training loss: 0.163
epoch 21,	batch   870,	training loss: 0.116
epoch 21,	batch   880,	training loss: 0.141
epoch 21,	batch   890,	training loss: 0.160
epoch 21,	batch   900,	training loss: 0.134
epoch 21,	batch   910,	training loss: 0.139
epoch 21,	batch   920,	training loss: 0.169
epoch 21,	batch   930,	training loss: 0.128
epoch 21,	batch   940,	training loss: 0.199
epoch 21,	batch   950,	training loss: 0.190
epoch 21,	batch   960,	training loss: 0.136
epoch 21,	batch   970,	training loss: 0.199
epoch 21,	batch   980,	training loss: 0.174
epoch 21,	batch   990,	training loss: 0.171
epoch 21,	batch  1000,	training loss: 0.144
epoch 21,	batch  1010,	training loss: 0.165
epoch 21,	batch  1020,	training loss: 0.156
epoch 21,	batch  1030,	training loss: 0.128
epoch 21,	batch  1040,	training loss: 0.235
epoch 21,	batch  1050,	training loss: 0.209
epoch 21,	batch  1060,	training loss: 0.147
epoch 21,	batch  1070,	training loss: 0.152
epoch 21,	batch  1080,	training loss: 0.150
epoch 21,	batch  1090,	training loss: 0.141
epoch 21,	batch  1100,	training loss: 0.172
epoch 21,	batch  1110,	training loss: 0.136
epoch 21,	batch  1120,	training loss: 0.173
epoch 21,	batch  1130,	training loss: 0.187
epoch 21,	batch  1140,	training loss: 0.192
epoch 21,	batch  1150,	training loss: 0.187
epoch 21,	batch  1160,	training loss: 0.141
epoch 21,	batch  1170,	training loss: 0.141
epoch 21,	batch  1180,	training loss: 0.120
epoch 21,	batch  1190,	training loss: 0.162
epoch 21,	batch  1200,	training loss: 0.162
epoch 21,	batch  1210,	training loss: 0.147
epoch 21,	batch  1220,	training loss: 0.123
epoch 21,	batch  1230,	training loss: 0.198
epoch 21,	batch  1240,	training loss: 0.136
epoch 21,	batch  1250,	training loss: 0.132
epoch 21,	batch  1260,	training loss: 0.128
epoch 21,	batch  1270,	training loss: 0.199
epoch 21,	batch  1280,	training loss: 0.149
epoch 21,	batch  1290,	training loss: 0.171
epoch 21,	batch  1300,	training loss: 0.196
epoch 21,	batch  1310,	training loss: 0.263
epoch 21,	batch  1320,	training loss: 0.241
epoch 21,	batch  1330,	training loss: 0.137
epoch 21,	batch  1340,	training loss: 0.155
epoch 21,	batch  1350,	training loss: 0.154
epoch 21,	batch  1360,	training loss: 0.224
epoch 21,	batch  1370,	training loss: 0.202
epoch 21,	batch  1380,	training loss: 0.179
epoch 21,	batch  1390,	training loss: 0.183
epoch 21,	batch  1400,	training loss: 0.184
epoch 21,	batch  1410,	training loss: 0.233
epoch 21,	batch  1420,	training loss: 0.165
epoch 21,	batch  1430,	training loss: 0.208
epoch 21,	batch  1440,	training loss: 0.119
epoch 21,	batch  1450,	training loss: 0.214
epoch 21,	batch  1460,	training loss: 0.175
epoch 21,	batch  1470,	training loss: 0.148
epoch 21,	batch  1480,	training loss: 0.183
epoch 21,	batch  1490,	training loss: 0.135
epoch 21,	batch  1500,	training loss: 0.172
epoch 21,	batch  1510,	training loss: 0.169
epoch 21,	batch  1520,	training loss: 0.167
epoch 21,	batch  1530,	training loss: 0.180
epoch 21,	batch  1540,	training loss: 0.146
epoch 21,	batch  1550,	training loss: 0.180
epoch 21,	batch  1560,	training loss: 0.167
epoch 21,	batch  1570,	training loss: 0.146
epoch 21,	batch  1580,	training loss: 0.139
epoch 21,	batch  1590,	training loss: 0.147
epoch 21,	batch  1600,	training loss: 0.164
epoch 21,	batch  1610,	training loss: 0.153
epoch 21,	batch  1620,	training loss: 0.143
epoch 21,	batch  1630,	training loss: 0.137
epoch 21,	batch  1640,	training loss: 0.203
epoch 21,	batch  1650,	training loss: 0.139
epoch 21,	batch  1660,	training loss: 0.171
epoch 21,	batch  1670,	training loss: 0.142
epoch 21,	batch  1680,	training loss: 0.195
epoch 21,	batch  1690,	training loss: 0.167
epoch 21,	batch  1700,	training loss: 0.154
epoch 21,	batch  1710,	training loss: 0.169
epoch 21,	batch  1720,	training loss: 0.183
epoch 21,	batch  1730,	training loss: 0.230
epoch 21,	batch  1740,	training loss: 0.141
epoch 21,	batch  1750,	training loss: 0.234
epoch 21,	batch  1760,	training loss: 0.226
epoch 21,	batch  1770,	training loss: 0.167
epoch 21,	batch  1780,	training loss: 0.189
epoch 21,	batch  1790,	training loss: 0.162
epoch 21,	batch  1800,	training loss: 0.210
epoch 21,	batch  1810,	training loss: 0.278
epoch 21,	batch  1820,	training loss: 0.198
epoch 21,	batch  1830,	training loss: 0.213
epoch 21,	batch  1840,	training loss: 0.180
epoch 21,	batch  1850,	training loss: 0.215
epoch 21,	batch  1860,	training loss: 0.206
epoch 21,	batch  1870,	training loss: 0.235
epoch 21,	batch  1880,	training loss: 0.165
epoch 21,	batch  1890,	training loss: 0.249
epoch 21,	batch  1900,	training loss: 0.162
epoch 21,	batch  1910,	training loss: 0.197
epoch 21,	batch  1920,	training loss: 0.163
epoch 21,	batch  1930,	training loss: 0.166
epoch 21,	batch  1940,	training loss: 0.156
epoch 21,	batch  1950,	training loss: 0.167
epoch 21,	batch  1960,	training loss: 0.163
epoch 21,	batch  1970,	training loss: 0.156
epoch 21,	batch  1980,	training loss: 0.204
END OF EPOCH 21
Testing on validation set...
# correct:	6141/54904 = 11.184977415124582%
# off by 1:	10615/54904 = 19.333746175142068%

epoch 22,	batch    10,	training loss: 0.169
epoch 22,	batch    20,	training loss: 0.126
epoch 22,	batch    30,	training loss: 0.136
epoch 22,	batch    40,	training loss: 0.117
epoch 22,	batch    50,	training loss: 0.103
epoch 22,	batch    60,	training loss: 0.118
epoch 22,	batch    70,	training loss: 0.094
epoch 22,	batch    80,	training loss: 0.082
epoch 22,	batch    90,	training loss: 0.137
epoch 22,	batch   100,	training loss: 0.167
epoch 22,	batch   110,	training loss: 0.108
epoch 22,	batch   120,	training loss: 0.145
epoch 22,	batch   130,	training loss: 0.135
epoch 22,	batch   140,	training loss: 0.122
epoch 22,	batch   150,	training loss: 0.114
epoch 22,	batch   160,	training loss: 0.105
epoch 22,	batch   170,	training loss: 0.106
epoch 22,	batch   180,	training loss: 0.116
epoch 22,	batch   190,	training loss: 0.096
epoch 22,	batch   200,	training loss: 0.116
epoch 22,	batch   210,	training loss: 0.155
epoch 22,	batch   220,	training loss: 0.113
epoch 22,	batch   230,	training loss: 0.147
epoch 22,	batch   240,	training loss: 0.166
epoch 22,	batch   250,	training loss: 0.172
epoch 22,	batch   260,	training loss: 0.107
epoch 22,	batch   270,	training loss: 0.126
epoch 22,	batch   280,	training loss: 0.126
epoch 22,	batch   290,	training loss: 0.193
epoch 22,	batch   300,	training loss: 0.151
epoch 22,	batch   310,	training loss: 0.149
epoch 22,	batch   320,	training loss: 0.106
epoch 22,	batch   330,	training loss: 0.124
epoch 22,	batch   340,	training loss: 0.087
epoch 22,	batch   350,	training loss: 0.143
epoch 22,	batch   360,	training loss: 0.081
epoch 22,	batch   370,	training loss: 0.086
epoch 22,	batch   380,	training loss: 0.111
epoch 22,	batch   390,	training loss: 0.094
epoch 22,	batch   400,	training loss: 0.102
epoch 22,	batch   410,	training loss: 0.169
epoch 22,	batch   420,	training loss: 0.154
epoch 22,	batch   430,	training loss: 0.139
epoch 22,	batch   440,	training loss: 0.163
epoch 22,	batch   450,	training loss: 0.159
epoch 22,	batch   460,	training loss: 0.090
epoch 22,	batch   470,	training loss: 0.142
epoch 22,	batch   480,	training loss: 0.110
epoch 22,	batch   490,	training loss: 0.128
epoch 22,	batch   500,	training loss: 0.131
epoch 22,	batch   510,	training loss: 0.119
epoch 22,	batch   520,	training loss: 0.111
epoch 22,	batch   530,	training loss: 0.122
epoch 22,	batch   540,	training loss: 0.107
epoch 22,	batch   550,	training loss: 0.096
epoch 22,	batch   560,	training loss: 0.203
epoch 22,	batch   570,	training loss: 0.193
epoch 22,	batch   580,	training loss: 0.147
epoch 22,	batch   590,	training loss: 0.149
epoch 22,	batch   600,	training loss: 0.137
epoch 22,	batch   610,	training loss: 0.112
epoch 22,	batch   620,	training loss: 0.114
epoch 22,	batch   630,	training loss: 0.116
epoch 22,	batch   640,	training loss: 0.118
epoch 22,	batch   650,	training loss: 0.094
epoch 22,	batch   660,	training loss: 0.109
epoch 22,	batch   670,	training loss: 0.139
epoch 22,	batch   680,	training loss: 0.158
epoch 22,	batch   690,	training loss: 0.157
epoch 22,	batch   700,	training loss: 0.131
epoch 22,	batch   710,	training loss: 0.153
epoch 22,	batch   720,	training loss: 0.196
epoch 22,	batch   730,	training loss: 0.194
epoch 22,	batch   740,	training loss: 0.167
epoch 22,	batch   750,	training loss: 0.210
epoch 22,	batch   760,	training loss: 0.120
epoch 22,	batch   770,	training loss: 0.190
epoch 22,	batch   780,	training loss: 0.177
epoch 22,	batch   790,	training loss: 0.153
epoch 22,	batch   800,	training loss: 0.193
epoch 22,	batch   810,	training loss: 0.155
epoch 22,	batch   820,	training loss: 0.150
epoch 22,	batch   830,	training loss: 0.121
epoch 22,	batch   840,	training loss: 0.204
epoch 22,	batch   850,	training loss: 0.136
epoch 22,	batch   860,	training loss: 0.142
epoch 22,	batch   870,	training loss: 0.138
epoch 22,	batch   880,	training loss: 0.142
epoch 22,	batch   890,	training loss: 0.173
epoch 22,	batch   900,	training loss: 0.160
epoch 22,	batch   910,	training loss: 0.142
epoch 22,	batch   920,	training loss: 0.119
epoch 22,	batch   930,	training loss: 0.131
epoch 22,	batch   940,	training loss: 0.130
epoch 22,	batch   950,	training loss: 0.226
epoch 22,	batch   960,	training loss: 0.121
epoch 22,	batch   970,	training loss: 0.163
epoch 22,	batch   980,	training loss: 0.114
epoch 22,	batch   990,	training loss: 0.164
epoch 22,	batch  1000,	training loss: 0.123
epoch 22,	batch  1010,	training loss: 0.196
epoch 22,	batch  1020,	training loss: 0.163
epoch 22,	batch  1030,	training loss: 0.149
epoch 22,	batch  1040,	training loss: 0.163
epoch 22,	batch  1050,	training loss: 0.191
epoch 22,	batch  1060,	training loss: 0.172
epoch 22,	batch  1070,	training loss: 0.213
epoch 22,	batch  1080,	training loss: 0.120
epoch 22,	batch  1090,	training loss: 0.141
epoch 22,	batch  1100,	training loss: 0.150
epoch 22,	batch  1110,	training loss: 0.173
epoch 22,	batch  1120,	training loss: 0.154
epoch 22,	batch  1130,	training loss: 0.121
epoch 22,	batch  1140,	training loss: 0.159
epoch 22,	batch  1150,	training loss: 0.163
epoch 22,	batch  1160,	training loss: 0.187
epoch 22,	batch  1170,	training loss: 0.155
epoch 22,	batch  1180,	training loss: 0.169
epoch 22,	batch  1190,	training loss: 0.191
epoch 22,	batch  1200,	training loss: 0.181
epoch 22,	batch  1210,	training loss: 0.169
epoch 22,	batch  1220,	training loss: 0.190
epoch 22,	batch  1230,	training loss: 0.199
epoch 22,	batch  1240,	training loss: 0.185
epoch 22,	batch  1250,	training loss: 0.132
epoch 22,	batch  1260,	training loss: 0.158
epoch 22,	batch  1270,	training loss: 0.187
epoch 22,	batch  1280,	training loss: 0.128
epoch 22,	batch  1290,	training loss: 0.150
epoch 22,	batch  1300,	training loss: 0.161
epoch 22,	batch  1310,	training loss: 0.174
epoch 22,	batch  1320,	training loss: 0.130
epoch 22,	batch  1330,	training loss: 0.137
epoch 22,	batch  1340,	training loss: 0.139
epoch 22,	batch  1350,	training loss: 0.176
epoch 22,	batch  1360,	training loss: 0.156
epoch 22,	batch  1370,	training loss: 0.136
epoch 22,	batch  1380,	training loss: 0.130
epoch 22,	batch  1390,	training loss: 0.157
epoch 22,	batch  1400,	training loss: 0.135
epoch 22,	batch  1410,	training loss: 0.131
epoch 22,	batch  1420,	training loss: 0.116
epoch 22,	batch  1430,	training loss: 0.188
epoch 22,	batch  1440,	training loss: 0.171
epoch 22,	batch  1450,	training loss: 0.144
epoch 22,	batch  1460,	training loss: 0.191
epoch 22,	batch  1470,	training loss: 0.140
epoch 22,	batch  1480,	training loss: 0.147
epoch 22,	batch  1490,	training loss: 0.199
epoch 22,	batch  1500,	training loss: 0.188
epoch 22,	batch  1510,	training loss: 0.144
epoch 22,	batch  1520,	training loss: 0.148
epoch 22,	batch  1530,	training loss: 0.192
epoch 22,	batch  1540,	training loss: 0.235
epoch 22,	batch  1550,	training loss: 0.228
epoch 22,	batch  1560,	training loss: 0.139
epoch 22,	batch  1570,	training loss: 0.094
epoch 22,	batch  1580,	training loss: 0.141
epoch 22,	batch  1590,	training loss: 0.208
epoch 22,	batch  1600,	training loss: 0.123
epoch 22,	batch  1610,	training loss: 0.124
epoch 22,	batch  1620,	training loss: 0.160
epoch 22,	batch  1630,	training loss: 0.135
epoch 22,	batch  1640,	training loss: 0.196
epoch 22,	batch  1650,	training loss: 0.132
epoch 22,	batch  1660,	training loss: 0.170
epoch 22,	batch  1670,	training loss: 0.182
epoch 22,	batch  1680,	training loss: 0.137
epoch 22,	batch  1690,	training loss: 0.160
epoch 22,	batch  1700,	training loss: 0.181
epoch 22,	batch  1710,	training loss: 0.133
epoch 22,	batch  1720,	training loss: 0.202
epoch 22,	batch  1730,	training loss: 0.165
epoch 22,	batch  1740,	training loss: 0.227
epoch 22,	batch  1750,	training loss: 0.196
epoch 22,	batch  1760,	training loss: 0.185
epoch 22,	batch  1770,	training loss: 0.232
epoch 22,	batch  1780,	training loss: 0.195
epoch 22,	batch  1790,	training loss: 0.196
epoch 22,	batch  1800,	training loss: 0.155
epoch 22,	batch  1810,	training loss: 0.142
epoch 22,	batch  1820,	training loss: 0.179
epoch 22,	batch  1830,	training loss: 0.136
epoch 22,	batch  1840,	training loss: 0.180
epoch 22,	batch  1850,	training loss: 0.162
epoch 22,	batch  1860,	training loss: 0.170
epoch 22,	batch  1870,	training loss: 0.144
epoch 22,	batch  1880,	training loss: 0.167
epoch 22,	batch  1890,	training loss: 0.162
epoch 22,	batch  1900,	training loss: 0.159
epoch 22,	batch  1910,	training loss: 0.196
epoch 22,	batch  1920,	training loss: 0.273
epoch 22,	batch  1930,	training loss: 0.170
epoch 22,	batch  1940,	training loss: 0.170
epoch 22,	batch  1950,	training loss: 0.224
epoch 22,	batch  1960,	training loss: 0.232
epoch 22,	batch  1970,	training loss: 0.126
epoch 22,	batch  1980,	training loss: 0.156
END OF EPOCH 22
Testing on validation set...
# correct:	6109/54904 = 11.126693865656419%
# off by 1:	10449/54904 = 19.031400262275973%

epoch 23,	batch    10,	training loss: 0.132
epoch 23,	batch    20,	training loss: 0.142
epoch 23,	batch    30,	training loss: 0.082
epoch 23,	batch    40,	training loss: 0.111
epoch 23,	batch    50,	training loss: 0.104
epoch 23,	batch    60,	training loss: 0.078
epoch 23,	batch    70,	training loss: 0.107
epoch 23,	batch    80,	training loss: 0.081
epoch 23,	batch    90,	training loss: 0.095
epoch 23,	batch   100,	training loss: 0.157
epoch 23,	batch   110,	training loss: 0.131
epoch 23,	batch   120,	training loss: 0.113
epoch 23,	batch   130,	training loss: 0.135
epoch 23,	batch   140,	training loss: 0.115
epoch 23,	batch   150,	training loss: 0.078
epoch 23,	batch   160,	training loss: 0.120
epoch 23,	batch   170,	training loss: 0.095
epoch 23,	batch   180,	training loss: 0.076
epoch 23,	batch   190,	training loss: 0.115
epoch 23,	batch   200,	training loss: 0.132
epoch 23,	batch   210,	training loss: 0.118
epoch 23,	batch   220,	training loss: 0.114
epoch 23,	batch   230,	training loss: 0.107
epoch 23,	batch   240,	training loss: 0.117
epoch 23,	batch   250,	training loss: 0.102
epoch 23,	batch   260,	training loss: 0.113
epoch 23,	batch   270,	training loss: 0.136
epoch 23,	batch   280,	training loss: 0.168
epoch 23,	batch   290,	training loss: 0.124
epoch 23,	batch   300,	training loss: 0.083
epoch 23,	batch   310,	training loss: 0.115
epoch 23,	batch   320,	training loss: 0.183
epoch 23,	batch   330,	training loss: 0.097
epoch 23,	batch   340,	training loss: 0.154
epoch 23,	batch   350,	training loss: 0.112
epoch 23,	batch   360,	training loss: 0.129
epoch 23,	batch   370,	training loss: 0.149
epoch 23,	batch   380,	training loss: 0.106
epoch 23,	batch   390,	training loss: 0.082
epoch 23,	batch   400,	training loss: 0.111
epoch 23,	batch   410,	training loss: 0.099
epoch 23,	batch   420,	training loss: 0.091
epoch 23,	batch   430,	training loss: 0.105
epoch 23,	batch   440,	training loss: 0.181
epoch 23,	batch   450,	training loss: 0.133
epoch 23,	batch   460,	training loss: 0.095
epoch 23,	batch   470,	training loss: 0.104
epoch 23,	batch   480,	training loss: 0.128
epoch 23,	batch   490,	training loss: 0.182
epoch 23,	batch   500,	training loss: 0.111
epoch 23,	batch   510,	training loss: 0.118
epoch 23,	batch   520,	training loss: 0.131
epoch 23,	batch   530,	training loss: 0.112
epoch 23,	batch   540,	training loss: 0.122
epoch 23,	batch   550,	training loss: 0.089
epoch 23,	batch   560,	training loss: 0.115
epoch 23,	batch   570,	training loss: 0.110
epoch 23,	batch   580,	training loss: 0.089
epoch 23,	batch   590,	training loss: 0.102
epoch 23,	batch   600,	training loss: 0.075
epoch 23,	batch   610,	training loss: 0.094
epoch 23,	batch   620,	training loss: 0.142
epoch 23,	batch   630,	training loss: 0.106
epoch 23,	batch   640,	training loss: 0.085
epoch 23,	batch   650,	training loss: 0.109
epoch 23,	batch   660,	training loss: 0.086
epoch 23,	batch   670,	training loss: 0.097
epoch 23,	batch   680,	training loss: 0.133
epoch 23,	batch   690,	training loss: 0.087
epoch 23,	batch   700,	training loss: 0.112
epoch 23,	batch   710,	training loss: 0.106
epoch 23,	batch   720,	training loss: 0.100
epoch 23,	batch   730,	training loss: 0.095
epoch 23,	batch   740,	training loss: 0.092
epoch 23,	batch   750,	training loss: 0.117
epoch 23,	batch   760,	training loss: 0.116
epoch 23,	batch   770,	training loss: 0.115
epoch 23,	batch   780,	training loss: 0.097
epoch 23,	batch   790,	training loss: 0.147
epoch 23,	batch   800,	training loss: 0.167
epoch 23,	batch   810,	training loss: 0.094
epoch 23,	batch   820,	training loss: 0.142
epoch 23,	batch   830,	training loss: 0.114
epoch 23,	batch   840,	training loss: 0.159
epoch 23,	batch   850,	training loss: 0.107
epoch 23,	batch   860,	training loss: 0.121
epoch 23,	batch   870,	training loss: 0.128
epoch 23,	batch   880,	training loss: 0.116
epoch 23,	batch   890,	training loss: 0.168
epoch 23,	batch   900,	training loss: 0.107
epoch 23,	batch   910,	training loss: 0.109
epoch 23,	batch   920,	training loss: 0.121
epoch 23,	batch   930,	training loss: 0.112
epoch 23,	batch   940,	training loss: 0.236
epoch 23,	batch   950,	training loss: 0.095
epoch 23,	batch   960,	training loss: 0.103
epoch 23,	batch   970,	training loss: 0.165
epoch 23,	batch   980,	training loss: 0.110
epoch 23,	batch   990,	training loss: 0.137
epoch 23,	batch  1000,	training loss: 0.140
epoch 23,	batch  1010,	training loss: 0.103
epoch 23,	batch  1020,	training loss: 0.136
epoch 23,	batch  1030,	training loss: 0.102
epoch 23,	batch  1040,	training loss: 0.150
epoch 23,	batch  1050,	training loss: 0.098
epoch 23,	batch  1060,	training loss: 0.137
epoch 23,	batch  1070,	training loss: 0.131
epoch 23,	batch  1080,	training loss: 0.143
epoch 23,	batch  1090,	training loss: 0.103
epoch 23,	batch  1100,	training loss: 0.153
epoch 23,	batch  1110,	training loss: 0.122
epoch 23,	batch  1120,	training loss: 0.119
epoch 23,	batch  1130,	training loss: 0.121
epoch 23,	batch  1140,	training loss: 0.158
epoch 23,	batch  1150,	training loss: 0.145
epoch 23,	batch  1160,	training loss: 0.174
epoch 23,	batch  1170,	training loss: 0.094
epoch 23,	batch  1180,	training loss: 0.116
epoch 23,	batch  1190,	training loss: 0.102
epoch 23,	batch  1200,	training loss: 0.099
epoch 23,	batch  1210,	training loss: 0.100
epoch 23,	batch  1220,	training loss: 0.119
epoch 23,	batch  1230,	training loss: 0.126
epoch 23,	batch  1240,	training loss: 0.148
epoch 23,	batch  1250,	training loss: 0.129
epoch 23,	batch  1260,	training loss: 0.194
epoch 23,	batch  1270,	training loss: 0.150
epoch 23,	batch  1280,	training loss: 0.132
epoch 23,	batch  1290,	training loss: 0.115
epoch 23,	batch  1300,	training loss: 0.146
epoch 23,	batch  1310,	training loss: 0.147
epoch 23,	batch  1320,	training loss: 0.137
epoch 23,	batch  1330,	training loss: 0.116
epoch 23,	batch  1340,	training loss: 0.191
epoch 23,	batch  1350,	training loss: 0.127
epoch 23,	batch  1360,	training loss: 0.100
epoch 23,	batch  1370,	training loss: 0.184
epoch 23,	batch  1380,	training loss: 0.133
epoch 23,	batch  1390,	training loss: 0.143
epoch 23,	batch  1400,	training loss: 0.123
epoch 23,	batch  1410,	training loss: 0.188
epoch 23,	batch  1420,	training loss: 0.142
epoch 23,	batch  1430,	training loss: 0.152
epoch 23,	batch  1440,	training loss: 0.111
epoch 23,	batch  1450,	training loss: 0.178
epoch 23,	batch  1460,	training loss: 0.117
epoch 23,	batch  1470,	training loss: 0.107
epoch 23,	batch  1480,	training loss: 0.124
epoch 23,	batch  1490,	training loss: 0.138
epoch 23,	batch  1500,	training loss: 0.116
epoch 23,	batch  1510,	training loss: 0.139
epoch 23,	batch  1520,	training loss: 0.157
epoch 23,	batch  1530,	training loss: 0.161
epoch 23,	batch  1540,	training loss: 0.155
epoch 23,	batch  1550,	training loss: 0.136
epoch 23,	batch  1560,	training loss: 0.137
epoch 23,	batch  1570,	training loss: 0.176
epoch 23,	batch  1580,	training loss: 0.164
epoch 23,	batch  1590,	training loss: 0.189
epoch 23,	batch  1600,	training loss: 0.169
epoch 23,	batch  1610,	training loss: 0.159
epoch 23,	batch  1620,	training loss: 0.119
epoch 23,	batch  1630,	training loss: 0.161
epoch 23,	batch  1640,	training loss: 0.138
epoch 23,	batch  1650,	training loss: 0.165
epoch 23,	batch  1660,	training loss: 0.131
epoch 23,	batch  1670,	training loss: 0.123
epoch 23,	batch  1680,	training loss: 0.126
epoch 23,	batch  1690,	training loss: 0.141
epoch 23,	batch  1700,	training loss: 0.157
epoch 23,	batch  1710,	training loss: 0.153
epoch 23,	batch  1720,	training loss: 0.147
epoch 23,	batch  1730,	training loss: 0.137
epoch 23,	batch  1740,	training loss: 0.228
epoch 23,	batch  1750,	training loss: 0.140
epoch 23,	batch  1760,	training loss: 0.179
epoch 23,	batch  1770,	training loss: 0.259
epoch 23,	batch  1780,	training loss: 0.197
epoch 23,	batch  1790,	training loss: 0.135
epoch 23,	batch  1800,	training loss: 0.165
epoch 23,	batch  1810,	training loss: 0.130
epoch 23,	batch  1820,	training loss: 0.177
epoch 23,	batch  1830,	training loss: 0.105
epoch 23,	batch  1840,	training loss: 0.139
epoch 23,	batch  1850,	training loss: 0.149
epoch 23,	batch  1860,	training loss: 0.155
epoch 23,	batch  1870,	training loss: 0.135
epoch 23,	batch  1880,	training loss: 0.189
epoch 23,	batch  1890,	training loss: 0.163
epoch 23,	batch  1900,	training loss: 0.129
epoch 23,	batch  1910,	training loss: 0.144
epoch 23,	batch  1920,	training loss: 0.152
epoch 23,	batch  1930,	training loss: 0.144
epoch 23,	batch  1940,	training loss: 0.118
epoch 23,	batch  1950,	training loss: 0.132
epoch 23,	batch  1960,	training loss: 0.138
epoch 23,	batch  1970,	training loss: 0.130
epoch 23,	batch  1980,	training loss: 0.109
END OF EPOCH 23
Testing on validation set...
# correct:	6348/54904 = 11.561999125746757%
# off by 1:	10980/54904 = 19.998542911263296%

epoch 24,	batch    10,	training loss: 0.092
epoch 24,	batch    20,	training loss: 0.158
epoch 24,	batch    30,	training loss: 0.124
epoch 24,	batch    40,	training loss: 0.116
epoch 24,	batch    50,	training loss: 0.155
epoch 24,	batch    60,	training loss: 0.080
epoch 24,	batch    70,	training loss: 0.082
epoch 24,	batch    80,	training loss: 0.112
epoch 24,	batch    90,	training loss: 0.157
epoch 24,	batch   100,	training loss: 0.136
epoch 24,	batch   110,	training loss: 0.077
epoch 24,	batch   120,	training loss: 0.106
epoch 24,	batch   130,	training loss: 0.188
epoch 24,	batch   140,	training loss: 0.112
epoch 24,	batch   150,	training loss: 0.130
epoch 24,	batch   160,	training loss: 0.089
epoch 24,	batch   170,	training loss: 0.094
epoch 24,	batch   180,	training loss: 0.092
epoch 24,	batch   190,	training loss: 0.092
epoch 24,	batch   200,	training loss: 0.118
epoch 24,	batch   210,	training loss: 0.111
epoch 24,	batch   220,	training loss: 0.114
epoch 24,	batch   230,	training loss: 0.097
epoch 24,	batch   240,	training loss: 0.087
epoch 24,	batch   250,	training loss: 0.058
epoch 24,	batch   260,	training loss: 0.099
epoch 24,	batch   270,	training loss: 0.106
epoch 24,	batch   280,	training loss: 0.086
epoch 24,	batch   290,	training loss: 0.084
epoch 24,	batch   300,	training loss: 0.106
epoch 24,	batch   310,	training loss: 0.096
epoch 24,	batch   320,	training loss: 0.142
epoch 24,	batch   330,	training loss: 0.112
epoch 24,	batch   340,	training loss: 0.064
epoch 24,	batch   350,	training loss: 0.124
epoch 24,	batch   360,	training loss: 0.111
epoch 24,	batch   370,	training loss: 0.101
epoch 24,	batch   380,	training loss: 0.077
epoch 24,	batch   390,	training loss: 0.083
epoch 24,	batch   400,	training loss: 0.148
epoch 24,	batch   410,	training loss: 0.105
epoch 24,	batch   420,	training loss: 0.082
epoch 24,	batch   430,	training loss: 0.096
epoch 24,	batch   440,	training loss: 0.062
epoch 24,	batch   450,	training loss: 0.076
epoch 24,	batch   460,	training loss: 0.095
epoch 24,	batch   470,	training loss: 0.177
epoch 24,	batch   480,	training loss: 0.124
epoch 24,	batch   490,	training loss: 0.103
epoch 24,	batch   500,	training loss: 0.107
epoch 24,	batch   510,	training loss: 0.087
epoch 24,	batch   520,	training loss: 0.104
epoch 24,	batch   530,	training loss: 0.109
epoch 24,	batch   540,	training loss: 0.093
epoch 24,	batch   550,	training loss: 0.116
epoch 24,	batch   560,	training loss: 0.086
epoch 24,	batch   570,	training loss: 0.061
epoch 24,	batch   580,	training loss: 0.080
epoch 24,	batch   590,	training loss: 0.097
epoch 24,	batch   600,	training loss: 0.085
epoch 24,	batch   610,	training loss: 0.151
epoch 24,	batch   620,	training loss: 0.087
epoch 24,	batch   630,	training loss: 0.106
epoch 24,	batch   640,	training loss: 0.121
epoch 24,	batch   650,	training loss: 0.180
epoch 24,	batch   660,	training loss: 0.109
epoch 24,	batch   670,	training loss: 0.166
epoch 24,	batch   680,	training loss: 0.152
epoch 24,	batch   690,	training loss: 0.088
epoch 24,	batch   700,	training loss: 0.107
epoch 24,	batch   710,	training loss: 0.085
epoch 24,	batch   720,	training loss: 0.112
epoch 24,	batch   730,	training loss: 0.138
epoch 24,	batch   740,	training loss: 0.092
epoch 24,	batch   750,	training loss: 0.114
epoch 24,	batch   760,	training loss: 0.100
epoch 24,	batch   770,	training loss: 0.170
epoch 24,	batch   780,	training loss: 0.091
epoch 24,	batch   790,	training loss: 0.115
epoch 24,	batch   800,	training loss: 0.122
epoch 24,	batch   810,	training loss: 0.085
epoch 24,	batch   820,	training loss: 0.114
epoch 24,	batch   830,	training loss: 0.084
epoch 24,	batch   840,	training loss: 0.089
epoch 24,	batch   850,	training loss: 0.095
epoch 24,	batch   860,	training loss: 0.184
epoch 24,	batch   870,	training loss: 0.103
epoch 24,	batch   880,	training loss: 0.103
epoch 24,	batch   890,	training loss: 0.101
epoch 24,	batch   900,	training loss: 0.138
epoch 24,	batch   910,	training loss: 0.111
epoch 24,	batch   920,	training loss: 0.127
epoch 24,	batch   930,	training loss: 0.107
epoch 24,	batch   940,	training loss: 0.108
epoch 24,	batch   950,	training loss: 0.091
epoch 24,	batch   960,	training loss: 0.112
epoch 24,	batch   970,	training loss: 0.112
epoch 24,	batch   980,	training loss: 0.158
epoch 24,	batch   990,	training loss: 0.110
epoch 24,	batch  1000,	training loss: 0.082
epoch 24,	batch  1010,	training loss: 0.116
epoch 24,	batch  1020,	training loss: 0.074
epoch 24,	batch  1030,	training loss: 0.092
epoch 24,	batch  1040,	training loss: 0.117
epoch 24,	batch  1050,	training loss: 0.118
epoch 24,	batch  1060,	training loss: 0.056
epoch 24,	batch  1070,	training loss: 0.148
epoch 24,	batch  1080,	training loss: 0.144
epoch 24,	batch  1090,	training loss: 0.114
epoch 24,	batch  1100,	training loss: 0.128
epoch 24,	batch  1110,	training loss: 0.093
epoch 24,	batch  1120,	training loss: 0.100
epoch 24,	batch  1130,	training loss: 0.175
epoch 24,	batch  1140,	training loss: 0.142
epoch 24,	batch  1150,	training loss: 0.097
epoch 24,	batch  1160,	training loss: 0.143
epoch 24,	batch  1170,	training loss: 0.106
epoch 24,	batch  1180,	training loss: 0.147
epoch 24,	batch  1190,	training loss: 0.136
epoch 24,	batch  1200,	training loss: 0.145
epoch 24,	batch  1210,	training loss: 0.123
epoch 24,	batch  1220,	training loss: 0.102
epoch 24,	batch  1230,	training loss: 0.126
epoch 24,	batch  1240,	training loss: 0.103
epoch 24,	batch  1250,	training loss: 0.109
epoch 24,	batch  1260,	training loss: 0.144
epoch 24,	batch  1270,	training loss: 0.114
epoch 24,	batch  1280,	training loss: 0.113
epoch 24,	batch  1290,	training loss: 0.106
epoch 24,	batch  1300,	training loss: 0.127
epoch 24,	batch  1310,	training loss: 0.123
epoch 24,	batch  1320,	training loss: 0.140
epoch 24,	batch  1330,	training loss: 0.174
epoch 24,	batch  1340,	training loss: 0.163
epoch 24,	batch  1350,	training loss: 0.152
epoch 24,	batch  1360,	training loss: 0.134
epoch 24,	batch  1370,	training loss: 0.119
epoch 24,	batch  1380,	training loss: 0.122
epoch 24,	batch  1390,	training loss: 0.108
epoch 24,	batch  1400,	training loss: 0.149
epoch 24,	batch  1410,	training loss: 0.193
epoch 24,	batch  1420,	training loss: 0.121
epoch 24,	batch  1430,	training loss: 0.182
epoch 24,	batch  1440,	training loss: 0.149
epoch 24,	batch  1450,	training loss: 0.141
epoch 24,	batch  1460,	training loss: 0.114
epoch 24,	batch  1470,	training loss: 0.128
epoch 24,	batch  1480,	training loss: 0.144
epoch 24,	batch  1490,	training loss: 0.150
epoch 24,	batch  1500,	training loss: 0.155
epoch 24,	batch  1510,	training loss: 0.110
epoch 24,	batch  1520,	training loss: 0.148
epoch 24,	batch  1530,	training loss: 0.164
epoch 24,	batch  1540,	training loss: 0.090
epoch 24,	batch  1550,	training loss: 0.081
epoch 24,	batch  1560,	training loss: 0.090
epoch 24,	batch  1570,	training loss: 0.090
epoch 24,	batch  1580,	training loss: 0.137
epoch 24,	batch  1590,	training loss: 0.129
epoch 24,	batch  1600,	training loss: 0.121
epoch 24,	batch  1610,	training loss: 0.166
epoch 24,	batch  1620,	training loss: 0.125
epoch 24,	batch  1630,	training loss: 0.135
epoch 24,	batch  1640,	training loss: 0.101
epoch 24,	batch  1650,	training loss: 0.154
epoch 24,	batch  1660,	training loss: 0.245
epoch 24,	batch  1670,	training loss: 0.167
epoch 24,	batch  1680,	training loss: 0.157
epoch 24,	batch  1690,	training loss: 0.168
epoch 24,	batch  1700,	training loss: 0.111
epoch 24,	batch  1710,	training loss: 0.130
epoch 24,	batch  1720,	training loss: 0.147
epoch 24,	batch  1730,	training loss: 0.163
epoch 24,	batch  1740,	training loss: 0.126
epoch 24,	batch  1750,	training loss: 0.198
epoch 24,	batch  1760,	training loss: 0.125
epoch 24,	batch  1770,	training loss: 0.102
epoch 24,	batch  1780,	training loss: 0.108
epoch 24,	batch  1790,	training loss: 0.112
epoch 24,	batch  1800,	training loss: 0.132
epoch 24,	batch  1810,	training loss: 0.153
epoch 24,	batch  1820,	training loss: 0.123
epoch 24,	batch  1830,	training loss: 0.151
epoch 24,	batch  1840,	training loss: 0.108
epoch 24,	batch  1850,	training loss: 0.114
epoch 24,	batch  1860,	training loss: 0.140
epoch 24,	batch  1870,	training loss: 0.114
epoch 24,	batch  1880,	training loss: 0.127
epoch 24,	batch  1890,	training loss: 0.093
epoch 24,	batch  1900,	training loss: 0.164
epoch 24,	batch  1910,	training loss: 0.162
epoch 24,	batch  1920,	training loss: 0.159
epoch 24,	batch  1930,	training loss: 0.179
epoch 24,	batch  1940,	training loss: 0.182
epoch 24,	batch  1950,	training loss: 0.135
epoch 24,	batch  1960,	training loss: 0.112
epoch 24,	batch  1970,	training loss: 0.115
epoch 24,	batch  1980,	training loss: 0.164
END OF EPOCH 24
Testing on validation set...
# correct:	6546/54904 = 11.922628588081015%
# off by 1:	10865/54904 = 19.789086405362088%

epoch 25,	batch    10,	training loss: 0.098
epoch 25,	batch    20,	training loss: 0.135
epoch 25,	batch    30,	training loss: 0.095
epoch 25,	batch    40,	training loss: 0.092
epoch 25,	batch    50,	training loss: 0.089
epoch 25,	batch    60,	training loss: 0.068
epoch 25,	batch    70,	training loss: 0.127
epoch 25,	batch    80,	training loss: 0.083
epoch 25,	batch    90,	training loss: 0.097
epoch 25,	batch   100,	training loss: 0.083
epoch 25,	batch   110,	training loss: 0.099
epoch 25,	batch   120,	training loss: 0.073
epoch 25,	batch   130,	training loss: 0.062
epoch 25,	batch   140,	training loss: 0.150
epoch 25,	batch   150,	training loss: 0.079
epoch 25,	batch   160,	training loss: 0.063
epoch 25,	batch   170,	training loss: 0.123
epoch 25,	batch   180,	training loss: 0.077
epoch 25,	batch   190,	training loss: 0.087
epoch 25,	batch   200,	training loss: 0.131
epoch 25,	batch   210,	training loss: 0.101
epoch 25,	batch   220,	training loss: 0.111
epoch 25,	batch   230,	training loss: 0.073
epoch 25,	batch   240,	training loss: 0.085
epoch 25,	batch   250,	training loss: 0.143
epoch 25,	batch   260,	training loss: 0.116
epoch 25,	batch   270,	training loss: 0.119
epoch 25,	batch   280,	training loss: 0.096
epoch 25,	batch   290,	training loss: 0.075
epoch 25,	batch   300,	training loss: 0.090
epoch 25,	batch   310,	training loss: 0.082
epoch 25,	batch   320,	training loss: 0.061
epoch 25,	batch   330,	training loss: 0.050
epoch 25,	batch   340,	training loss: 0.068
epoch 25,	batch   350,	training loss: 0.100
epoch 25,	batch   360,	training loss: 0.097
epoch 25,	batch   370,	training loss: 0.090
epoch 25,	batch   380,	training loss: 0.105
epoch 25,	batch   390,	training loss: 0.114
epoch 25,	batch   400,	training loss: 0.087
epoch 25,	batch   410,	training loss: 0.109
epoch 25,	batch   420,	training loss: 0.148
epoch 25,	batch   430,	training loss: 0.136
epoch 25,	batch   440,	training loss: 0.127
epoch 25,	batch   450,	training loss: 0.113
epoch 25,	batch   460,	training loss: 0.082
epoch 25,	batch   470,	training loss: 0.058
epoch 25,	batch   480,	training loss: 0.103
epoch 25,	batch   490,	training loss: 0.097
epoch 25,	batch   500,	training loss: 0.086
epoch 25,	batch   510,	training loss: 0.064
epoch 25,	batch   520,	training loss: 0.100
epoch 25,	batch   530,	training loss: 0.096
epoch 25,	batch   540,	training loss: 0.126
epoch 25,	batch   550,	training loss: 0.107
epoch 25,	batch   560,	training loss: 0.110
epoch 25,	batch   570,	training loss: 0.082
epoch 25,	batch   580,	training loss: 0.096
epoch 25,	batch   590,	training loss: 0.066
epoch 25,	batch   600,	training loss: 0.078
epoch 25,	batch   610,	training loss: 0.136
epoch 25,	batch   620,	training loss: 0.140
epoch 25,	batch   630,	training loss: 0.205
epoch 25,	batch   640,	training loss: 0.086
epoch 25,	batch   650,	training loss: 0.076
epoch 25,	batch   660,	training loss: 0.090
epoch 25,	batch   670,	training loss: 0.102
epoch 25,	batch   680,	training loss: 0.074
epoch 25,	batch   690,	training loss: 0.082
epoch 25,	batch   700,	training loss: 0.081
epoch 25,	batch   710,	training loss: 0.116
epoch 25,	batch   720,	training loss: 0.099
epoch 25,	batch   730,	training loss: 0.056
epoch 25,	batch   740,	training loss: 0.068
epoch 25,	batch   750,	training loss: 0.080
epoch 25,	batch   760,	training loss: 0.105
epoch 25,	batch   770,	training loss: 0.108
epoch 25,	batch   780,	training loss: 0.112
epoch 25,	batch   790,	training loss: 0.109
epoch 25,	batch   800,	training loss: 0.152
epoch 25,	batch   810,	training loss: 0.110
epoch 25,	batch   820,	training loss: 0.092
epoch 25,	batch   830,	training loss: 0.133
epoch 25,	batch   840,	training loss: 0.168
epoch 25,	batch   850,	training loss: 0.128
epoch 25,	batch   860,	training loss: 0.100
epoch 25,	batch   870,	training loss: 0.086
epoch 25,	batch   880,	training loss: 0.131
epoch 25,	batch   890,	training loss: 0.074
epoch 25,	batch   900,	training loss: 0.115
epoch 25,	batch   910,	training loss: 0.127
epoch 25,	batch   920,	training loss: 0.094
epoch 25,	batch   930,	training loss: 0.094
epoch 25,	batch   940,	training loss: 0.103
epoch 25,	batch   950,	training loss: 0.162
epoch 25,	batch   960,	training loss: 0.096
epoch 25,	batch   970,	training loss: 0.118
epoch 25,	batch   980,	training loss: 0.096
epoch 25,	batch   990,	training loss: 0.100
epoch 25,	batch  1000,	training loss: 0.091
epoch 25,	batch  1010,	training loss: 0.091
epoch 25,	batch  1020,	training loss: 0.121
epoch 25,	batch  1030,	training loss: 0.094
epoch 25,	batch  1040,	training loss: 0.100
epoch 25,	batch  1050,	training loss: 0.110
epoch 25,	batch  1060,	training loss: 0.074
epoch 25,	batch  1070,	training loss: 0.075
epoch 25,	batch  1080,	training loss: 0.120
epoch 25,	batch  1090,	training loss: 0.093
epoch 25,	batch  1100,	training loss: 0.089
epoch 25,	batch  1110,	training loss: 0.116
epoch 25,	batch  1120,	training loss: 0.112
epoch 25,	batch  1130,	training loss: 0.110
epoch 25,	batch  1140,	training loss: 0.100
epoch 25,	batch  1150,	training loss: 0.083
epoch 25,	batch  1160,	training loss: 0.127
epoch 25,	batch  1170,	training loss: 0.086
epoch 25,	batch  1180,	training loss: 0.123
epoch 25,	batch  1190,	training loss: 0.082
epoch 25,	batch  1200,	training loss: 0.168
epoch 25,	batch  1210,	training loss: 0.104
epoch 25,	batch  1220,	training loss: 0.093
epoch 25,	batch  1230,	training loss: 0.107
epoch 25,	batch  1240,	training loss: 0.110
epoch 25,	batch  1250,	training loss: 0.092
epoch 25,	batch  1260,	training loss: 0.099
epoch 25,	batch  1270,	training loss: 0.126
epoch 25,	batch  1280,	training loss: 0.100
epoch 25,	batch  1290,	training loss: 0.085
epoch 25,	batch  1300,	training loss: 0.112
epoch 25,	batch  1310,	training loss: 0.099
epoch 25,	batch  1320,	training loss: 0.117
epoch 25,	batch  1330,	training loss: 0.114
epoch 25,	batch  1340,	training loss: 0.138
epoch 25,	batch  1350,	training loss: 0.063
epoch 25,	batch  1360,	training loss: 0.129
epoch 25,	batch  1370,	training loss: 0.168
epoch 25,	batch  1380,	training loss: 0.087
epoch 25,	batch  1390,	training loss: 0.099
epoch 25,	batch  1400,	training loss: 0.112
epoch 25,	batch  1410,	training loss: 0.089
epoch 25,	batch  1420,	training loss: 0.132
epoch 25,	batch  1430,	training loss: 0.136
epoch 25,	batch  1440,	training loss: 0.125
epoch 25,	batch  1450,	training loss: 0.118
epoch 25,	batch  1460,	training loss: 0.117
epoch 25,	batch  1470,	training loss: 0.136
epoch 25,	batch  1480,	training loss: 0.146
epoch 25,	batch  1490,	training loss: 0.113
epoch 25,	batch  1500,	training loss: 0.139
epoch 25,	batch  1510,	training loss: 0.103
epoch 25,	batch  1520,	training loss: 0.100
epoch 25,	batch  1530,	training loss: 0.100
epoch 25,	batch  1540,	training loss: 0.112
epoch 25,	batch  1550,	training loss: 0.106
epoch 25,	batch  1560,	training loss: 0.141
epoch 25,	batch  1570,	training loss: 0.124
epoch 25,	batch  1580,	training loss: 0.082
epoch 25,	batch  1590,	training loss: 0.150
epoch 25,	batch  1600,	training loss: 0.120
epoch 25,	batch  1610,	training loss: 0.103
epoch 25,	batch  1620,	training loss: 0.091
epoch 25,	batch  1630,	training loss: 0.119
epoch 25,	batch  1640,	training loss: 0.072
epoch 25,	batch  1650,	training loss: 0.100
epoch 25,	batch  1660,	training loss: 0.105
epoch 25,	batch  1670,	training loss: 0.119
epoch 25,	batch  1680,	training loss: 0.075
epoch 25,	batch  1690,	training loss: 0.165
epoch 25,	batch  1700,	training loss: 0.102
epoch 25,	batch  1710,	training loss: 0.108
epoch 25,	batch  1720,	training loss: 0.097
epoch 25,	batch  1730,	training loss: 0.090
epoch 25,	batch  1740,	training loss: 0.078
epoch 25,	batch  1750,	training loss: 0.097
epoch 25,	batch  1760,	training loss: 0.121
epoch 25,	batch  1770,	training loss: 0.082
epoch 25,	batch  1780,	training loss: 0.135
epoch 25,	batch  1790,	training loss: 0.103
epoch 25,	batch  1800,	training loss: 0.177
epoch 25,	batch  1810,	training loss: 0.104
epoch 25,	batch  1820,	training loss: 0.104
epoch 25,	batch  1830,	training loss: 0.106
epoch 25,	batch  1840,	training loss: 0.140
epoch 25,	batch  1850,	training loss: 0.111
epoch 25,	batch  1860,	training loss: 0.098
epoch 25,	batch  1870,	training loss: 0.107
epoch 25,	batch  1880,	training loss: 0.167
epoch 25,	batch  1890,	training loss: 0.121
epoch 25,	batch  1900,	training loss: 0.139
epoch 25,	batch  1910,	training loss: 0.246
epoch 25,	batch  1920,	training loss: 0.113
epoch 25,	batch  1930,	training loss: 0.150
epoch 25,	batch  1940,	training loss: 0.116
epoch 25,	batch  1950,	training loss: 0.145
epoch 25,	batch  1960,	training loss: 0.091
epoch 25,	batch  1970,	training loss: 0.129
epoch 25,	batch  1980,	training loss: 0.131
END OF EPOCH 25
Testing on validation set...
# correct:	6149/54904 = 11.199548302491621%
# off by 1:	11303/54904 = 20.586842488707564%

epoch 26,	batch    10,	training loss: 0.089
epoch 26,	batch    20,	training loss: 0.082
epoch 26,	batch    30,	training loss: 0.127
epoch 26,	batch    40,	training loss: 0.085
epoch 26,	batch    50,	training loss: 0.061
epoch 26,	batch    60,	training loss: 0.111
epoch 26,	batch    70,	training loss: 0.061
epoch 26,	batch    80,	training loss: 0.086
epoch 26,	batch    90,	training loss: 0.084
epoch 26,	batch   100,	training loss: 0.092
epoch 26,	batch   110,	training loss: 0.097
epoch 26,	batch   120,	training loss: 0.092
epoch 26,	batch   130,	training loss: 0.071
epoch 26,	batch   140,	training loss: 0.091
epoch 26,	batch   150,	training loss: 0.065
epoch 26,	batch   160,	training loss: 0.059
epoch 26,	batch   170,	training loss: 0.072
epoch 26,	batch   180,	training loss: 0.060
epoch 26,	batch   190,	training loss: 0.117
epoch 26,	batch   200,	training loss: 0.080
epoch 26,	batch   210,	training loss: 0.051
epoch 26,	batch   220,	training loss: 0.087
epoch 26,	batch   230,	training loss: 0.094
epoch 26,	batch   240,	training loss: 0.102
epoch 26,	batch   250,	training loss: 0.089
epoch 26,	batch   260,	training loss: 0.138
epoch 26,	batch   270,	training loss: 0.143
epoch 26,	batch   280,	training loss: 0.112
epoch 26,	batch   290,	training loss: 0.090
epoch 26,	batch   300,	training loss: 0.058
epoch 26,	batch   310,	training loss: 0.084
epoch 26,	batch   320,	training loss: 0.090
epoch 26,	batch   330,	training loss: 0.075
epoch 26,	batch   340,	training loss: 0.087
epoch 26,	batch   350,	training loss: 0.096
epoch 26,	batch   360,	training loss: 0.073
epoch 26,	batch   370,	training loss: 0.113
epoch 26,	batch   380,	training loss: 0.077
epoch 26,	batch   390,	training loss: 0.099
epoch 26,	batch   400,	training loss: 0.071
epoch 26,	batch   410,	training loss: 0.099
epoch 26,	batch   420,	training loss: 0.085
epoch 26,	batch   430,	training loss: 0.067
epoch 26,	batch   440,	training loss: 0.081
epoch 26,	batch   450,	training loss: 0.109
epoch 26,	batch   460,	training loss: 0.067
epoch 26,	batch   470,	training loss: 0.089
epoch 26,	batch   480,	training loss: 0.136
epoch 26,	batch   490,	training loss: 0.085
epoch 26,	batch   500,	training loss: 0.086
epoch 26,	batch   510,	training loss: 0.100
epoch 26,	batch   520,	training loss: 0.100
epoch 26,	batch   530,	training loss: 0.092
epoch 26,	batch   540,	training loss: 0.073
epoch 26,	batch   550,	training loss: 0.076
epoch 26,	batch   560,	training loss: 0.129
epoch 26,	batch   570,	training loss: 0.085
epoch 26,	batch   580,	training loss: 0.085
epoch 26,	batch   590,	training loss: 0.091
epoch 26,	batch   600,	training loss: 0.078
epoch 26,	batch   610,	training loss: 0.076
epoch 26,	batch   620,	training loss: 0.056
epoch 26,	batch   630,	training loss: 0.073
epoch 26,	batch   640,	training loss: 0.070
epoch 26,	batch   650,	training loss: 0.059
epoch 26,	batch   660,	training loss: 0.073
epoch 26,	batch   670,	training loss: 0.113
epoch 26,	batch   680,	training loss: 0.111
epoch 26,	batch   690,	training loss: 0.106
epoch 26,	batch   700,	training loss: 0.097
epoch 26,	batch   710,	training loss: 0.073
epoch 26,	batch   720,	training loss: 0.068
epoch 26,	batch   730,	training loss: 0.114
epoch 26,	batch   740,	training loss: 0.085
epoch 26,	batch   750,	training loss: 0.064
epoch 26,	batch   760,	training loss: 0.111
epoch 26,	batch   770,	training loss: 0.079
epoch 26,	batch   780,	training loss: 0.095
epoch 26,	batch   790,	training loss: 0.095
epoch 26,	batch   800,	training loss: 0.102
epoch 26,	batch   810,	training loss: 0.090
epoch 26,	batch   820,	training loss: 0.071
epoch 26,	batch   830,	training loss: 0.073
epoch 26,	batch   840,	training loss: 0.087
epoch 26,	batch   850,	training loss: 0.058
epoch 26,	batch   860,	training loss: 0.054
epoch 26,	batch   870,	training loss: 0.052
epoch 26,	batch   880,	training loss: 0.095
epoch 26,	batch   890,	training loss: 0.102
epoch 26,	batch   900,	training loss: 0.083
epoch 26,	batch   910,	training loss: 0.081
epoch 26,	batch   920,	training loss: 0.100
epoch 26,	batch   930,	training loss: 0.058
epoch 26,	batch   940,	training loss: 0.081
epoch 26,	batch   950,	training loss: 0.065
epoch 26,	batch   960,	training loss: 0.088
epoch 26,	batch   970,	training loss: 0.103
epoch 26,	batch   980,	training loss: 0.077
epoch 26,	batch   990,	training loss: 0.075
epoch 26,	batch  1000,	training loss: 0.087
epoch 26,	batch  1010,	training loss: 0.076
epoch 26,	batch  1020,	training loss: 0.087
epoch 26,	batch  1030,	training loss: 0.090
epoch 26,	batch  1040,	training loss: 0.128
epoch 26,	batch  1050,	training loss: 0.070
epoch 26,	batch  1060,	training loss: 0.085
epoch 26,	batch  1070,	training loss: 0.100
epoch 26,	batch  1080,	training loss: 0.079
epoch 26,	batch  1090,	training loss: 0.087
epoch 26,	batch  1100,	training loss: 0.065
epoch 26,	batch  1110,	training loss: 0.069
epoch 26,	batch  1120,	training loss: 0.113
epoch 26,	batch  1130,	training loss: 0.101
epoch 26,	batch  1140,	training loss: 0.061
epoch 26,	batch  1150,	training loss: 0.086
epoch 26,	batch  1160,	training loss: 0.074
epoch 26,	batch  1170,	training loss: 0.176
epoch 26,	batch  1180,	training loss: 0.086
epoch 26,	batch  1190,	training loss: 0.088
epoch 26,	batch  1200,	training loss: 0.074
epoch 26,	batch  1210,	training loss: 0.127
epoch 26,	batch  1220,	training loss: 0.089
epoch 26,	batch  1230,	training loss: 0.071
epoch 26,	batch  1240,	training loss: 0.078
epoch 26,	batch  1250,	training loss: 0.090
epoch 26,	batch  1260,	training loss: 0.089
epoch 26,	batch  1270,	training loss: 0.137
epoch 26,	batch  1280,	training loss: 0.102
epoch 26,	batch  1290,	training loss: 0.076
epoch 26,	batch  1300,	training loss: 0.098
epoch 26,	batch  1310,	training loss: 0.099
epoch 26,	batch  1320,	training loss: 0.071
epoch 26,	batch  1330,	training loss: 0.081
epoch 26,	batch  1340,	training loss: 0.115
epoch 26,	batch  1350,	training loss: 0.138
epoch 26,	batch  1360,	training loss: 0.075
epoch 26,	batch  1370,	training loss: 0.144
epoch 26,	batch  1380,	training loss: 0.097
epoch 26,	batch  1390,	training loss: 0.088
epoch 26,	batch  1400,	training loss: 0.084
epoch 26,	batch  1410,	training loss: 0.088
epoch 26,	batch  1420,	training loss: 0.077
epoch 26,	batch  1430,	training loss: 0.070
epoch 26,	batch  1440,	training loss: 0.082
epoch 26,	batch  1450,	training loss: 0.097
epoch 26,	batch  1460,	training loss: 0.110
epoch 26,	batch  1470,	training loss: 0.145
epoch 26,	batch  1480,	training loss: 0.099
epoch 26,	batch  1490,	training loss: 0.088
epoch 26,	batch  1500,	training loss: 0.085
epoch 26,	batch  1510,	training loss: 0.098
epoch 26,	batch  1520,	training loss: 0.126
epoch 26,	batch  1530,	training loss: 0.110
epoch 26,	batch  1540,	training loss: 0.111
epoch 26,	batch  1550,	training loss: 0.098
epoch 26,	batch  1560,	training loss: 0.098
epoch 26,	batch  1570,	training loss: 0.054
epoch 26,	batch  1580,	training loss: 0.086
epoch 26,	batch  1590,	training loss: 0.073
epoch 26,	batch  1600,	training loss: 0.085
epoch 26,	batch  1610,	training loss: 0.103
epoch 26,	batch  1620,	training loss: 0.088
epoch 26,	batch  1630,	training loss: 0.074
epoch 26,	batch  1640,	training loss: 0.099
epoch 26,	batch  1650,	training loss: 0.103
epoch 26,	batch  1660,	training loss: 0.085
epoch 26,	batch  1670,	training loss: 0.088
epoch 26,	batch  1680,	training loss: 0.122
epoch 26,	batch  1690,	training loss: 0.100
epoch 26,	batch  1700,	training loss: 0.089
epoch 26,	batch  1710,	training loss: 0.121
epoch 26,	batch  1720,	training loss: 0.077
epoch 26,	batch  1730,	training loss: 0.086
epoch 26,	batch  1740,	training loss: 0.074
epoch 26,	batch  1750,	training loss: 0.167
epoch 26,	batch  1760,	training loss: 0.119
epoch 26,	batch  1770,	training loss: 0.115
epoch 26,	batch  1780,	training loss: 0.114
epoch 26,	batch  1790,	training loss: 0.108
epoch 26,	batch  1800,	training loss: 0.106
epoch 26,	batch  1810,	training loss: 0.108
epoch 26,	batch  1820,	training loss: 0.152
epoch 26,	batch  1830,	training loss: 0.151
epoch 26,	batch  1840,	training loss: 0.107
epoch 26,	batch  1850,	training loss: 0.133
epoch 26,	batch  1860,	training loss: 0.140
epoch 26,	batch  1870,	training loss: 0.092
epoch 26,	batch  1880,	training loss: 0.117
epoch 26,	batch  1890,	training loss: 0.121
epoch 26,	batch  1900,	training loss: 0.095
epoch 26,	batch  1910,	training loss: 0.083
epoch 26,	batch  1920,	training loss: 0.100
epoch 26,	batch  1930,	training loss: 0.130
epoch 26,	batch  1940,	training loss: 0.144
epoch 26,	batch  1950,	training loss: 0.132
epoch 26,	batch  1960,	training loss: 0.139
epoch 26,	batch  1970,	training loss: 0.138
epoch 26,	batch  1980,	training loss: 0.138
END OF EPOCH 26
Testing on validation set...
# correct:	6609/54904 = 12.03737432609646%
# off by 1:	11221/54904 = 20.437490893195395%

epoch 27,	batch    10,	training loss: 0.085
epoch 27,	batch    20,	training loss: 0.070
epoch 27,	batch    30,	training loss: 0.080
epoch 27,	batch    40,	training loss: 0.064
epoch 27,	batch    50,	training loss: 0.073
epoch 27,	batch    60,	training loss: 0.077
epoch 27,	batch    70,	training loss: 0.083
epoch 27,	batch    80,	training loss: 0.080
epoch 27,	batch    90,	training loss: 0.077
epoch 27,	batch   100,	training loss: 0.083
epoch 27,	batch   110,	training loss: 0.071
epoch 27,	batch   120,	training loss: 0.077
epoch 27,	batch   130,	training loss: 0.063
epoch 27,	batch   140,	training loss: 0.067
epoch 27,	batch   150,	training loss: 0.052
epoch 27,	batch   160,	training loss: 0.090
epoch 27,	batch   170,	training loss: 0.075
epoch 27,	batch   180,	training loss: 0.052
epoch 27,	batch   190,	training loss: 0.049
epoch 27,	batch   200,	training loss: 0.069
epoch 27,	batch   210,	training loss: 0.080
epoch 27,	batch   220,	training loss: 0.085
epoch 27,	batch   230,	training loss: 0.035
epoch 27,	batch   240,	training loss: 0.063
epoch 27,	batch   250,	training loss: 0.043
epoch 27,	batch   260,	training loss: 0.055
epoch 27,	batch   270,	training loss: 0.059
epoch 27,	batch   280,	training loss: 0.082
epoch 27,	batch   290,	training loss: 0.057
epoch 27,	batch   300,	training loss: 0.077
epoch 27,	batch   310,	training loss: 0.057
epoch 27,	batch   320,	training loss: 0.075
epoch 27,	batch   330,	training loss: 0.130
epoch 27,	batch   340,	training loss: 0.089
epoch 27,	batch   350,	training loss: 0.084
epoch 27,	batch   360,	training loss: 0.063
epoch 27,	batch   370,	training loss: 0.094
epoch 27,	batch   380,	training loss: 0.066
epoch 27,	batch   390,	training loss: 0.075
epoch 27,	batch   400,	training loss: 0.082
epoch 27,	batch   410,	training loss: 0.040
epoch 27,	batch   420,	training loss: 0.063
epoch 27,	batch   430,	training loss: 0.106
epoch 27,	batch   440,	training loss: 0.077
epoch 27,	batch   450,	training loss: 0.062
epoch 27,	batch   460,	training loss: 0.093
epoch 27,	batch   470,	training loss: 0.057
epoch 27,	batch   480,	training loss: 0.061
epoch 27,	batch   490,	training loss: 0.087
epoch 27,	batch   500,	training loss: 0.090
epoch 27,	batch   510,	training loss: 0.093
epoch 27,	batch   520,	training loss: 0.060
epoch 27,	batch   530,	training loss: 0.085
epoch 27,	batch   540,	training loss: 0.067
epoch 27,	batch   550,	training loss: 0.104
epoch 27,	batch   560,	training loss: 0.081
epoch 27,	batch   570,	training loss: 0.065
epoch 27,	batch   580,	training loss: 0.081
epoch 27,	batch   590,	training loss: 0.097
epoch 27,	batch   600,	training loss: 0.067
epoch 27,	batch   610,	training loss: 0.072
epoch 27,	batch   620,	training loss: 0.084
epoch 27,	batch   630,	training loss: 0.067
epoch 27,	batch   640,	training loss: 0.073
epoch 27,	batch   650,	training loss: 0.087
epoch 27,	batch   660,	training loss: 0.064
epoch 27,	batch   670,	training loss: 0.084
epoch 27,	batch   680,	training loss: 0.069
epoch 27,	batch   690,	training loss: 0.050
epoch 27,	batch   700,	training loss: 0.117
epoch 27,	batch   710,	training loss: 0.085
epoch 27,	batch   720,	training loss: 0.108
epoch 27,	batch   730,	training loss: 0.067
epoch 27,	batch   740,	training loss: 0.138
epoch 27,	batch   750,	training loss: 0.076
epoch 27,	batch   760,	training loss: 0.073
epoch 27,	batch   770,	training loss: 0.065
epoch 27,	batch   780,	training loss: 0.061
epoch 27,	batch   790,	training loss: 0.102
epoch 27,	batch   800,	training loss: 0.097
epoch 27,	batch   810,	training loss: 0.104
epoch 27,	batch   820,	training loss: 0.067
epoch 27,	batch   830,	training loss: 0.062
epoch 27,	batch   840,	training loss: 0.106
epoch 27,	batch   850,	training loss: 0.074
epoch 27,	batch   860,	training loss: 0.067
epoch 27,	batch   870,	training loss: 0.088
epoch 27,	batch   880,	training loss: 0.090
epoch 27,	batch   890,	training loss: 0.074
epoch 27,	batch   900,	training loss: 0.060
epoch 27,	batch   910,	training loss: 0.065
epoch 27,	batch   920,	training loss: 0.151
epoch 27,	batch   930,	training loss: 0.062
epoch 27,	batch   940,	training loss: 0.106
epoch 27,	batch   950,	training loss: 0.102
epoch 27,	batch   960,	training loss: 0.076
epoch 27,	batch   970,	training loss: 0.062
epoch 27,	batch   980,	training loss: 0.091
epoch 27,	batch   990,	training loss: 0.081
epoch 27,	batch  1000,	training loss: 0.064
epoch 27,	batch  1010,	training loss: 0.096
epoch 27,	batch  1020,	training loss: 0.112
epoch 27,	batch  1030,	training loss: 0.064
epoch 27,	batch  1040,	training loss: 0.079
epoch 27,	batch  1050,	training loss: 0.167
epoch 27,	batch  1060,	training loss: 0.086
epoch 27,	batch  1070,	training loss: 0.067
epoch 27,	batch  1080,	training loss: 0.077
epoch 27,	batch  1090,	training loss: 0.086
epoch 27,	batch  1100,	training loss: 0.098
epoch 27,	batch  1110,	training loss: 0.059
epoch 27,	batch  1120,	training loss: 0.129
epoch 27,	batch  1130,	training loss: 0.088
epoch 27,	batch  1140,	training loss: 0.123
epoch 27,	batch  1150,	training loss: 0.117
epoch 27,	batch  1160,	training loss: 0.058
epoch 27,	batch  1170,	training loss: 0.075
epoch 27,	batch  1180,	training loss: 0.111
epoch 27,	batch  1190,	training loss: 0.071
epoch 27,	batch  1200,	training loss: 0.133
epoch 27,	batch  1210,	training loss: 0.063
epoch 27,	batch  1220,	training loss: 0.129
epoch 27,	batch  1230,	training loss: 0.106
epoch 27,	batch  1240,	training loss: 0.071
epoch 27,	batch  1250,	training loss: 0.117
epoch 27,	batch  1260,	training loss: 0.103
epoch 27,	batch  1270,	training loss: 0.069
epoch 27,	batch  1280,	training loss: 0.074
epoch 27,	batch  1290,	training loss: 0.097
epoch 27,	batch  1300,	training loss: 0.106
epoch 27,	batch  1310,	training loss: 0.075
epoch 27,	batch  1320,	training loss: 0.080
epoch 27,	batch  1330,	training loss: 0.080
epoch 27,	batch  1340,	training loss: 0.079
epoch 27,	batch  1350,	training loss: 0.072
epoch 27,	batch  1360,	training loss: 0.090
epoch 27,	batch  1370,	training loss: 0.068
epoch 27,	batch  1380,	training loss: 0.115
epoch 27,	batch  1390,	training loss: 0.098
epoch 27,	batch  1400,	training loss: 0.129
epoch 27,	batch  1410,	training loss: 0.071
epoch 27,	batch  1420,	training loss: 0.097
epoch 27,	batch  1430,	training loss: 0.073
epoch 27,	batch  1440,	training loss: 0.064
epoch 27,	batch  1450,	training loss: 0.117
epoch 27,	batch  1460,	training loss: 0.073
epoch 27,	batch  1470,	training loss: 0.098
epoch 27,	batch  1480,	training loss: 0.076
epoch 27,	batch  1490,	training loss: 0.088
epoch 27,	batch  1500,	training loss: 0.151
epoch 27,	batch  1510,	training loss: 0.087
epoch 27,	batch  1520,	training loss: 0.134
epoch 27,	batch  1530,	training loss: 0.119
epoch 27,	batch  1540,	training loss: 0.107
epoch 27,	batch  1550,	training loss: 0.083
epoch 27,	batch  1560,	training loss: 0.095
epoch 27,	batch  1570,	training loss: 0.108
epoch 27,	batch  1580,	training loss: 0.069
epoch 27,	batch  1590,	training loss: 0.091
epoch 27,	batch  1600,	training loss: 0.090
epoch 27,	batch  1610,	training loss: 0.068
epoch 27,	batch  1620,	training loss: 0.098
epoch 27,	batch  1630,	training loss: 0.098
epoch 27,	batch  1640,	training loss: 0.069
epoch 27,	batch  1650,	training loss: 0.089
epoch 27,	batch  1660,	training loss: 0.073
epoch 27,	batch  1670,	training loss: 0.098
epoch 27,	batch  1680,	training loss: 0.130
epoch 27,	batch  1690,	training loss: 0.098
epoch 27,	batch  1700,	training loss: 0.054
epoch 27,	batch  1710,	training loss: 0.093
epoch 27,	batch  1720,	training loss: 0.086
epoch 27,	batch  1730,	training loss: 0.093
epoch 27,	batch  1740,	training loss: 0.070
epoch 27,	batch  1750,	training loss: 0.073
epoch 27,	batch  1760,	training loss: 0.101
epoch 27,	batch  1770,	training loss: 0.104
epoch 27,	batch  1780,	training loss: 0.099
epoch 27,	batch  1790,	training loss: 0.120
epoch 27,	batch  1800,	training loss: 0.089
epoch 27,	batch  1810,	training loss: 0.116
epoch 27,	batch  1820,	training loss: 0.093
epoch 27,	batch  1830,	training loss: 0.065
epoch 27,	batch  1840,	training loss: 0.068
epoch 27,	batch  1850,	training loss: 0.105
epoch 27,	batch  1860,	training loss: 0.064
epoch 27,	batch  1870,	training loss: 0.083
epoch 27,	batch  1880,	training loss: 0.084
epoch 27,	batch  1890,	training loss: 0.063
epoch 27,	batch  1900,	training loss: 0.075
epoch 27,	batch  1910,	training loss: 0.078
epoch 27,	batch  1920,	training loss: 0.089
epoch 27,	batch  1930,	training loss: 0.100
epoch 27,	batch  1940,	training loss: 0.117
epoch 27,	batch  1950,	training loss: 0.095
epoch 27,	batch  1960,	training loss: 0.078
epoch 27,	batch  1970,	training loss: 0.083
epoch 27,	batch  1980,	training loss: 0.089
END OF EPOCH 27
Testing on validation set...
# correct:	5958/54904 = 10.851668366603526%
# off by 1:	11133/54904 = 20.277211132157948%

epoch 28,	batch    10,	training loss: 0.082
epoch 28,	batch    20,	training loss: 0.095
epoch 28,	batch    30,	training loss: 0.062
epoch 28,	batch    40,	training loss: 0.079
epoch 28,	batch    50,	training loss: 0.081
epoch 28,	batch    60,	training loss: 0.087
epoch 28,	batch    70,	training loss: 0.065
epoch 28,	batch    80,	training loss: 0.072
epoch 28,	batch    90,	training loss: 0.054
epoch 28,	batch   100,	training loss: 0.060
epoch 28,	batch   110,	training loss: 0.080
epoch 28,	batch   120,	training loss: 0.061
epoch 28,	batch   130,	training loss: 0.063
epoch 28,	batch   140,	training loss: 0.070
epoch 28,	batch   150,	training loss: 0.083
epoch 28,	batch   160,	training loss: 0.059
epoch 28,	batch   170,	training loss: 0.081
epoch 28,	batch   180,	training loss: 0.061
epoch 28,	batch   190,	training loss: 0.048
epoch 28,	batch   200,	training loss: 0.054
epoch 28,	batch   210,	training loss: 0.039
epoch 28,	batch   220,	training loss: 0.061
epoch 28,	batch   230,	training loss: 0.062
epoch 28,	batch   240,	training loss: 0.070
epoch 28,	batch   250,	training loss: 0.082
epoch 28,	batch   260,	training loss: 0.068
epoch 28,	batch   270,	training loss: 0.057
epoch 28,	batch   280,	training loss: 0.065
epoch 28,	batch   290,	training loss: 0.082
epoch 28,	batch   300,	training loss: 0.054
epoch 28,	batch   310,	training loss: 0.100
epoch 28,	batch   320,	training loss: 0.075
epoch 28,	batch   330,	training loss: 0.055
epoch 28,	batch   340,	training loss: 0.050
epoch 28,	batch   350,	training loss: 0.079
epoch 28,	batch   360,	training loss: 0.069
epoch 28,	batch   370,	training loss: 0.063
epoch 28,	batch   380,	training loss: 0.074
epoch 28,	batch   390,	training loss: 0.047
epoch 28,	batch   400,	training loss: 0.040
epoch 28,	batch   410,	training loss: 0.057
epoch 28,	batch   420,	training loss: 0.057
epoch 28,	batch   430,	training loss: 0.060
epoch 28,	batch   440,	training loss: 0.055
epoch 28,	batch   450,	training loss: 0.081
epoch 28,	batch   460,	training loss: 0.061
epoch 28,	batch   470,	training loss: 0.039
epoch 28,	batch   480,	training loss: 0.071
epoch 28,	batch   490,	training loss: 0.055
epoch 28,	batch   500,	training loss: 0.052
epoch 28,	batch   510,	training loss: 0.093
epoch 28,	batch   520,	training loss: 0.065
epoch 28,	batch   530,	training loss: 0.044
epoch 28,	batch   540,	training loss: 0.044
epoch 28,	batch   550,	training loss: 0.050
epoch 28,	batch   560,	training loss: 0.057
epoch 28,	batch   570,	training loss: 0.076
epoch 28,	batch   580,	training loss: 0.128
epoch 28,	batch   590,	training loss: 0.050
epoch 28,	batch   600,	training loss: 0.075
epoch 28,	batch   610,	training loss: 0.099
epoch 28,	batch   620,	training loss: 0.055
epoch 28,	batch   630,	training loss: 0.059
epoch 28,	batch   640,	training loss: 0.074
epoch 28,	batch   650,	training loss: 0.059
epoch 28,	batch   660,	training loss: 0.075
epoch 28,	batch   670,	training loss: 0.067
epoch 28,	batch   680,	training loss: 0.073
epoch 28,	batch   690,	training loss: 0.056
epoch 28,	batch   700,	training loss: 0.049
epoch 28,	batch   710,	training loss: 0.068
epoch 28,	batch   720,	training loss: 0.038
epoch 28,	batch   730,	training loss: 0.047
epoch 28,	batch   740,	training loss: 0.071
epoch 28,	batch   750,	training loss: 0.081
epoch 28,	batch   760,	training loss: 0.059
epoch 28,	batch   770,	training loss: 0.058
epoch 28,	batch   780,	training loss: 0.102
epoch 28,	batch   790,	training loss: 0.056
epoch 28,	batch   800,	training loss: 0.067
epoch 28,	batch   810,	training loss: 0.053
epoch 28,	batch   820,	training loss: 0.072
epoch 28,	batch   830,	training loss: 0.087
epoch 28,	batch   840,	training loss: 0.064
epoch 28,	batch   850,	training loss: 0.078
epoch 28,	batch   860,	training loss: 0.096
epoch 28,	batch   870,	training loss: 0.063
epoch 28,	batch   880,	training loss: 0.162
epoch 28,	batch   890,	training loss: 0.133
epoch 28,	batch   900,	training loss: 0.087
epoch 28,	batch   910,	training loss: 0.069
epoch 28,	batch   920,	training loss: 0.106
epoch 28,	batch   930,	training loss: 0.074
epoch 28,	batch   940,	training loss: 0.078
epoch 28,	batch   950,	training loss: 0.055
epoch 28,	batch   960,	training loss: 0.057
epoch 28,	batch   970,	training loss: 0.071
epoch 28,	batch   980,	training loss: 0.054
epoch 28,	batch   990,	training loss: 0.088
epoch 28,	batch  1000,	training loss: 0.072
epoch 28,	batch  1010,	training loss: 0.078
epoch 28,	batch  1020,	training loss: 0.096
epoch 28,	batch  1030,	training loss: 0.107
epoch 28,	batch  1040,	training loss: 0.077
epoch 28,	batch  1050,	training loss: 0.069
epoch 28,	batch  1060,	training loss: 0.077
epoch 28,	batch  1070,	training loss: 0.097
epoch 28,	batch  1080,	training loss: 0.078
epoch 28,	batch  1090,	training loss: 0.054
epoch 28,	batch  1100,	training loss: 0.051
epoch 28,	batch  1110,	training loss: 0.105
epoch 28,	batch  1120,	training loss: 0.071
epoch 28,	batch  1130,	training loss: 0.083
epoch 28,	batch  1140,	training loss: 0.096
epoch 28,	batch  1150,	training loss: 0.087
epoch 28,	batch  1160,	training loss: 0.061
epoch 28,	batch  1170,	training loss: 0.069
epoch 28,	batch  1180,	training loss: 0.128
epoch 28,	batch  1190,	training loss: 0.069
epoch 28,	batch  1200,	training loss: 0.089
epoch 28,	batch  1210,	training loss: 0.044
epoch 28,	batch  1220,	training loss: 0.088
epoch 28,	batch  1230,	training loss: 0.094
epoch 28,	batch  1240,	training loss: 0.057
epoch 28,	batch  1250,	training loss: 0.083
epoch 28,	batch  1260,	training loss: 0.072
epoch 28,	batch  1270,	training loss: 0.080
epoch 28,	batch  1280,	training loss: 0.143
epoch 28,	batch  1290,	training loss: 0.108
epoch 28,	batch  1300,	training loss: 0.102
epoch 28,	batch  1310,	training loss: 0.060
epoch 28,	batch  1320,	training loss: 0.080
epoch 28,	batch  1330,	training loss: 0.125
epoch 28,	batch  1340,	training loss: 0.100
epoch 28,	batch  1350,	training loss: 0.081
epoch 28,	batch  1360,	training loss: 0.069
epoch 28,	batch  1370,	training loss: 0.104
epoch 28,	batch  1380,	training loss: 0.102
epoch 28,	batch  1390,	training loss: 0.081
epoch 28,	batch  1400,	training loss: 0.079
epoch 28,	batch  1410,	training loss: 0.083
epoch 28,	batch  1420,	training loss: 0.129
epoch 28,	batch  1430,	training loss: 0.099
epoch 28,	batch  1440,	training loss: 0.119
epoch 28,	batch  1450,	training loss: 0.066
epoch 28,	batch  1460,	training loss: 0.093
epoch 28,	batch  1470,	training loss: 0.053
epoch 28,	batch  1480,	training loss: 0.095
epoch 28,	batch  1490,	training loss: 0.094
epoch 28,	batch  1500,	training loss: 0.113
epoch 28,	batch  1510,	training loss: 0.115
epoch 28,	batch  1520,	training loss: 0.075
epoch 28,	batch  1530,	training loss: 0.086
epoch 28,	batch  1540,	training loss: 0.041
epoch 28,	batch  1550,	training loss: 0.072
epoch 28,	batch  1560,	training loss: 0.046
epoch 28,	batch  1570,	training loss: 0.072
epoch 28,	batch  1580,	training loss: 0.064
epoch 28,	batch  1590,	training loss: 0.067
epoch 28,	batch  1600,	training loss: 0.080
epoch 28,	batch  1610,	training loss: 0.051
epoch 28,	batch  1620,	training loss: 0.154
epoch 28,	batch  1630,	training loss: 0.092
epoch 28,	batch  1640,	training loss: 0.074
epoch 28,	batch  1650,	training loss: 0.070
epoch 28,	batch  1660,	training loss: 0.065
epoch 28,	batch  1670,	training loss: 0.080
epoch 28,	batch  1680,	training loss: 0.113
epoch 28,	batch  1690,	training loss: 0.110
epoch 28,	batch  1700,	training loss: 0.060
epoch 28,	batch  1710,	training loss: 0.096
epoch 28,	batch  1720,	training loss: 0.092
epoch 28,	batch  1730,	training loss: 0.065
epoch 28,	batch  1740,	training loss: 0.069
epoch 28,	batch  1750,	training loss: 0.042
epoch 28,	batch  1760,	training loss: 0.112
epoch 28,	batch  1770,	training loss: 0.106
epoch 28,	batch  1780,	training loss: 0.060
epoch 28,	batch  1790,	training loss: 0.063
epoch 28,	batch  1800,	training loss: 0.057
epoch 28,	batch  1810,	training loss: 0.091
epoch 28,	batch  1820,	training loss: 0.082
epoch 28,	batch  1830,	training loss: 0.080
epoch 28,	batch  1840,	training loss: 0.060
epoch 28,	batch  1850,	training loss: 0.085
epoch 28,	batch  1860,	training loss: 0.053
epoch 28,	batch  1870,	training loss: 0.096
epoch 28,	batch  1880,	training loss: 0.049
epoch 28,	batch  1890,	training loss: 0.076
epoch 28,	batch  1900,	training loss: 0.087
epoch 28,	batch  1910,	training loss: 0.046
epoch 28,	batch  1920,	training loss: 0.062
epoch 28,	batch  1930,	training loss: 0.106
epoch 28,	batch  1940,	training loss: 0.071
epoch 28,	batch  1950,	training loss: 0.093
epoch 28,	batch  1960,	training loss: 0.072
epoch 28,	batch  1970,	training loss: 0.071
epoch 28,	batch  1980,	training loss: 0.082
END OF EPOCH 28
Testing on validation set...
# correct:	6205/54904 = 11.301544514060906%
# off by 1:	10898/54904 = 19.849191315751128%

epoch 29,	batch    10,	training loss: 0.060
epoch 29,	batch    20,	training loss: 0.057
epoch 29,	batch    30,	training loss: 0.079
epoch 29,	batch    40,	training loss: 0.070
epoch 29,	batch    50,	training loss: 0.031
epoch 29,	batch    60,	training loss: 0.056
epoch 29,	batch    70,	training loss: 0.043
epoch 29,	batch    80,	training loss: 0.050
epoch 29,	batch    90,	training loss: 0.055
epoch 29,	batch   100,	training loss: 0.067
epoch 29,	batch   110,	training loss: 0.120
epoch 29,	batch   120,	training loss: 0.088
epoch 29,	batch   130,	training loss: 0.060
epoch 29,	batch   140,	training loss: 0.085
epoch 29,	batch   150,	training loss: 0.063
epoch 29,	batch   160,	training loss: 0.043
epoch 29,	batch   170,	training loss: 0.060
epoch 29,	batch   180,	training loss: 0.103
epoch 29,	batch   190,	training loss: 0.097
epoch 29,	batch   200,	training loss: 0.103
epoch 29,	batch   210,	training loss: 0.054
epoch 29,	batch   220,	training loss: 0.051
epoch 29,	batch   230,	training loss: 0.047
epoch 29,	batch   240,	training loss: 0.087
epoch 29,	batch   250,	training loss: 0.065
epoch 29,	batch   260,	training loss: 0.073
epoch 29,	batch   270,	training loss: 0.077
epoch 29,	batch   280,	training loss: 0.083
epoch 29,	batch   290,	training loss: 0.068
epoch 29,	batch   300,	training loss: 0.050
epoch 29,	batch   310,	training loss: 0.050
epoch 29,	batch   320,	training loss: 0.028
epoch 29,	batch   330,	training loss: 0.050
epoch 29,	batch   340,	training loss: 0.039
epoch 29,	batch   350,	training loss: 0.083
epoch 29,	batch   360,	training loss: 0.085
epoch 29,	batch   370,	training loss: 0.077
epoch 29,	batch   380,	training loss: 0.053
epoch 29,	batch   390,	training loss: 0.103
epoch 29,	batch   400,	training loss: 0.045
epoch 29,	batch   410,	training loss: 0.096
epoch 29,	batch   420,	training loss: 0.073
epoch 29,	batch   430,	training loss: 0.103
epoch 29,	batch   440,	training loss: 0.058
epoch 29,	batch   450,	training loss: 0.086
epoch 29,	batch   460,	training loss: 0.059
epoch 29,	batch   470,	training loss: 0.063
epoch 29,	batch   480,	training loss: 0.073
epoch 29,	batch   490,	training loss: 0.044
epoch 29,	batch   500,	training loss: 0.056
epoch 29,	batch   510,	training loss: 0.054
epoch 29,	batch   520,	training loss: 0.040
epoch 29,	batch   530,	training loss: 0.062
epoch 29,	batch   540,	training loss: 0.057
epoch 29,	batch   550,	training loss: 0.071
epoch 29,	batch   560,	training loss: 0.066
epoch 29,	batch   570,	training loss: 0.057
epoch 29,	batch   580,	training loss: 0.051
epoch 29,	batch   590,	training loss: 0.076
epoch 29,	batch   600,	training loss: 0.065
epoch 29,	batch   610,	training loss: 0.080
epoch 29,	batch   620,	training loss: 0.055
epoch 29,	batch   630,	training loss: 0.113
epoch 29,	batch   640,	training loss: 0.073
epoch 29,	batch   650,	training loss: 0.058
epoch 29,	batch   660,	training loss: 0.077
epoch 29,	batch   670,	training loss: 0.064
epoch 29,	batch   680,	training loss: 0.047
epoch 29,	batch   690,	training loss: 0.067
epoch 29,	batch   700,	training loss: 0.059
epoch 29,	batch   710,	training loss: 0.097
epoch 29,	batch   720,	training loss: 0.043
epoch 29,	batch   730,	training loss: 0.081
epoch 29,	batch   740,	training loss: 0.070
epoch 29,	batch   750,	training loss: 0.041
epoch 29,	batch   760,	training loss: 0.090
epoch 29,	batch   770,	training loss: 0.048
epoch 29,	batch   780,	training loss: 0.104
epoch 29,	batch   790,	training loss: 0.053
epoch 29,	batch   800,	training loss: 0.127
epoch 29,	batch   810,	training loss: 0.053
epoch 29,	batch   820,	training loss: 0.128
epoch 29,	batch   830,	training loss: 0.044
epoch 29,	batch   840,	training loss: 0.120
epoch 29,	batch   850,	training loss: 0.074
epoch 29,	batch   860,	training loss: 0.045
epoch 29,	batch   870,	training loss: 0.075
epoch 29,	batch   880,	training loss: 0.055
epoch 29,	batch   890,	training loss: 0.049
epoch 29,	batch   900,	training loss: 0.060
epoch 29,	batch   910,	training loss: 0.051
epoch 29,	batch   920,	training loss: 0.064
epoch 29,	batch   930,	training loss: 0.062
epoch 29,	batch   940,	training loss: 0.067
epoch 29,	batch   950,	training loss: 0.069
epoch 29,	batch   960,	training loss: 0.030
epoch 29,	batch   970,	training loss: 0.053
epoch 29,	batch   980,	training loss: 0.051
epoch 29,	batch   990,	training loss: 0.085
epoch 29,	batch  1000,	training loss: 0.060
epoch 29,	batch  1010,	training loss: 0.043
epoch 29,	batch  1020,	training loss: 0.047
epoch 29,	batch  1030,	training loss: 0.061
epoch 29,	batch  1040,	training loss: 0.046
epoch 29,	batch  1050,	training loss: 0.067
epoch 29,	batch  1060,	training loss: 0.065
epoch 29,	batch  1070,	training loss: 0.096
epoch 29,	batch  1080,	training loss: 0.038
epoch 29,	batch  1090,	training loss: 0.051
epoch 29,	batch  1100,	training loss: 0.030
epoch 29,	batch  1110,	training loss: 0.055
epoch 29,	batch  1120,	training loss: 0.043
epoch 29,	batch  1130,	training loss: 0.066
epoch 29,	batch  1140,	training loss: 0.047
epoch 29,	batch  1150,	training loss: 0.069
epoch 29,	batch  1160,	training loss: 0.046
epoch 29,	batch  1170,	training loss: 0.057
epoch 29,	batch  1180,	training loss: 0.140
epoch 29,	batch  1190,	training loss: 0.071
epoch 29,	batch  1200,	training loss: 0.080
epoch 29,	batch  1210,	training loss: 0.071
epoch 29,	batch  1220,	training loss: 0.050
epoch 29,	batch  1230,	training loss: 0.070
epoch 29,	batch  1240,	training loss: 0.090
epoch 29,	batch  1250,	training loss: 0.080
epoch 29,	batch  1260,	training loss: 0.081
epoch 29,	batch  1270,	training loss: 0.076
epoch 29,	batch  1280,	training loss: 0.089
epoch 29,	batch  1290,	training loss: 0.057
epoch 29,	batch  1300,	training loss: 0.079
epoch 29,	batch  1310,	training loss: 0.087
epoch 29,	batch  1320,	training loss: 0.103
epoch 29,	batch  1330,	training loss: 0.045
epoch 29,	batch  1340,	training loss: 0.097
epoch 29,	batch  1350,	training loss: 0.069
epoch 29,	batch  1360,	training loss: 0.107
epoch 29,	batch  1370,	training loss: 0.067
epoch 29,	batch  1380,	training loss: 0.072
epoch 29,	batch  1390,	training loss: 0.061
epoch 29,	batch  1400,	training loss: 0.055
epoch 29,	batch  1410,	training loss: 0.060
epoch 29,	batch  1420,	training loss: 0.079
epoch 29,	batch  1430,	training loss: 0.084
epoch 29,	batch  1440,	training loss: 0.071
epoch 29,	batch  1450,	training loss: 0.064
epoch 29,	batch  1460,	training loss: 0.083
epoch 29,	batch  1470,	training loss: 0.081
epoch 29,	batch  1480,	training loss: 0.073
epoch 29,	batch  1490,	training loss: 0.068
epoch 29,	batch  1500,	training loss: 0.072
epoch 29,	batch  1510,	training loss: 0.093
epoch 29,	batch  1520,	training loss: 0.056
epoch 29,	batch  1530,	training loss: 0.081
epoch 29,	batch  1540,	training loss: 0.084
epoch 29,	batch  1550,	training loss: 0.043
epoch 29,	batch  1560,	training loss: 0.054
epoch 29,	batch  1570,	training loss: 0.090
epoch 29,	batch  1580,	training loss: 0.119
epoch 29,	batch  1590,	training loss: 0.149
epoch 29,	batch  1600,	training loss: 0.090
epoch 29,	batch  1610,	training loss: 0.112
epoch 29,	batch  1620,	training loss: 0.086
epoch 29,	batch  1630,	training loss: 0.058
epoch 29,	batch  1640,	training loss: 0.066
epoch 29,	batch  1650,	training loss: 0.077
epoch 29,	batch  1660,	training loss: 0.097
epoch 29,	batch  1670,	training loss: 0.074
epoch 29,	batch  1680,	training loss: 0.071
epoch 29,	batch  1690,	training loss: 0.086
epoch 29,	batch  1700,	training loss: 0.064
epoch 29,	batch  1710,	training loss: 0.050
epoch 29,	batch  1720,	training loss: 0.081
epoch 29,	batch  1730,	training loss: 0.060
epoch 29,	batch  1740,	training loss: 0.056
epoch 29,	batch  1750,	training loss: 0.046
epoch 29,	batch  1760,	training loss: 0.049
epoch 29,	batch  1770,	training loss: 0.080
epoch 29,	batch  1780,	training loss: 0.069
epoch 29,	batch  1790,	training loss: 0.094
epoch 29,	batch  1800,	training loss: 0.137
epoch 29,	batch  1810,	training loss: 0.065
epoch 29,	batch  1820,	training loss: 0.123
epoch 29,	batch  1830,	training loss: 0.070
epoch 29,	batch  1840,	training loss: 0.097
epoch 29,	batch  1850,	training loss: 0.123
epoch 29,	batch  1860,	training loss: 0.112
epoch 29,	batch  1870,	training loss: 0.111
epoch 29,	batch  1880,	training loss: 0.106
epoch 29,	batch  1890,	training loss: 0.068
epoch 29,	batch  1900,	training loss: 0.109
epoch 29,	batch  1910,	training loss: 0.076
epoch 29,	batch  1920,	training loss: 0.060
epoch 29,	batch  1930,	training loss: 0.101
epoch 29,	batch  1940,	training loss: 0.094
epoch 29,	batch  1950,	training loss: 0.074
epoch 29,	batch  1960,	training loss: 0.123
epoch 29,	batch  1970,	training loss: 0.091
epoch 29,	batch  1980,	training loss: 0.080
END OF EPOCH 29
Testing on validation set...
# correct:	5539/54904 = 10.088518140754772%
# off by 1:	10284/54904 = 18.73087571033076%

epoch 30,	batch    10,	training loss: 0.058
epoch 30,	batch    20,	training loss: 0.060
epoch 30,	batch    30,	training loss: 0.058
epoch 30,	batch    40,	training loss: 0.059
epoch 30,	batch    50,	training loss: 0.059
epoch 30,	batch    60,	training loss: 0.066
epoch 30,	batch    70,	training loss: 0.059
epoch 30,	batch    80,	training loss: 0.048
epoch 30,	batch    90,	training loss: 0.050
epoch 30,	batch   100,	training loss: 0.044
epoch 30,	batch   110,	training loss: 0.052
epoch 30,	batch   120,	training loss: 0.042
epoch 30,	batch   130,	training loss: 0.038
epoch 30,	batch   140,	training loss: 0.069
epoch 30,	batch   150,	training loss: 0.037
epoch 30,	batch   160,	training loss: 0.044
epoch 30,	batch   170,	training loss: 0.061
epoch 30,	batch   180,	training loss: 0.040
epoch 30,	batch   190,	training loss: 0.065
epoch 30,	batch   200,	training loss: 0.027
epoch 30,	batch   210,	training loss: 0.053
epoch 30,	batch   220,	training loss: 0.054
epoch 30,	batch   230,	training loss: 0.070
epoch 30,	batch   240,	training loss: 0.054
epoch 30,	batch   250,	training loss: 0.077
epoch 30,	batch   260,	training loss: 0.051
epoch 30,	batch   270,	training loss: 0.040
epoch 30,	batch   280,	training loss: 0.050
epoch 30,	batch   290,	training loss: 0.043
epoch 30,	batch   300,	training loss: 0.039
epoch 30,	batch   310,	training loss: 0.036
epoch 30,	batch   320,	training loss: 0.036
epoch 30,	batch   330,	training loss: 0.057
epoch 30,	batch   340,	training loss: 0.036
epoch 30,	batch   350,	training loss: 0.060
epoch 30,	batch   360,	training loss: 0.066
epoch 30,	batch   370,	training loss: 0.061
epoch 30,	batch   380,	training loss: 0.056
epoch 30,	batch   390,	training loss: 0.062
epoch 30,	batch   400,	training loss: 0.047
epoch 30,	batch   410,	training loss: 0.062
epoch 30,	batch   420,	training loss: 0.050
epoch 30,	batch   430,	training loss: 0.029
epoch 30,	batch   440,	training loss: 0.030
epoch 30,	batch   450,	training loss: 0.030
epoch 30,	batch   460,	training loss: 0.037
epoch 30,	batch   470,	training loss: 0.047
epoch 30,	batch   480,	training loss: 0.051
epoch 30,	batch   490,	training loss: 0.054
epoch 30,	batch   500,	training loss: 0.064
epoch 30,	batch   510,	training loss: 0.065
epoch 30,	batch   520,	training loss: 0.063
epoch 30,	batch   530,	training loss: 0.047
epoch 30,	batch   540,	training loss: 0.057
epoch 30,	batch   550,	training loss: 0.050
epoch 30,	batch   560,	training loss: 0.061
epoch 30,	batch   570,	training loss: 0.061
epoch 30,	batch   580,	training loss: 0.065
epoch 30,	batch   590,	training loss: 0.070
epoch 30,	batch   600,	training loss: 0.083
epoch 30,	batch   610,	training loss: 0.054
epoch 30,	batch   620,	training loss: 0.067
epoch 30,	batch   630,	training loss: 0.041
epoch 30,	batch   640,	training loss: 0.050
epoch 30,	batch   650,	training loss: 0.052
epoch 30,	batch   660,	training loss: 0.086
epoch 30,	batch   670,	training loss: 0.062
epoch 30,	batch   680,	training loss: 0.046
epoch 30,	batch   690,	training loss: 0.071
epoch 30,	batch   700,	training loss: 0.081
epoch 30,	batch   710,	training loss: 0.087
epoch 30,	batch   720,	training loss: 0.088
epoch 30,	batch   730,	training loss: 0.041
epoch 30,	batch   740,	training loss: 0.055
epoch 30,	batch   750,	training loss: 0.072
epoch 30,	batch   760,	training loss: 0.044
epoch 30,	batch   770,	training loss: 0.076
epoch 30,	batch   780,	training loss: 0.085
epoch 30,	batch   790,	training loss: 0.039
epoch 30,	batch   800,	training loss: 0.045
epoch 30,	batch   810,	training loss: 0.043
epoch 30,	batch   820,	training loss: 0.026
epoch 30,	batch   830,	training loss: 0.039
epoch 30,	batch   840,	training loss: 0.049
epoch 30,	batch   850,	training loss: 0.060
epoch 30,	batch   860,	training loss: 0.085
epoch 30,	batch   870,	training loss: 0.058
epoch 30,	batch   880,	training loss: 0.089
epoch 30,	batch   890,	training loss: 0.076
epoch 30,	batch   900,	training loss: 0.059
epoch 30,	batch   910,	training loss: 0.054
epoch 30,	batch   920,	training loss: 0.056
epoch 30,	batch   930,	training loss: 0.065
epoch 30,	batch   940,	training loss: 0.050
epoch 30,	batch   950,	training loss: 0.079
epoch 30,	batch   960,	training loss: 0.049
epoch 30,	batch   970,	training loss: 0.079
epoch 30,	batch   980,	training loss: 0.063
epoch 30,	batch   990,	training loss: 0.070
epoch 30,	batch  1000,	training loss: 0.057
epoch 30,	batch  1010,	training loss: 0.080
epoch 30,	batch  1020,	training loss: 0.055
epoch 30,	batch  1030,	training loss: 0.049
epoch 30,	batch  1040,	training loss: 0.099
epoch 30,	batch  1050,	training loss: 0.039
epoch 30,	batch  1060,	training loss: 0.057
epoch 30,	batch  1070,	training loss: 0.077
epoch 30,	batch  1080,	training loss: 0.098
epoch 30,	batch  1090,	training loss: 0.048
epoch 30,	batch  1100,	training loss: 0.059
epoch 30,	batch  1110,	training loss: 0.071
epoch 30,	batch  1120,	training loss: 0.076
epoch 30,	batch  1130,	training loss: 0.082
epoch 30,	batch  1140,	training loss: 0.049
epoch 30,	batch  1150,	training loss: 0.064
epoch 30,	batch  1160,	training loss: 0.038
epoch 30,	batch  1170,	training loss: 0.068
epoch 30,	batch  1180,	training loss: 0.084
epoch 30,	batch  1190,	training loss: 0.080
epoch 30,	batch  1200,	training loss: 0.054
epoch 30,	batch  1210,	training loss: 0.069
epoch 30,	batch  1220,	training loss: 0.055
epoch 30,	batch  1230,	training loss: 0.091
epoch 30,	batch  1240,	training loss: 0.056
epoch 30,	batch  1250,	training loss: 0.041
epoch 30,	batch  1260,	training loss: 0.079
epoch 30,	batch  1270,	training loss: 0.046
epoch 30,	batch  1280,	training loss: 0.082
epoch 30,	batch  1290,	training loss: 0.066
epoch 30,	batch  1300,	training loss: 0.066
epoch 30,	batch  1310,	training loss: 0.114
epoch 30,	batch  1320,	training loss: 0.093
epoch 30,	batch  1330,	training loss: 0.040
epoch 30,	batch  1340,	training loss: 0.096
epoch 30,	batch  1350,	training loss: 0.073
epoch 30,	batch  1360,	training loss: 0.070
epoch 30,	batch  1370,	training loss: 0.066
epoch 30,	batch  1380,	training loss: 0.072
epoch 30,	batch  1390,	training loss: 0.044
epoch 30,	batch  1400,	training loss: 0.071
epoch 30,	batch  1410,	training loss: 0.077
epoch 30,	batch  1420,	training loss: 0.048
epoch 30,	batch  1430,	training loss: 0.116
epoch 30,	batch  1440,	training loss: 0.090
epoch 30,	batch  1450,	training loss: 0.038
epoch 30,	batch  1460,	training loss: 0.050
epoch 30,	batch  1470,	training loss: 0.060
epoch 30,	batch  1480,	training loss: 0.059
epoch 30,	batch  1490,	training loss: 0.109
epoch 30,	batch  1500,	training loss: 0.087
epoch 30,	batch  1510,	training loss: 0.132
epoch 30,	batch  1520,	training loss: 0.085
epoch 30,	batch  1530,	training loss: 0.080
epoch 30,	batch  1540,	training loss: 0.108
epoch 30,	batch  1550,	training loss: 0.073
epoch 30,	batch  1560,	training loss: 0.103
epoch 30,	batch  1570,	training loss: 0.101
epoch 30,	batch  1580,	training loss: 0.111
epoch 30,	batch  1590,	training loss: 0.060
epoch 30,	batch  1600,	training loss: 0.074
epoch 30,	batch  1610,	training loss: 0.046
epoch 30,	batch  1620,	training loss: 0.069
epoch 30,	batch  1630,	training loss: 0.059
epoch 30,	batch  1640,	training loss: 0.083
epoch 30,	batch  1650,	training loss: 0.067
epoch 30,	batch  1660,	training loss: 0.067
epoch 30,	batch  1670,	training loss: 0.070
epoch 30,	batch  1680,	training loss: 0.070
epoch 30,	batch  1690,	training loss: 0.073
epoch 30,	batch  1700,	training loss: 0.074
epoch 30,	batch  1710,	training loss: 0.074
epoch 30,	batch  1720,	training loss: 0.063
epoch 30,	batch  1730,	training loss: 0.078
epoch 30,	batch  1740,	training loss: 0.106
epoch 30,	batch  1750,	training loss: 0.075
epoch 30,	batch  1760,	training loss: 0.071
epoch 30,	batch  1770,	training loss: 0.113
epoch 30,	batch  1780,	training loss: 0.094
epoch 30,	batch  1790,	training loss: 0.088
epoch 30,	batch  1800,	training loss: 0.086
epoch 30,	batch  1810,	training loss: 0.084
epoch 30,	batch  1820,	training loss: 0.069
epoch 30,	batch  1830,	training loss: 0.110
epoch 30,	batch  1840,	training loss: 0.061
epoch 30,	batch  1850,	training loss: 0.096
epoch 30,	batch  1860,	training loss: 0.067
epoch 30,	batch  1870,	training loss: 0.076
epoch 30,	batch  1880,	training loss: 0.084
epoch 30,	batch  1890,	training loss: 0.052
epoch 30,	batch  1900,	training loss: 0.125
epoch 30,	batch  1910,	training loss: 0.040
epoch 30,	batch  1920,	training loss: 0.090
epoch 30,	batch  1930,	training loss: 0.061
epoch 30,	batch  1940,	training loss: 0.055
epoch 30,	batch  1950,	training loss: 0.086
epoch 30,	batch  1960,	training loss: 0.086
epoch 30,	batch  1970,	training loss: 0.068
epoch 30,	batch  1980,	training loss: 0.115
END OF EPOCH 30
Testing on validation set...
# correct:	6319/54904 = 11.509179659041235%
# off by 1:	11478/54904 = 20.905580649861577%

epoch 31,	batch    10,	training loss: 0.055
epoch 31,	batch    20,	training loss: 0.064
epoch 31,	batch    30,	training loss: 0.082
epoch 31,	batch    40,	training loss: 0.038
epoch 31,	batch    50,	training loss: 0.045
epoch 31,	batch    60,	training loss: 0.041
epoch 31,	batch    70,	training loss: 0.050
epoch 31,	batch    80,	training loss: 0.076
epoch 31,	batch    90,	training loss: 0.074
epoch 31,	batch   100,	training loss: 0.053
epoch 31,	batch   110,	training loss: 0.068
epoch 31,	batch   120,	training loss: 0.056
epoch 31,	batch   130,	training loss: 0.042
epoch 31,	batch   140,	training loss: 0.055
epoch 31,	batch   150,	training loss: 0.030
epoch 31,	batch   160,	training loss: 0.035
epoch 31,	batch   170,	training loss: 0.059
epoch 31,	batch   180,	training loss: 0.070
epoch 31,	batch   190,	training loss: 0.064
epoch 31,	batch   200,	training loss: 0.037
epoch 31,	batch   210,	training loss: 0.063
epoch 31,	batch   220,	training loss: 0.046
epoch 31,	batch   230,	training loss: 0.037
epoch 31,	batch   240,	training loss: 0.024
epoch 31,	batch   250,	training loss: 0.045
epoch 31,	batch   260,	training loss: 0.093
epoch 31,	batch   270,	training loss: 0.043
epoch 31,	batch   280,	training loss: 0.049
epoch 31,	batch   290,	training loss: 0.038
epoch 31,	batch   300,	training loss: 0.042
epoch 31,	batch   310,	training loss: 0.107
epoch 31,	batch   320,	training loss: 0.044
epoch 31,	batch   330,	training loss: 0.045
epoch 31,	batch   340,	training loss: 0.039
epoch 31,	batch   350,	training loss: 0.042
epoch 31,	batch   360,	training loss: 0.031
epoch 31,	batch   370,	training loss: 0.065
epoch 31,	batch   380,	training loss: 0.051
epoch 31,	batch   390,	training loss: 0.033
epoch 31,	batch   400,	training loss: 0.034
epoch 31,	batch   410,	training loss: 0.030
epoch 31,	batch   420,	training loss: 0.037
epoch 31,	batch   430,	training loss: 0.068
epoch 31,	batch   440,	training loss: 0.065
epoch 31,	batch   450,	training loss: 0.054
epoch 31,	batch   460,	training loss: 0.037
epoch 31,	batch   470,	training loss: 0.064
epoch 31,	batch   480,	training loss: 0.044
epoch 31,	batch   490,	training loss: 0.057
epoch 31,	batch   500,	training loss: 0.054
epoch 31,	batch   510,	training loss: 0.053
epoch 31,	batch   520,	training loss: 0.070
epoch 31,	batch   530,	training loss: 0.055
epoch 31,	batch   540,	training loss: 0.058
epoch 31,	batch   550,	training loss: 0.062
epoch 31,	batch   560,	training loss: 0.097
epoch 31,	batch   570,	training loss: 0.068
epoch 31,	batch   580,	training loss: 0.062
epoch 31,	batch   590,	training loss: 0.045
epoch 31,	batch   600,	training loss: 0.044
epoch 31,	batch   610,	training loss: 0.049
epoch 31,	batch   620,	training loss: 0.087
epoch 31,	batch   630,	training loss: 0.045
epoch 31,	batch   640,	training loss: 0.034
epoch 31,	batch   650,	training loss: 0.054
epoch 31,	batch   660,	training loss: 0.074
epoch 31,	batch   670,	training loss: 0.041
epoch 31,	batch   680,	training loss: 0.034
epoch 31,	batch   690,	training loss: 0.029
epoch 31,	batch   700,	training loss: 0.035
epoch 31,	batch   710,	training loss: 0.062
epoch 31,	batch   720,	training loss: 0.054
epoch 31,	batch   730,	training loss: 0.058
epoch 31,	batch   740,	training loss: 0.078
epoch 31,	batch   750,	training loss: 0.091
epoch 31,	batch   760,	training loss: 0.077
epoch 31,	batch   770,	training loss: 0.059
epoch 31,	batch   780,	training loss: 0.051
epoch 31,	batch   790,	training loss: 0.063
epoch 31,	batch   800,	training loss: 0.114
epoch 31,	batch   810,	training loss: 0.046
epoch 31,	batch   820,	training loss: 0.075
epoch 31,	batch   830,	training loss: 0.058
epoch 31,	batch   840,	training loss: 0.060
epoch 31,	batch   850,	training loss: 0.069
epoch 31,	batch   860,	training loss: 0.047
epoch 31,	batch   870,	training loss: 0.072
epoch 31,	batch   880,	training loss: 0.048
epoch 31,	batch   890,	training loss: 0.047
epoch 31,	batch   900,	training loss: 0.064
epoch 31,	batch   910,	training loss: 0.076
epoch 31,	batch   920,	training loss: 0.048
epoch 31,	batch   930,	training loss: 0.055
epoch 31,	batch   940,	training loss: 0.066
epoch 31,	batch   950,	training loss: 0.059
epoch 31,	batch   960,	training loss: 0.051
epoch 31,	batch   970,	training loss: 0.067
epoch 31,	batch   980,	training loss: 0.109
epoch 31,	batch   990,	training loss: 0.046
epoch 31,	batch  1000,	training loss: 0.043
epoch 31,	batch  1010,	training loss: 0.055
epoch 31,	batch  1020,	training loss: 0.033
epoch 31,	batch  1030,	training loss: 0.051
epoch 31,	batch  1040,	training loss: 0.039
epoch 31,	batch  1050,	training loss: 0.055
epoch 31,	batch  1060,	training loss: 0.053
epoch 31,	batch  1070,	training loss: 0.067
epoch 31,	batch  1080,	training loss: 0.058
epoch 31,	batch  1090,	training loss: 0.036
epoch 31,	batch  1100,	training loss: 0.055
epoch 31,	batch  1110,	training loss: 0.046
epoch 31,	batch  1120,	training loss: 0.039
epoch 31,	batch  1130,	training loss: 0.059
epoch 31,	batch  1140,	training loss: 0.075
epoch 31,	batch  1150,	training loss: 0.099
epoch 31,	batch  1160,	training loss: 0.078
epoch 31,	batch  1170,	training loss: 0.050
epoch 31,	batch  1180,	training loss: 0.096
epoch 31,	batch  1190,	training loss: 0.079
epoch 31,	batch  1200,	training loss: 0.067
epoch 31,	batch  1210,	training loss: 0.060
epoch 31,	batch  1220,	training loss: 0.042
epoch 31,	batch  1230,	training loss: 0.078
epoch 31,	batch  1240,	training loss: 0.037
epoch 31,	batch  1250,	training loss: 0.081
epoch 31,	batch  1260,	training loss: 0.072
epoch 31,	batch  1270,	training loss: 0.073
epoch 31,	batch  1280,	training loss: 0.077
epoch 31,	batch  1290,	training loss: 0.048
epoch 31,	batch  1300,	training loss: 0.041
epoch 31,	batch  1310,	training loss: 0.073
epoch 31,	batch  1320,	training loss: 0.088
epoch 31,	batch  1330,	training loss: 0.056
epoch 31,	batch  1340,	training loss: 0.028
epoch 31,	batch  1350,	training loss: 0.109
epoch 31,	batch  1360,	training loss: 0.077
epoch 31,	batch  1370,	training loss: 0.075
epoch 31,	batch  1380,	training loss: 0.030
epoch 31,	batch  1390,	training loss: 0.049
epoch 31,	batch  1400,	training loss: 0.089
epoch 31,	batch  1410,	training loss: 0.045
epoch 31,	batch  1420,	training loss: 0.041
epoch 31,	batch  1430,	training loss: 0.036
epoch 31,	batch  1440,	training loss: 0.062
epoch 31,	batch  1450,	training loss: 0.058
epoch 31,	batch  1460,	training loss: 0.053
epoch 31,	batch  1470,	training loss: 0.064
epoch 31,	batch  1480,	training loss: 0.066
epoch 31,	batch  1490,	training loss: 0.116
epoch 31,	batch  1500,	training loss: 0.049
epoch 31,	batch  1510,	training loss: 0.064
epoch 31,	batch  1520,	training loss: 0.068
epoch 31,	batch  1530,	training loss: 0.050
epoch 31,	batch  1540,	training loss: 0.098
epoch 31,	batch  1550,	training loss: 0.076
epoch 31,	batch  1560,	training loss: 0.091
epoch 31,	batch  1570,	training loss: 0.049
epoch 31,	batch  1580,	training loss: 0.070
epoch 31,	batch  1590,	training loss: 0.054
epoch 31,	batch  1600,	training loss: 0.060
epoch 31,	batch  1610,	training loss: 0.047
epoch 31,	batch  1620,	training loss: 0.080
epoch 31,	batch  1630,	training loss: 0.070
epoch 31,	batch  1640,	training loss: 0.058
epoch 31,	batch  1650,	training loss: 0.078
epoch 31,	batch  1660,	training loss: 0.077
epoch 31,	batch  1670,	training loss: 0.066
epoch 31,	batch  1680,	training loss: 0.054
epoch 31,	batch  1690,	training loss: 0.089
epoch 31,	batch  1700,	training loss: 0.059
epoch 31,	batch  1710,	training loss: 0.087
epoch 31,	batch  1720,	training loss: 0.059
epoch 31,	batch  1730,	training loss: 0.045
epoch 31,	batch  1740,	training loss: 0.058
epoch 31,	batch  1750,	training loss: 0.054
epoch 31,	batch  1760,	training loss: 0.053
epoch 31,	batch  1770,	training loss: 0.061
epoch 31,	batch  1780,	training loss: 0.055
epoch 31,	batch  1790,	training loss: 0.072
epoch 31,	batch  1800,	training loss: 0.088
epoch 31,	batch  1810,	training loss: 0.049
epoch 31,	batch  1820,	training loss: 0.063
epoch 31,	batch  1830,	training loss: 0.049
epoch 31,	batch  1840,	training loss: 0.083
epoch 31,	batch  1850,	training loss: 0.087
epoch 31,	batch  1860,	training loss: 0.052
epoch 31,	batch  1870,	training loss: 0.057
epoch 31,	batch  1880,	training loss: 0.075
epoch 31,	batch  1890,	training loss: 0.068
epoch 31,	batch  1900,	training loss: 0.041
epoch 31,	batch  1910,	training loss: 0.046
epoch 31,	batch  1920,	training loss: 0.051
epoch 31,	batch  1930,	training loss: 0.096
epoch 31,	batch  1940,	training loss: 0.087
epoch 31,	batch  1950,	training loss: 0.053
epoch 31,	batch  1960,	training loss: 0.057
epoch 31,	batch  1970,	training loss: 0.057
epoch 31,	batch  1980,	training loss: 0.087
END OF EPOCH 31
Testing on validation set...
# correct:	6089/54904 = 11.090266647238817%
# off by 1:	10956/54904 = 19.954830249162175%

epoch 32,	batch    10,	training loss: 0.061
epoch 32,	batch    20,	training loss: 0.039
epoch 32,	batch    30,	training loss: 0.068
epoch 32,	batch    40,	training loss: 0.054
epoch 32,	batch    50,	training loss: 0.034
epoch 32,	batch    60,	training loss: 0.037
epoch 32,	batch    70,	training loss: 0.047
epoch 32,	batch    80,	training loss: 0.038
epoch 32,	batch    90,	training loss: 0.071
epoch 32,	batch   100,	training loss: 0.076
epoch 32,	batch   110,	training loss: 0.034
epoch 32,	batch   120,	training loss: 0.054
epoch 32,	batch   130,	training loss: 0.047
epoch 32,	batch   140,	training loss: 0.044
epoch 32,	batch   150,	training loss: 0.094
epoch 32,	batch   160,	training loss: 0.061
epoch 32,	batch   170,	training loss: 0.030
epoch 32,	batch   180,	training loss: 0.042
epoch 32,	batch   190,	training loss: 0.031
epoch 32,	batch   200,	training loss: 0.027
epoch 32,	batch   210,	training loss: 0.047
epoch 32,	batch   220,	training loss: 0.062
epoch 32,	batch   230,	training loss: 0.053
epoch 32,	batch   240,	training loss: 0.049
epoch 32,	batch   250,	training loss: 0.065
epoch 32,	batch   260,	training loss: 0.029
epoch 32,	batch   270,	training loss: 0.039
epoch 32,	batch   280,	training loss: 0.067
epoch 32,	batch   290,	training loss: 0.072
epoch 32,	batch   300,	training loss: 0.057
epoch 32,	batch   310,	training loss: 0.039
epoch 32,	batch   320,	training loss: 0.073
epoch 32,	batch   330,	training loss: 0.066
epoch 32,	batch   340,	training loss: 0.057
epoch 32,	batch   350,	training loss: 0.050
epoch 32,	batch   360,	training loss: 0.067
epoch 32,	batch   370,	training loss: 0.038
epoch 32,	batch   380,	training loss: 0.041
epoch 32,	batch   390,	training loss: 0.038
epoch 32,	batch   400,	training loss: 0.058
epoch 32,	batch   410,	training loss: 0.085
epoch 32,	batch   420,	training loss: 0.062
epoch 32,	batch   430,	training loss: 0.035
epoch 32,	batch   440,	training loss: 0.058
epoch 32,	batch   450,	training loss: 0.045
epoch 32,	batch   460,	training loss: 0.042
epoch 32,	batch   470,	training loss: 0.056
epoch 32,	batch   480,	training loss: 0.033
epoch 32,	batch   490,	training loss: 0.060
epoch 32,	batch   500,	training loss: 0.033
epoch 32,	batch   510,	training loss: 0.045
epoch 32,	batch   520,	training loss: 0.059
epoch 32,	batch   530,	training loss: 0.110
epoch 32,	batch   540,	training loss: 0.059
epoch 32,	batch   550,	training loss: 0.055
epoch 32,	batch   560,	training loss: 0.052
epoch 32,	batch   570,	training loss: 0.051
epoch 32,	batch   580,	training loss: 0.038
epoch 32,	batch   590,	training loss: 0.037
epoch 32,	batch   600,	training loss: 0.105
epoch 32,	batch   610,	training loss: 0.110
epoch 32,	batch   620,	training loss: 0.063
epoch 32,	batch   630,	training loss: 0.036
epoch 32,	batch   640,	training loss: 0.080
epoch 32,	batch   650,	training loss: 0.081
epoch 32,	batch   660,	training loss: 0.048
epoch 32,	batch   670,	training loss: 0.041
epoch 32,	batch   680,	training loss: 0.069
epoch 32,	batch   690,	training loss: 0.077
epoch 32,	batch   700,	training loss: 0.063
epoch 32,	batch   710,	training loss: 0.074
epoch 32,	batch   720,	training loss: 0.035
epoch 32,	batch   730,	training loss: 0.060
epoch 32,	batch   740,	training loss: 0.044
epoch 32,	batch   750,	training loss: 0.042
epoch 32,	batch   760,	training loss: 0.032
epoch 32,	batch   770,	training loss: 0.026
epoch 32,	batch   780,	training loss: 0.031
epoch 32,	batch   790,	training loss: 0.043
epoch 32,	batch   800,	training loss: 0.053
epoch 32,	batch   810,	training loss: 0.056
epoch 32,	batch   820,	training loss: 0.030
epoch 32,	batch   830,	training loss: 0.048
epoch 32,	batch   840,	training loss: 0.061
epoch 32,	batch   850,	training loss: 0.024
epoch 32,	batch   860,	training loss: 0.044
epoch 32,	batch   870,	training loss: 0.063
epoch 32,	batch   880,	training loss: 0.064
epoch 32,	batch   890,	training loss: 0.065
epoch 32,	batch   900,	training loss: 0.055
epoch 32,	batch   910,	training loss: 0.101
epoch 32,	batch   920,	training loss: 0.108
epoch 32,	batch   930,	training loss: 0.128
epoch 32,	batch   940,	training loss: 0.047
epoch 32,	batch   950,	training loss: 0.081
epoch 32,	batch   960,	training loss: 0.040
epoch 32,	batch   970,	training loss: 0.099
epoch 32,	batch   980,	training loss: 0.060
epoch 32,	batch   990,	training loss: 0.040
epoch 32,	batch  1000,	training loss: 0.076
epoch 32,	batch  1010,	training loss: 0.048
epoch 32,	batch  1020,	training loss: 0.051
epoch 32,	batch  1030,	training loss: 0.053
epoch 32,	batch  1040,	training loss: 0.042
epoch 32,	batch  1050,	training loss: 0.068
epoch 32,	batch  1060,	training loss: 0.053
epoch 32,	batch  1070,	training loss: 0.077
epoch 32,	batch  1080,	training loss: 0.064
epoch 32,	batch  1090,	training loss: 0.050
epoch 32,	batch  1100,	training loss: 0.045
epoch 32,	batch  1110,	training loss: 0.083
epoch 32,	batch  1120,	training loss: 0.050
epoch 32,	batch  1130,	training loss: 0.066
epoch 32,	batch  1140,	training loss: 0.089
epoch 32,	batch  1150,	training loss: 0.071
epoch 32,	batch  1160,	training loss: 0.042
epoch 32,	batch  1170,	training loss: 0.040
epoch 32,	batch  1180,	training loss: 0.060
epoch 32,	batch  1190,	training loss: 0.050
epoch 32,	batch  1200,	training loss: 0.062
epoch 32,	batch  1210,	training loss: 0.063
epoch 32,	batch  1220,	training loss: 0.053
epoch 32,	batch  1230,	training loss: 0.055
epoch 32,	batch  1240,	training loss: 0.091
epoch 32,	batch  1250,	training loss: 0.057
epoch 32,	batch  1260,	training loss: 0.047
epoch 32,	batch  1270,	training loss: 0.048
epoch 32,	batch  1280,	training loss: 0.071
epoch 32,	batch  1290,	training loss: 0.051
epoch 32,	batch  1300,	training loss: 0.046
epoch 32,	batch  1310,	training loss: 0.049
epoch 32,	batch  1320,	training loss: 0.035
epoch 32,	batch  1330,	training loss: 0.034
epoch 32,	batch  1340,	training loss: 0.044
epoch 32,	batch  1350,	training loss: 0.028
epoch 32,	batch  1360,	training loss: 0.105
epoch 32,	batch  1370,	training loss: 0.047
epoch 32,	batch  1380,	training loss: 0.074
epoch 32,	batch  1390,	training loss: 0.051
epoch 32,	batch  1400,	training loss: 0.045
epoch 32,	batch  1410,	training loss: 0.055
epoch 32,	batch  1420,	training loss: 0.047
epoch 32,	batch  1430,	training loss: 0.111
epoch 32,	batch  1440,	training loss: 0.070
epoch 32,	batch  1450,	training loss: 0.057
epoch 32,	batch  1460,	training loss: 0.081
epoch 32,	batch  1470,	training loss: 0.052
epoch 32,	batch  1480,	training loss: 0.067
epoch 32,	batch  1490,	training loss: 0.048
epoch 32,	batch  1500,	training loss: 0.065
epoch 32,	batch  1510,	training loss: 0.063
epoch 32,	batch  1520,	training loss: 0.061
epoch 32,	batch  1530,	training loss: 0.072
epoch 32,	batch  1540,	training loss: 0.059
epoch 32,	batch  1550,	training loss: 0.079
epoch 32,	batch  1560,	training loss: 0.055
epoch 32,	batch  1570,	training loss: 0.057
epoch 32,	batch  1580,	training loss: 0.109
epoch 32,	batch  1590,	training loss: 0.056
epoch 32,	batch  1600,	training loss: 0.056
epoch 32,	batch  1610,	training loss: 0.099
epoch 32,	batch  1620,	training loss: 0.052
epoch 32,	batch  1630,	training loss: 0.042
epoch 32,	batch  1640,	training loss: 0.061
epoch 32,	batch  1650,	training loss: 0.042
epoch 32,	batch  1660,	training loss: 0.064
epoch 32,	batch  1670,	training loss: 0.075
epoch 32,	batch  1680,	training loss: 0.051
epoch 32,	batch  1690,	training loss: 0.044
epoch 32,	batch  1700,	training loss: 0.070
epoch 32,	batch  1710,	training loss: 0.054
epoch 32,	batch  1720,	training loss: 0.078
epoch 32,	batch  1730,	training loss: 0.105
epoch 32,	batch  1740,	training loss: 0.066
epoch 32,	batch  1750,	training loss: 0.049
epoch 32,	batch  1760,	training loss: 0.100
epoch 32,	batch  1770,	training loss: 0.039
epoch 32,	batch  1780,	training loss: 0.084
epoch 32,	batch  1790,	training loss: 0.115
epoch 32,	batch  1800,	training loss: 0.087
epoch 32,	batch  1810,	training loss: 0.049
epoch 32,	batch  1820,	training loss: 0.065
epoch 32,	batch  1830,	training loss: 0.078
epoch 32,	batch  1840,	training loss: 0.102
epoch 32,	batch  1850,	training loss: 0.066
epoch 32,	batch  1860,	training loss: 0.116
epoch 32,	batch  1870,	training loss: 0.074
epoch 32,	batch  1880,	training loss: 0.061
epoch 32,	batch  1890,	training loss: 0.081
epoch 32,	batch  1900,	training loss: 0.068
epoch 32,	batch  1910,	training loss: 0.083
epoch 32,	batch  1920,	training loss: 0.051
epoch 32,	batch  1930,	training loss: 0.064
epoch 32,	batch  1940,	training loss: 0.069
epoch 32,	batch  1950,	training loss: 0.118
epoch 32,	batch  1960,	training loss: 0.071
epoch 32,	batch  1970,	training loss: 0.070
epoch 32,	batch  1980,	training loss: 0.062
END OF EPOCH 32
Testing on validation set...
# correct:	5797/54904 = 10.558429258341834%
# off by 1:	10846/54904 = 19.754480547865366%

epoch 33,	batch    10,	training loss: 0.067
epoch 33,	batch    20,	training loss: 0.072
epoch 33,	batch    30,	training loss: 0.035
epoch 33,	batch    40,	training loss: 0.073
epoch 33,	batch    50,	training loss: 0.019
epoch 33,	batch    60,	training loss: 0.042
epoch 33,	batch    70,	training loss: 0.031
epoch 33,	batch    80,	training loss: 0.030
epoch 33,	batch    90,	training loss: 0.042
epoch 33,	batch   100,	training loss: 0.064
epoch 33,	batch   110,	training loss: 0.045
epoch 33,	batch   120,	training loss: 0.029
epoch 33,	batch   130,	training loss: 0.063
epoch 33,	batch   140,	training loss: 0.034
epoch 33,	batch   150,	training loss: 0.052
epoch 33,	batch   160,	training loss: 0.036
epoch 33,	batch   170,	training loss: 0.022
epoch 33,	batch   180,	training loss: 0.042
epoch 33,	batch   190,	training loss: 0.037
epoch 33,	batch   200,	training loss: 0.029
epoch 33,	batch   210,	training loss: 0.044
epoch 33,	batch   220,	training loss: 0.055
epoch 33,	batch   230,	training loss: 0.065
epoch 33,	batch   240,	training loss: 0.064
epoch 33,	batch   250,	training loss: 0.051
epoch 33,	batch   260,	training loss: 0.074
epoch 33,	batch   270,	training loss: 0.059
epoch 33,	batch   280,	training loss: 0.057
epoch 33,	batch   290,	training loss: 0.037
epoch 33,	batch   300,	training loss: 0.043
epoch 33,	batch   310,	training loss: 0.068
epoch 33,	batch   320,	training loss: 0.057
epoch 33,	batch   330,	training loss: 0.025
epoch 33,	batch   340,	training loss: 0.032
epoch 33,	batch   350,	training loss: 0.031
epoch 33,	batch   360,	training loss: 0.029
epoch 33,	batch   370,	training loss: 0.053
epoch 33,	batch   380,	training loss: 0.030
epoch 33,	batch   390,	training loss: 0.040
epoch 33,	batch   400,	training loss: 0.038
epoch 33,	batch   410,	training loss: 0.050
epoch 33,	batch   420,	training loss: 0.035
epoch 33,	batch   430,	training loss: 0.033
epoch 33,	batch   440,	training loss: 0.036
epoch 33,	batch   450,	training loss: 0.053
epoch 33,	batch   460,	training loss: 0.076
epoch 33,	batch   470,	training loss: 0.115
epoch 33,	batch   480,	training loss: 0.070
epoch 33,	batch   490,	training loss: 0.047
epoch 33,	batch   500,	training loss: 0.082
epoch 33,	batch   510,	training loss: 0.091
epoch 33,	batch   520,	training loss: 0.031
epoch 33,	batch   530,	training loss: 0.059
epoch 33,	batch   540,	training loss: 0.059
epoch 33,	batch   550,	training loss: 0.065
epoch 33,	batch   560,	training loss: 0.041
epoch 33,	batch   570,	training loss: 0.058
epoch 33,	batch   580,	training loss: 0.037
epoch 33,	batch   590,	training loss: 0.053
epoch 33,	batch   600,	training loss: 0.059
epoch 33,	batch   610,	training loss: 0.020
epoch 33,	batch   620,	training loss: 0.059
epoch 33,	batch   630,	training loss: 0.031
epoch 33,	batch   640,	training loss: 0.034
epoch 33,	batch   650,	training loss: 0.028
epoch 33,	batch   660,	training loss: 0.043
epoch 33,	batch   670,	training loss: 0.027
epoch 33,	batch   680,	training loss: 0.050
epoch 33,	batch   690,	training loss: 0.031
epoch 33,	batch   700,	training loss: 0.047
epoch 33,	batch   710,	training loss: 0.047
epoch 33,	batch   720,	training loss: 0.034
epoch 33,	batch   730,	training loss: 0.085
epoch 33,	batch   740,	training loss: 0.045
epoch 33,	batch   750,	training loss: 0.026
epoch 33,	batch   760,	training loss: 0.042
epoch 33,	batch   770,	training loss: 0.034
epoch 33,	batch   780,	training loss: 0.062
epoch 33,	batch   790,	training loss: 0.050
epoch 33,	batch   800,	training loss: 0.028
epoch 33,	batch   810,	training loss: 0.055
epoch 33,	batch   820,	training loss: 0.039
epoch 33,	batch   830,	training loss: 0.099
epoch 33,	batch   840,	training loss: 0.086
epoch 33,	batch   850,	training loss: 0.034
epoch 33,	batch   860,	training loss: 0.059
epoch 33,	batch   870,	training loss: 0.050
epoch 33,	batch   880,	training loss: 0.048
epoch 33,	batch   890,	training loss: 0.033
epoch 33,	batch   900,	training loss: 0.028
epoch 33,	batch   910,	training loss: 0.052
epoch 33,	batch   920,	training loss: 0.067
epoch 33,	batch   930,	training loss: 0.037
epoch 33,	batch   940,	training loss: 0.049
epoch 33,	batch   950,	training loss: 0.042
epoch 33,	batch   960,	training loss: 0.134
epoch 33,	batch   970,	training loss: 0.082
epoch 33,	batch   980,	training loss: 0.059
epoch 33,	batch   990,	training loss: 0.045
epoch 33,	batch  1000,	training loss: 0.056
epoch 33,	batch  1010,	training loss: 0.045
epoch 33,	batch  1020,	training loss: 0.055
epoch 33,	batch  1030,	training loss: 0.060
epoch 33,	batch  1040,	training loss: 0.041
epoch 33,	batch  1050,	training loss: 0.032
epoch 33,	batch  1060,	training loss: 0.047
epoch 33,	batch  1070,	training loss: 0.043
epoch 33,	batch  1080,	training loss: 0.074
epoch 33,	batch  1090,	training loss: 0.044
epoch 33,	batch  1100,	training loss: 0.030
epoch 33,	batch  1110,	training loss: 0.046
epoch 33,	batch  1120,	training loss: 0.038
epoch 33,	batch  1130,	training loss: 0.029
epoch 33,	batch  1140,	training loss: 0.030
epoch 33,	batch  1150,	training loss: 0.029
epoch 33,	batch  1160,	training loss: 0.076
epoch 33,	batch  1170,	training loss: 0.049
epoch 33,	batch  1180,	training loss: 0.041
epoch 33,	batch  1190,	training loss: 0.111
epoch 33,	batch  1200,	training loss: 0.052
epoch 33,	batch  1210,	training loss: 0.055
epoch 33,	batch  1220,	training loss: 0.065
epoch 33,	batch  1230,	training loss: 0.059
epoch 33,	batch  1240,	training loss: 0.052
epoch 33,	batch  1250,	training loss: 0.078
epoch 33,	batch  1260,	training loss: 0.031
epoch 33,	batch  1270,	training loss: 0.025
epoch 33,	batch  1280,	training loss: 0.047
epoch 33,	batch  1290,	training loss: 0.070
epoch 33,	batch  1300,	training loss: 0.083
epoch 33,	batch  1310,	training loss: 0.043
epoch 33,	batch  1320,	training loss: 0.057
epoch 33,	batch  1330,	training loss: 0.067
epoch 33,	batch  1340,	training loss: 0.042
epoch 33,	batch  1350,	training loss: 0.070
epoch 33,	batch  1360,	training loss: 0.041
epoch 33,	batch  1370,	training loss: 0.060
epoch 33,	batch  1380,	training loss: 0.077
epoch 33,	batch  1390,	training loss: 0.087
epoch 33,	batch  1400,	training loss: 0.046
epoch 33,	batch  1410,	training loss: 0.060
epoch 33,	batch  1420,	training loss: 0.083
epoch 33,	batch  1430,	training loss: 0.047
epoch 33,	batch  1440,	training loss: 0.065
epoch 33,	batch  1450,	training loss: 0.069
epoch 33,	batch  1460,	training loss: 0.080
epoch 33,	batch  1470,	training loss: 0.058
epoch 33,	batch  1480,	training loss: 0.094
epoch 33,	batch  1490,	training loss: 0.043
epoch 33,	batch  1500,	training loss: 0.058
epoch 33,	batch  1510,	training loss: 0.075
epoch 33,	batch  1520,	training loss: 0.074
epoch 33,	batch  1530,	training loss: 0.065
epoch 33,	batch  1540,	training loss: 0.047
epoch 33,	batch  1550,	training loss: 0.056
epoch 33,	batch  1560,	training loss: 0.032
epoch 33,	batch  1570,	training loss: 0.086
epoch 33,	batch  1580,	training loss: 0.066
epoch 33,	batch  1590,	training loss: 0.037
epoch 33,	batch  1600,	training loss: 0.038
epoch 33,	batch  1610,	training loss: 0.033
epoch 33,	batch  1620,	training loss: 0.061
epoch 33,	batch  1630,	training loss: 0.039
epoch 33,	batch  1640,	training loss: 0.069
epoch 33,	batch  1650,	training loss: 0.057
epoch 33,	batch  1660,	training loss: 0.066
epoch 33,	batch  1670,	training loss: 0.101
epoch 33,	batch  1680,	training loss: 0.049
epoch 33,	batch  1690,	training loss: 0.090
epoch 33,	batch  1700,	training loss: 0.041
epoch 33,	batch  1710,	training loss: 0.071
epoch 33,	batch  1720,	training loss: 0.052
epoch 33,	batch  1730,	training loss: 0.078
epoch 33,	batch  1740,	training loss: 0.053
epoch 33,	batch  1750,	training loss: 0.059
epoch 33,	batch  1760,	training loss: 0.069
epoch 33,	batch  1770,	training loss: 0.046
epoch 33,	batch  1780,	training loss: 0.078
epoch 33,	batch  1790,	training loss: 0.055
epoch 33,	batch  1800,	training loss: 0.050
epoch 33,	batch  1810,	training loss: 0.070
epoch 33,	batch  1820,	training loss: 0.064
epoch 33,	batch  1830,	training loss: 0.126
epoch 33,	batch  1840,	training loss: 0.056
epoch 33,	batch  1850,	training loss: 0.061
epoch 33,	batch  1860,	training loss: 0.062
epoch 33,	batch  1870,	training loss: 0.056
epoch 33,	batch  1880,	training loss: 0.083
epoch 33,	batch  1890,	training loss: 0.118
epoch 33,	batch  1900,	training loss: 0.063
epoch 33,	batch  1910,	training loss: 0.055
epoch 33,	batch  1920,	training loss: 0.076
epoch 33,	batch  1930,	training loss: 0.068
epoch 33,	batch  1940,	training loss: 0.085
epoch 33,	batch  1950,	training loss: 0.072
epoch 33,	batch  1960,	training loss: 0.055
epoch 33,	batch  1970,	training loss: 0.057
epoch 33,	batch  1980,	training loss: 0.114
END OF EPOCH 33
Testing on validation set...
# correct:	6276/54904 = 11.430861139443392%
# off by 1:	10768/54904 = 19.61241439603672%

epoch 34,	batch    10,	training loss: 0.037
epoch 34,	batch    20,	training loss: 0.045
epoch 34,	batch    30,	training loss: 0.028
epoch 34,	batch    40,	training loss: 0.060
epoch 34,	batch    50,	training loss: 0.023
epoch 34,	batch    60,	training loss: 0.082
epoch 34,	batch    70,	training loss: 0.036
epoch 34,	batch    80,	training loss: 0.058
epoch 34,	batch    90,	training loss: 0.033
epoch 34,	batch   100,	training loss: 0.036
epoch 34,	batch   110,	training loss: 0.041
epoch 34,	batch   120,	training loss: 0.041
epoch 34,	batch   130,	training loss: 0.044
epoch 34,	batch   140,	training loss: 0.051
epoch 34,	batch   150,	training loss: 0.041
epoch 34,	batch   160,	training loss: 0.034
epoch 34,	batch   170,	training loss: 0.102
epoch 34,	batch   180,	training loss: 0.072
epoch 34,	batch   190,	training loss: 0.062
epoch 34,	batch   200,	training loss: 0.043
epoch 34,	batch   210,	training loss: 0.067
epoch 34,	batch   220,	training loss: 0.038
epoch 34,	batch   230,	training loss: 0.056
epoch 34,	batch   240,	training loss: 0.049
epoch 34,	batch   250,	training loss: 0.052
epoch 34,	batch   260,	training loss: 0.044
epoch 34,	batch   270,	training loss: 0.030
epoch 34,	batch   280,	training loss: 0.039
epoch 34,	batch   290,	training loss: 0.076
epoch 34,	batch   300,	training loss: 0.046
epoch 34,	batch   310,	training loss: 0.029
epoch 34,	batch   320,	training loss: 0.051
epoch 34,	batch   330,	training loss: 0.046
epoch 34,	batch   340,	training loss: 0.041
epoch 34,	batch   350,	training loss: 0.050
epoch 34,	batch   360,	training loss: 0.060
epoch 34,	batch   370,	training loss: 0.032
epoch 34,	batch   380,	training loss: 0.048
epoch 34,	batch   390,	training loss: 0.063
epoch 34,	batch   400,	training loss: 0.059
epoch 34,	batch   410,	training loss: 0.042
epoch 34,	batch   420,	training loss: 0.065
epoch 34,	batch   430,	training loss: 0.043
epoch 34,	batch   440,	training loss: 0.034
epoch 34,	batch   450,	training loss: 0.038
epoch 34,	batch   460,	training loss: 0.058
epoch 34,	batch   470,	training loss: 0.040
epoch 34,	batch   480,	training loss: 0.019
epoch 34,	batch   490,	training loss: 0.036
epoch 34,	batch   500,	training loss: 0.027
epoch 34,	batch   510,	training loss: 0.034
epoch 34,	batch   520,	training loss: 0.049
epoch 34,	batch   530,	training loss: 0.070
epoch 34,	batch   540,	training loss: 0.043
epoch 34,	batch   550,	training loss: 0.074
epoch 34,	batch   560,	training loss: 0.053
epoch 34,	batch   570,	training loss: 0.062
epoch 34,	batch   580,	training loss: 0.053
epoch 34,	batch   590,	training loss: 0.049
epoch 34,	batch   600,	training loss: 0.070
epoch 34,	batch   610,	training loss: 0.061
epoch 34,	batch   620,	training loss: 0.042
epoch 34,	batch   630,	training loss: 0.028
epoch 34,	batch   640,	training loss: 0.027
epoch 34,	batch   650,	training loss: 0.060
epoch 34,	batch   660,	training loss: 0.037
epoch 34,	batch   670,	training loss: 0.037
epoch 34,	batch   680,	training loss: 0.040
epoch 34,	batch   690,	training loss: 0.040
epoch 34,	batch   700,	training loss: 0.030
epoch 34,	batch   710,	training loss: 0.032
epoch 34,	batch   720,	training loss: 0.067
epoch 34,	batch   730,	training loss: 0.042
epoch 34,	batch   740,	training loss: 0.106
epoch 34,	batch   750,	training loss: 0.059
epoch 34,	batch   760,	training loss: 0.049
epoch 34,	batch   770,	training loss: 0.073
epoch 34,	batch   780,	training loss: 0.070
epoch 34,	batch   790,	training loss: 0.045
epoch 34,	batch   800,	training loss: 0.039
epoch 34,	batch   810,	training loss: 0.059
epoch 34,	batch   820,	training loss: 0.030
epoch 34,	batch   830,	training loss: 0.047
epoch 34,	batch   840,	training loss: 0.038
epoch 34,	batch   850,	training loss: 0.043
epoch 34,	batch   860,	training loss: 0.033
epoch 34,	batch   870,	training loss: 0.056
epoch 34,	batch   880,	training loss: 0.039
epoch 34,	batch   890,	training loss: 0.050
epoch 34,	batch   900,	training loss: 0.033
epoch 34,	batch   910,	training loss: 0.053
epoch 34,	batch   920,	training loss: 0.044
epoch 34,	batch   930,	training loss: 0.058
epoch 34,	batch   940,	training loss: 0.044
epoch 34,	batch   950,	training loss: 0.049
epoch 34,	batch   960,	training loss: 0.040
epoch 34,	batch   970,	training loss: 0.033
epoch 34,	batch   980,	training loss: 0.018
epoch 34,	batch   990,	training loss: 0.045
epoch 34,	batch  1000,	training loss: 0.096
epoch 34,	batch  1010,	training loss: 0.050
epoch 34,	batch  1020,	training loss: 0.053
epoch 34,	batch  1030,	training loss: 0.035
epoch 34,	batch  1040,	training loss: 0.038
epoch 34,	batch  1050,	training loss: 0.049
epoch 34,	batch  1060,	training loss: 0.069
epoch 34,	batch  1070,	training loss: 0.069
epoch 34,	batch  1080,	training loss: 0.028
epoch 34,	batch  1090,	training loss: 0.055
epoch 34,	batch  1100,	training loss: 0.030
epoch 34,	batch  1110,	training loss: 0.046
epoch 34,	batch  1120,	training loss: 0.039
epoch 34,	batch  1130,	training loss: 0.039
epoch 34,	batch  1140,	training loss: 0.053
epoch 34,	batch  1150,	training loss: 0.059
epoch 34,	batch  1160,	training loss: 0.048
epoch 34,	batch  1170,	training loss: 0.041
epoch 34,	batch  1180,	training loss: 0.035
epoch 34,	batch  1190,	training loss: 0.024
epoch 34,	batch  1200,	training loss: 0.079
epoch 34,	batch  1210,	training loss: 0.038
epoch 34,	batch  1220,	training loss: 0.052
epoch 34,	batch  1230,	training loss: 0.058
epoch 34,	batch  1240,	training loss: 0.047
epoch 34,	batch  1250,	training loss: 0.057
epoch 34,	batch  1260,	training loss: 0.037
epoch 34,	batch  1270,	training loss: 0.034
epoch 34,	batch  1280,	training loss: 0.029
epoch 34,	batch  1290,	training loss: 0.061
epoch 34,	batch  1300,	training loss: 0.040
epoch 34,	batch  1310,	training loss: 0.074
epoch 34,	batch  1320,	training loss: 0.034
epoch 34,	batch  1330,	training loss: 0.038
epoch 34,	batch  1340,	training loss: 0.064
epoch 34,	batch  1350,	training loss: 0.077
epoch 34,	batch  1360,	training loss: 0.038
epoch 34,	batch  1370,	training loss: 0.045
epoch 34,	batch  1380,	training loss: 0.025
epoch 34,	batch  1390,	training loss: 0.034
epoch 34,	batch  1400,	training loss: 0.046
epoch 34,	batch  1410,	training loss: 0.141
epoch 34,	batch  1420,	training loss: 0.070
epoch 34,	batch  1430,	training loss: 0.052
epoch 34,	batch  1440,	training loss: 0.087
epoch 34,	batch  1450,	training loss: 0.041
epoch 34,	batch  1460,	training loss: 0.056
epoch 34,	batch  1470,	training loss: 0.068
epoch 34,	batch  1480,	training loss: 0.042
epoch 34,	batch  1490,	training loss: 0.025
epoch 34,	batch  1500,	training loss: 0.039
epoch 34,	batch  1510,	training loss: 0.060
epoch 34,	batch  1520,	training loss: 0.046
epoch 34,	batch  1530,	training loss: 0.062
epoch 34,	batch  1540,	training loss: 0.056
epoch 34,	batch  1550,	training loss: 0.063
epoch 34,	batch  1560,	training loss: 0.035
epoch 34,	batch  1570,	training loss: 0.059
epoch 34,	batch  1580,	training loss: 0.036
epoch 34,	batch  1590,	training loss: 0.025
epoch 34,	batch  1600,	training loss: 0.032
epoch 34,	batch  1610,	training loss: 0.044
epoch 34,	batch  1620,	training loss: 0.038
epoch 34,	batch  1630,	training loss: 0.054
epoch 34,	batch  1640,	training loss: 0.061
epoch 34,	batch  1650,	training loss: 0.042
epoch 34,	batch  1660,	training loss: 0.034
epoch 34,	batch  1670,	training loss: 0.059
epoch 34,	batch  1680,	training loss: 0.053
epoch 34,	batch  1690,	training loss: 0.051
epoch 34,	batch  1700,	training loss: 0.064
epoch 34,	batch  1710,	training loss: 0.081
epoch 34,	batch  1720,	training loss: 0.051
epoch 34,	batch  1730,	training loss: 0.075
epoch 34,	batch  1740,	training loss: 0.070
epoch 34,	batch  1750,	training loss: 0.064
epoch 34,	batch  1760,	training loss: 0.078
epoch 34,	batch  1770,	training loss: 0.066
epoch 34,	batch  1780,	training loss: 0.127
epoch 34,	batch  1790,	training loss: 0.129
epoch 34,	batch  1800,	training loss: 0.060
epoch 34,	batch  1810,	training loss: 0.063
epoch 34,	batch  1820,	training loss: 0.051
epoch 34,	batch  1830,	training loss: 0.076
epoch 34,	batch  1840,	training loss: 0.076
epoch 34,	batch  1850,	training loss: 0.048
epoch 34,	batch  1860,	training loss: 0.051
epoch 34,	batch  1870,	training loss: 0.064
epoch 34,	batch  1880,	training loss: 0.053
epoch 34,	batch  1890,	training loss: 0.079
epoch 34,	batch  1900,	training loss: 0.063
epoch 34,	batch  1910,	training loss: 0.059
epoch 34,	batch  1920,	training loss: 0.083
epoch 34,	batch  1930,	training loss: 0.041
epoch 34,	batch  1940,	training loss: 0.086
epoch 34,	batch  1950,	training loss: 0.083
epoch 34,	batch  1960,	training loss: 0.038
epoch 34,	batch  1970,	training loss: 0.055
epoch 34,	batch  1980,	training loss: 0.085
END OF EPOCH 34
Testing on validation set...
# correct:	5772/54904 = 10.512895235319831%
# off by 1:	10703/54904 = 19.494025936179515%

epoch 35,	batch    10,	training loss: 0.092
epoch 35,	batch    20,	training loss: 0.087
epoch 35,	batch    30,	training loss: 0.062
epoch 35,	batch    40,	training loss: 0.029
epoch 35,	batch    50,	training loss: 0.028
epoch 35,	batch    60,	training loss: 0.048
epoch 35,	batch    70,	training loss: 0.038
epoch 35,	batch    80,	training loss: 0.057
epoch 35,	batch    90,	training loss: 0.053
epoch 35,	batch   100,	training loss: 0.035
epoch 35,	batch   110,	training loss: 0.032
epoch 35,	batch   120,	training loss: 0.023
epoch 35,	batch   130,	training loss: 0.054
epoch 35,	batch   140,	training loss: 0.028
epoch 35,	batch   150,	training loss: 0.049
epoch 35,	batch   160,	training loss: 0.032
epoch 35,	batch   170,	training loss: 0.029
epoch 35,	batch   180,	training loss: 0.025
epoch 35,	batch   190,	training loss: 0.034
epoch 35,	batch   200,	training loss: 0.039
epoch 35,	batch   210,	training loss: 0.024
epoch 35,	batch   220,	training loss: 0.026
epoch 35,	batch   230,	training loss: 0.040
epoch 35,	batch   240,	training loss: 0.038
epoch 35,	batch   250,	training loss: 0.049
epoch 35,	batch   260,	training loss: 0.022
epoch 35,	batch   270,	training loss: 0.032
epoch 35,	batch   280,	training loss: 0.048
epoch 35,	batch   290,	training loss: 0.037
epoch 35,	batch   300,	training loss: 0.036
epoch 35,	batch   310,	training loss: 0.104
epoch 35,	batch   320,	training loss: 0.022
epoch 35,	batch   330,	training loss: 0.039
epoch 35,	batch   340,	training loss: 0.035
epoch 35,	batch   350,	training loss: 0.030
epoch 35,	batch   360,	training loss: 0.064
epoch 35,	batch   370,	training loss: 0.080
epoch 35,	batch   380,	training loss: 0.058
epoch 35,	batch   390,	training loss: 0.062
epoch 35,	batch   400,	training loss: 0.039
epoch 35,	batch   410,	training loss: 0.025
epoch 35,	batch   420,	training loss: 0.120
epoch 35,	batch   430,	training loss: 0.043
epoch 35,	batch   440,	training loss: 0.038
epoch 35,	batch   450,	training loss: 0.045
epoch 35,	batch   460,	training loss: 0.032
epoch 35,	batch   470,	training loss: 0.049
epoch 35,	batch   480,	training loss: 0.031
epoch 35,	batch   490,	training loss: 0.056
epoch 35,	batch   500,	training loss: 0.031
epoch 35,	batch   510,	training loss: 0.063
epoch 35,	batch   520,	training loss: 0.056
epoch 35,	batch   530,	training loss: 0.040
epoch 35,	batch   540,	training loss: 0.037
epoch 35,	batch   550,	training loss: 0.054
epoch 35,	batch   560,	training loss: 0.065
epoch 35,	batch   570,	training loss: 0.033
epoch 35,	batch   580,	training loss: 0.045
epoch 35,	batch   590,	training loss: 0.029
epoch 35,	batch   600,	training loss: 0.052
epoch 35,	batch   610,	training loss: 0.034
epoch 35,	batch   620,	training loss: 0.039
epoch 35,	batch   630,	training loss: 0.034
epoch 35,	batch   640,	training loss: 0.047
epoch 35,	batch   650,	training loss: 0.059
epoch 35,	batch   660,	training loss: 0.045
epoch 35,	batch   670,	training loss: 0.118
epoch 35,	batch   680,	training loss: 0.019
epoch 35,	batch   690,	training loss: 0.050
epoch 35,	batch   700,	training loss: 0.030
epoch 35,	batch   710,	training loss: 0.056
epoch 35,	batch   720,	training loss: 0.055
epoch 35,	batch   730,	training loss: 0.074
epoch 35,	batch   740,	training loss: 0.067
epoch 35,	batch   750,	training loss: 0.060
epoch 35,	batch   760,	training loss: 0.039
epoch 35,	batch   770,	training loss: 0.049
epoch 35,	batch   780,	training loss: 0.045
epoch 35,	batch   790,	training loss: 0.092
epoch 35,	batch   800,	training loss: 0.057
epoch 35,	batch   810,	training loss: 0.064
epoch 35,	batch   820,	training loss: 0.055
epoch 35,	batch   830,	training loss: 0.033
epoch 35,	batch   840,	training loss: 0.058
epoch 35,	batch   850,	training loss: 0.039
epoch 35,	batch   860,	training loss: 0.059
epoch 35,	batch   870,	training loss: 0.077
epoch 35,	batch   880,	training loss: 0.031
epoch 35,	batch   890,	training loss: 0.052
epoch 35,	batch   900,	training loss: 0.060
epoch 35,	batch   910,	training loss: 0.053
epoch 35,	batch   920,	training loss: 0.066
epoch 35,	batch   930,	training loss: 0.058
epoch 35,	batch   940,	training loss: 0.058
epoch 35,	batch   950,	training loss: 0.057
epoch 35,	batch   960,	training loss: 0.053
epoch 35,	batch   970,	training loss: 0.075
epoch 35,	batch   980,	training loss: 0.044
epoch 35,	batch   990,	training loss: 0.067
epoch 35,	batch  1000,	training loss: 0.039
epoch 35,	batch  1010,	training loss: 0.047
epoch 35,	batch  1020,	training loss: 0.048
epoch 35,	batch  1030,	training loss: 0.039
epoch 35,	batch  1040,	training loss: 0.035
epoch 35,	batch  1050,	training loss: 0.061
epoch 35,	batch  1060,	training loss: 0.027
epoch 35,	batch  1070,	training loss: 0.020
epoch 35,	batch  1080,	training loss: 0.021
epoch 35,	batch  1090,	training loss: 0.064
epoch 35,	batch  1100,	training loss: 0.047
epoch 35,	batch  1110,	training loss: 0.057
epoch 35,	batch  1120,	training loss: 0.086
epoch 35,	batch  1130,	training loss: 0.066
epoch 35,	batch  1140,	training loss: 0.047
epoch 35,	batch  1150,	training loss: 0.035
epoch 35,	batch  1160,	training loss: 0.044
epoch 35,	batch  1170,	training loss: 0.056
epoch 35,	batch  1180,	training loss: 0.055
epoch 35,	batch  1190,	training loss: 0.076
epoch 35,	batch  1200,	training loss: 0.041
epoch 35,	batch  1210,	training loss: 0.039
epoch 35,	batch  1220,	training loss: 0.053
epoch 35,	batch  1230,	training loss: 0.077
epoch 35,	batch  1240,	training loss: 0.040
epoch 35,	batch  1250,	training loss: 0.068
epoch 35,	batch  1260,	training loss: 0.065
epoch 35,	batch  1270,	training loss: 0.048
epoch 35,	batch  1280,	training loss: 0.026
epoch 35,	batch  1290,	training loss: 0.036
epoch 35,	batch  1300,	training loss: 0.060
epoch 35,	batch  1310,	training loss: 0.032
epoch 35,	batch  1320,	training loss: 0.063
epoch 35,	batch  1330,	training loss: 0.055
epoch 35,	batch  1340,	training loss: 0.042
epoch 35,	batch  1350,	training loss: 0.037
epoch 35,	batch  1360,	training loss: 0.054
epoch 35,	batch  1370,	training loss: 0.024
epoch 35,	batch  1380,	training loss: 0.043
epoch 35,	batch  1390,	training loss: 0.060
epoch 35,	batch  1400,	training loss: 0.062
epoch 35,	batch  1410,	training loss: 0.067
epoch 35,	batch  1420,	training loss: 0.020
epoch 35,	batch  1430,	training loss: 0.047
epoch 35,	batch  1440,	training loss: 0.059
epoch 35,	batch  1450,	training loss: 0.093
epoch 35,	batch  1460,	training loss: 0.035
epoch 35,	batch  1470,	training loss: 0.047
epoch 35,	batch  1480,	training loss: 0.101
epoch 35,	batch  1490,	training loss: 0.039
epoch 35,	batch  1500,	training loss: 0.064
epoch 35,	batch  1510,	training loss: 0.099
epoch 35,	batch  1520,	training loss: 0.058
epoch 35,	batch  1530,	training loss: 0.045
epoch 35,	batch  1540,	training loss: 0.081
epoch 35,	batch  1550,	training loss: 0.027
epoch 35,	batch  1560,	training loss: 0.079
epoch 35,	batch  1570,	training loss: 0.048
epoch 35,	batch  1580,	training loss: 0.063
epoch 35,	batch  1590,	training loss: 0.044
epoch 35,	batch  1600,	training loss: 0.046
epoch 35,	batch  1610,	training loss: 0.047
epoch 35,	batch  1620,	training loss: 0.038
epoch 35,	batch  1630,	training loss: 0.052
epoch 35,	batch  1640,	training loss: 0.046
epoch 35,	batch  1650,	training loss: 0.046
epoch 35,	batch  1660,	training loss: 0.044
epoch 35,	batch  1670,	training loss: 0.054
epoch 35,	batch  1680,	training loss: 0.044
epoch 35,	batch  1690,	training loss: 0.045
epoch 35,	batch  1700,	training loss: 0.044
epoch 35,	batch  1710,	training loss: 0.069
epoch 35,	batch  1720,	training loss: 0.031
epoch 35,	batch  1730,	training loss: 0.041
epoch 35,	batch  1740,	training loss: 0.024
epoch 35,	batch  1750,	training loss: 0.054
epoch 35,	batch  1760,	training loss: 0.037
epoch 35,	batch  1770,	training loss: 0.034
epoch 35,	batch  1780,	training loss: 0.042
epoch 35,	batch  1790,	training loss: 0.072
epoch 35,	batch  1800,	training loss: 0.049
epoch 35,	batch  1810,	training loss: 0.063
epoch 35,	batch  1820,	training loss: 0.042
epoch 35,	batch  1830,	training loss: 0.072
epoch 35,	batch  1840,	training loss: 0.064
epoch 35,	batch  1850,	training loss: 0.073
epoch 35,	batch  1860,	training loss: 0.052
epoch 35,	batch  1870,	training loss: 0.056
epoch 35,	batch  1880,	training loss: 0.079
epoch 35,	batch  1890,	training loss: 0.056
epoch 35,	batch  1900,	training loss: 0.051
epoch 35,	batch  1910,	training loss: 0.068
epoch 35,	batch  1920,	training loss: 0.063
epoch 35,	batch  1930,	training loss: 0.052
epoch 35,	batch  1940,	training loss: 0.062
epoch 35,	batch  1950,	training loss: 0.059
epoch 35,	batch  1960,	training loss: 0.081
epoch 35,	batch  1970,	training loss: 0.077
epoch 35,	batch  1980,	training loss: 0.057
END OF EPOCH 35
Testing on validation set...
# correct:	5784/54904 = 10.534751566370392%
# off by 1:	10647/54904 = 19.39202972461023%

epoch 36,	batch    10,	training loss: 0.063
epoch 36,	batch    20,	training loss: 0.033
epoch 36,	batch    30,	training loss: 0.048
epoch 36,	batch    40,	training loss: 0.052
epoch 36,	batch    50,	training loss: 0.024
epoch 36,	batch    60,	training loss: 0.061
epoch 36,	batch    70,	training loss: 0.023
epoch 36,	batch    80,	training loss: 0.045
epoch 36,	batch    90,	training loss: 0.036
epoch 36,	batch   100,	training loss: 0.030
epoch 36,	batch   110,	training loss: 0.021
epoch 36,	batch   120,	training loss: 0.032
epoch 36,	batch   130,	training loss: 0.020
epoch 36,	batch   140,	training loss: 0.026
epoch 36,	batch   150,	training loss: 0.073
epoch 36,	batch   160,	training loss: 0.025
epoch 36,	batch   170,	training loss: 0.041
epoch 36,	batch   180,	training loss: 0.073
epoch 36,	batch   190,	training loss: 0.037
epoch 36,	batch   200,	training loss: 0.059
epoch 36,	batch   210,	training loss: 0.039
epoch 36,	batch   220,	training loss: 0.045
epoch 36,	batch   230,	training loss: 0.042
epoch 36,	batch   240,	training loss: 0.038
epoch 36,	batch   250,	training loss: 0.057
epoch 36,	batch   260,	training loss: 0.047
epoch 36,	batch   270,	training loss: 0.051
epoch 36,	batch   280,	training loss: 0.064
epoch 36,	batch   290,	training loss: 0.046
epoch 36,	batch   300,	training loss: 0.047
epoch 36,	batch   310,	training loss: 0.034
epoch 36,	batch   320,	training loss: 0.045
epoch 36,	batch   330,	training loss: 0.030
epoch 36,	batch   340,	training loss: 0.028
epoch 36,	batch   350,	training loss: 0.056
epoch 36,	batch   360,	training loss: 0.030
epoch 36,	batch   370,	training loss: 0.024
epoch 36,	batch   380,	training loss: 0.048
epoch 36,	batch   390,	training loss: 0.037
epoch 36,	batch   400,	training loss: 0.049
epoch 36,	batch   410,	training loss: 0.035
epoch 36,	batch   420,	training loss: 0.070
epoch 36,	batch   430,	training loss: 0.021
epoch 36,	batch   440,	training loss: 0.048
epoch 36,	batch   450,	training loss: 0.084
epoch 36,	batch   460,	training loss: 0.043
epoch 36,	batch   470,	training loss: 0.062
epoch 36,	batch   480,	training loss: 0.034
epoch 36,	batch   490,	training loss: 0.024
epoch 36,	batch   500,	training loss: 0.089
epoch 36,	batch   510,	training loss: 0.091
epoch 36,	batch   520,	training loss: 0.050
epoch 36,	batch   530,	training loss: 0.033
epoch 36,	batch   540,	training loss: 0.040
epoch 36,	batch   550,	training loss: 0.041
epoch 36,	batch   560,	training loss: 0.023
epoch 36,	batch   570,	training loss: 0.054
epoch 36,	batch   580,	training loss: 0.034
epoch 36,	batch   590,	training loss: 0.041
epoch 36,	batch   600,	training loss: 0.045
epoch 36,	batch   610,	training loss: 0.025
epoch 36,	batch   620,	training loss: 0.050
epoch 36,	batch   630,	training loss: 0.031
epoch 36,	batch   640,	training loss: 0.055
epoch 36,	batch   650,	training loss: 0.025
epoch 36,	batch   660,	training loss: 0.028
epoch 36,	batch   670,	training loss: 0.025
epoch 36,	batch   680,	training loss: 0.020
epoch 36,	batch   690,	training loss: 0.047
epoch 36,	batch   700,	training loss: 0.031
epoch 36,	batch   710,	training loss: 0.025
epoch 36,	batch   720,	training loss: 0.072
epoch 36,	batch   730,	training loss: 0.044
epoch 36,	batch   740,	training loss: 0.052
epoch 36,	batch   750,	training loss: 0.020
epoch 36,	batch   760,	training loss: 0.063
epoch 36,	batch   770,	training loss: 0.029
epoch 36,	batch   780,	training loss: 0.072
epoch 36,	batch   790,	training loss: 0.040
epoch 36,	batch   800,	training loss: 0.042
epoch 36,	batch   810,	training loss: 0.045
epoch 36,	batch   820,	training loss: 0.037
epoch 36,	batch   830,	training loss: 0.046
epoch 36,	batch   840,	training loss: 0.059
epoch 36,	batch   850,	training loss: 0.046
epoch 36,	batch   860,	training loss: 0.044
epoch 36,	batch   870,	training loss: 0.053
epoch 36,	batch   880,	training loss: 0.053
epoch 36,	batch   890,	training loss: 0.038
epoch 36,	batch   900,	training loss: 0.021
epoch 36,	batch   910,	training loss: 0.082
epoch 36,	batch   920,	training loss: 0.039
epoch 36,	batch   930,	training loss: 0.031
epoch 36,	batch   940,	training loss: 0.029
epoch 36,	batch   950,	training loss: 0.073
epoch 36,	batch   960,	training loss: 0.088
epoch 36,	batch   970,	training loss: 0.026
epoch 36,	batch   980,	training loss: 0.065
epoch 36,	batch   990,	training loss: 0.088
epoch 36,	batch  1000,	training loss: 0.067
epoch 36,	batch  1010,	training loss: 0.047
epoch 36,	batch  1020,	training loss: 0.039
epoch 36,	batch  1030,	training loss: 0.032
epoch 36,	batch  1040,	training loss: 0.043
epoch 36,	batch  1050,	training loss: 0.067
epoch 36,	batch  1060,	training loss: 0.028
epoch 36,	batch  1070,	training loss: 0.096
epoch 36,	batch  1080,	training loss: 0.064
epoch 36,	batch  1090,	training loss: 0.031
epoch 36,	batch  1100,	training loss: 0.045
epoch 36,	batch  1110,	training loss: 0.020
epoch 36,	batch  1120,	training loss: 0.036
epoch 36,	batch  1130,	training loss: 0.038
epoch 36,	batch  1140,	training loss: 0.036
epoch 36,	batch  1150,	training loss: 0.051
epoch 36,	batch  1160,	training loss: 0.054
epoch 36,	batch  1170,	training loss: 0.069
epoch 36,	batch  1180,	training loss: 0.035
epoch 36,	batch  1190,	training loss: 0.102
epoch 36,	batch  1200,	training loss: 0.065
epoch 36,	batch  1210,	training loss: 0.045
epoch 36,	batch  1220,	training loss: 0.052
epoch 36,	batch  1230,	training loss: 0.070
epoch 36,	batch  1240,	training loss: 0.043
epoch 36,	batch  1250,	training loss: 0.047
epoch 36,	batch  1260,	training loss: 0.036
epoch 36,	batch  1270,	training loss: 0.080
epoch 36,	batch  1280,	training loss: 0.053
epoch 36,	batch  1290,	training loss: 0.035
epoch 36,	batch  1300,	training loss: 0.068
epoch 36,	batch  1310,	training loss: 0.038
epoch 36,	batch  1320,	training loss: 0.040
epoch 36,	batch  1330,	training loss: 0.053
epoch 36,	batch  1340,	training loss: 0.084
epoch 36,	batch  1350,	training loss: 0.053
epoch 36,	batch  1360,	training loss: 0.031
epoch 36,	batch  1370,	training loss: 0.032
epoch 36,	batch  1380,	training loss: 0.053
epoch 36,	batch  1390,	training loss: 0.087
epoch 36,	batch  1400,	training loss: 0.056
epoch 36,	batch  1410,	training loss: 0.065
epoch 36,	batch  1420,	training loss: 0.068
epoch 36,	batch  1430,	training loss: 0.050
epoch 36,	batch  1440,	training loss: 0.055
epoch 36,	batch  1450,	training loss: 0.070
epoch 36,	batch  1460,	training loss: 0.066
epoch 36,	batch  1470,	training loss: 0.032
epoch 36,	batch  1480,	training loss: 0.066
epoch 36,	batch  1490,	training loss: 0.053
epoch 36,	batch  1500,	training loss: 0.052
epoch 36,	batch  1510,	training loss: 0.049
epoch 36,	batch  1520,	training loss: 0.056
epoch 36,	batch  1530,	training loss: 0.074
epoch 36,	batch  1540,	training loss: 0.052
epoch 36,	batch  1550,	training loss: 0.078
epoch 36,	batch  1560,	training loss: 0.035
epoch 36,	batch  1570,	training loss: 0.036
epoch 36,	batch  1580,	training loss: 0.047
epoch 36,	batch  1590,	training loss: 0.047
epoch 36,	batch  1600,	training loss: 0.047
epoch 36,	batch  1610,	training loss: 0.038
epoch 36,	batch  1620,	training loss: 0.069
epoch 36,	batch  1630,	training loss: 0.035
epoch 36,	batch  1640,	training loss: 0.040
epoch 36,	batch  1650,	training loss: 0.033
epoch 36,	batch  1660,	training loss: 0.034
epoch 36,	batch  1670,	training loss: 0.042
epoch 36,	batch  1680,	training loss: 0.116
epoch 36,	batch  1690,	training loss: 0.071
epoch 36,	batch  1700,	training loss: 0.069
epoch 36,	batch  1710,	training loss: 0.072
epoch 36,	batch  1720,	training loss: 0.021
epoch 36,	batch  1730,	training loss: 0.022
epoch 36,	batch  1740,	training loss: 0.071
epoch 36,	batch  1750,	training loss: 0.049
epoch 36,	batch  1760,	training loss: 0.043
epoch 36,	batch  1770,	training loss: 0.039
epoch 36,	batch  1780,	training loss: 0.020
epoch 36,	batch  1790,	training loss: 0.094
epoch 36,	batch  1800,	training loss: 0.051
epoch 36,	batch  1810,	training loss: 0.059
epoch 36,	batch  1820,	training loss: 0.061
epoch 36,	batch  1830,	training loss: 0.093
epoch 36,	batch  1840,	training loss: 0.058
epoch 36,	batch  1850,	training loss: 0.056
epoch 36,	batch  1860,	training loss: 0.070
epoch 36,	batch  1870,	training loss: 0.050
epoch 36,	batch  1880,	training loss: 0.047
epoch 36,	batch  1890,	training loss: 0.079
epoch 36,	batch  1900,	training loss: 0.057
epoch 36,	batch  1910,	training loss: 0.062
epoch 36,	batch  1920,	training loss: 0.027
epoch 36,	batch  1930,	training loss: 0.029
epoch 36,	batch  1940,	training loss: 0.051
epoch 36,	batch  1950,	training loss: 0.085
epoch 36,	batch  1960,	training loss: 0.067
epoch 36,	batch  1970,	training loss: 0.077
epoch 36,	batch  1980,	training loss: 0.069
END OF EPOCH 36
Testing on validation set...
# correct:	6007/54904 = 10.94091505172665%
# off by 1:	11061/54904 = 20.146073145854583%

epoch 37,	batch    10,	training loss: 0.024
epoch 37,	batch    20,	training loss: 0.059
epoch 37,	batch    30,	training loss: 0.044
epoch 37,	batch    40,	training loss: 0.047
epoch 37,	batch    50,	training loss: 0.045
epoch 37,	batch    60,	training loss: 0.073
epoch 37,	batch    70,	training loss: 0.047
epoch 37,	batch    80,	training loss: 0.049
epoch 37,	batch    90,	training loss: 0.053
epoch 37,	batch   100,	training loss: 0.102
epoch 37,	batch   110,	training loss: 0.031
epoch 37,	batch   120,	training loss: 0.042
epoch 37,	batch   130,	training loss: 0.070
epoch 37,	batch   140,	training loss: 0.025
epoch 37,	batch   150,	training loss: 0.039
epoch 37,	batch   160,	training loss: 0.029
epoch 37,	batch   170,	training loss: 0.026
epoch 37,	batch   180,	training loss: 0.039
epoch 37,	batch   190,	training loss: 0.059
epoch 37,	batch   200,	training loss: 0.056
epoch 37,	batch   210,	training loss: 0.020
epoch 37,	batch   220,	training loss: 0.054
epoch 37,	batch   230,	training loss: 0.023
epoch 37,	batch   240,	training loss: 0.037
epoch 37,	batch   250,	training loss: 0.059
epoch 37,	batch   260,	training loss: 0.054
epoch 37,	batch   270,	training loss: 0.047
epoch 37,	batch   280,	training loss: 0.031
epoch 37,	batch   290,	training loss: 0.085
epoch 37,	batch   300,	training loss: 0.033
epoch 37,	batch   310,	training loss: 0.019
epoch 37,	batch   320,	training loss: 0.049
epoch 37,	batch   330,	training loss: 0.020
epoch 37,	batch   340,	training loss: 0.031
epoch 37,	batch   350,	training loss: 0.042
epoch 37,	batch   360,	training loss: 0.027
epoch 37,	batch   370,	training loss: 0.061
epoch 37,	batch   380,	training loss: 0.031
epoch 37,	batch   390,	training loss: 0.051
epoch 37,	batch   400,	training loss: 0.038
epoch 37,	batch   410,	training loss: 0.051
epoch 37,	batch   420,	training loss: 0.022
epoch 37,	batch   430,	training loss: 0.045
epoch 37,	batch   440,	training loss: 0.033
epoch 37,	batch   450,	training loss: 0.057
epoch 37,	batch   460,	training loss: 0.046
epoch 37,	batch   470,	training loss: 0.020
epoch 37,	batch   480,	training loss: 0.022
epoch 37,	batch   490,	training loss: 0.019
epoch 37,	batch   500,	training loss: 0.033
epoch 37,	batch   510,	training loss: 0.021
epoch 37,	batch   520,	training loss: 0.073
epoch 37,	batch   530,	training loss: 0.033
epoch 37,	batch   540,	training loss: 0.043
epoch 37,	batch   550,	training loss: 0.024
epoch 37,	batch   560,	training loss: 0.040
epoch 37,	batch   570,	training loss: 0.032
epoch 37,	batch   580,	training loss: 0.026
epoch 37,	batch   590,	training loss: 0.028
epoch 37,	batch   600,	training loss: 0.058
epoch 37,	batch   610,	training loss: 0.040
epoch 37,	batch   620,	training loss: 0.047
epoch 37,	batch   630,	training loss: 0.026
epoch 37,	batch   640,	training loss: 0.046
epoch 37,	batch   650,	training loss: 0.027
epoch 37,	batch   660,	training loss: 0.030
epoch 37,	batch   670,	training loss: 0.029
epoch 37,	batch   680,	training loss: 0.045
epoch 37,	batch   690,	training loss: 0.028
epoch 37,	batch   700,	training loss: 0.054
epoch 37,	batch   710,	training loss: 0.031
epoch 37,	batch   720,	training loss: 0.020
epoch 37,	batch   730,	training loss: 0.032
epoch 37,	batch   740,	training loss: 0.026
epoch 37,	batch   750,	training loss: 0.026
epoch 37,	batch   760,	training loss: 0.027
epoch 37,	batch   770,	training loss: 0.036
epoch 37,	batch   780,	training loss: 0.023
epoch 37,	batch   790,	training loss: 0.021
epoch 37,	batch   800,	training loss: 0.049
epoch 37,	batch   810,	training loss: 0.028
epoch 37,	batch   820,	training loss: 0.034
epoch 37,	batch   830,	training loss: 0.024
epoch 37,	batch   840,	training loss: 0.053
epoch 37,	batch   850,	training loss: 0.035
epoch 37,	batch   860,	training loss: 0.059
epoch 37,	batch   870,	training loss: 0.048
epoch 37,	batch   880,	training loss: 0.033
epoch 37,	batch   890,	training loss: 0.040
epoch 37,	batch   900,	training loss: 0.033
epoch 37,	batch   910,	training loss: 0.022
epoch 37,	batch   920,	training loss: 0.031
epoch 37,	batch   930,	training loss: 0.043
epoch 37,	batch   940,	training loss: 0.030
epoch 37,	batch   950,	training loss: 0.031
epoch 37,	batch   960,	training loss: 0.017
epoch 37,	batch   970,	training loss: 0.019
epoch 37,	batch   980,	training loss: 0.061
epoch 37,	batch   990,	training loss: 0.057
epoch 37,	batch  1000,	training loss: 0.064
epoch 37,	batch  1010,	training loss: 0.040
epoch 37,	batch  1020,	training loss: 0.057
epoch 37,	batch  1030,	training loss: 0.020
epoch 37,	batch  1040,	training loss: 0.049
epoch 37,	batch  1050,	training loss: 0.055
epoch 37,	batch  1060,	training loss: 0.051
epoch 37,	batch  1070,	training loss: 0.027
epoch 37,	batch  1080,	training loss: 0.030
epoch 37,	batch  1090,	training loss: 0.046
epoch 37,	batch  1100,	training loss: 0.046
epoch 37,	batch  1110,	training loss: 0.069
epoch 37,	batch  1120,	training loss: 0.057
epoch 37,	batch  1130,	training loss: 0.048
epoch 37,	batch  1140,	training loss: 0.025
epoch 37,	batch  1150,	training loss: 0.049
epoch 37,	batch  1160,	training loss: 0.079
epoch 37,	batch  1170,	training loss: 0.084
epoch 37,	batch  1180,	training loss: 0.055
epoch 37,	batch  1190,	training loss: 0.073
epoch 37,	batch  1200,	training loss: 0.066
epoch 37,	batch  1210,	training loss: 0.063
epoch 37,	batch  1220,	training loss: 0.032
epoch 37,	batch  1230,	training loss: 0.061
epoch 37,	batch  1240,	training loss: 0.030
epoch 37,	batch  1250,	training loss: 0.070
epoch 37,	batch  1260,	training loss: 0.062
epoch 37,	batch  1270,	training loss: 0.074
epoch 37,	batch  1280,	training loss: 0.023
epoch 37,	batch  1290,	training loss: 0.057
epoch 37,	batch  1300,	training loss: 0.037
epoch 37,	batch  1310,	training loss: 0.050
epoch 37,	batch  1320,	training loss: 0.039
epoch 37,	batch  1330,	training loss: 0.014
epoch 37,	batch  1340,	training loss: 0.017
epoch 37,	batch  1350,	training loss: 0.037
epoch 37,	batch  1360,	training loss: 0.031
epoch 37,	batch  1370,	training loss: 0.052
epoch 37,	batch  1380,	training loss: 0.040
epoch 37,	batch  1390,	training loss: 0.031
epoch 37,	batch  1400,	training loss: 0.073
epoch 37,	batch  1410,	training loss: 0.051
epoch 37,	batch  1420,	training loss: 0.054
epoch 37,	batch  1430,	training loss: 0.028
epoch 37,	batch  1440,	training loss: 0.060
epoch 37,	batch  1450,	training loss: 0.036
epoch 37,	batch  1460,	training loss: 0.031
epoch 37,	batch  1470,	training loss: 0.058
epoch 37,	batch  1480,	training loss: 0.028
epoch 37,	batch  1490,	training loss: 0.024
epoch 37,	batch  1500,	training loss: 0.063
epoch 37,	batch  1510,	training loss: 0.039
epoch 37,	batch  1520,	training loss: 0.041
epoch 37,	batch  1530,	training loss: 0.063
epoch 37,	batch  1540,	training loss: 0.026
epoch 37,	batch  1550,	training loss: 0.027
epoch 37,	batch  1560,	training loss: 0.058
epoch 37,	batch  1570,	training loss: 0.068
epoch 37,	batch  1580,	training loss: 0.019
epoch 37,	batch  1590,	training loss: 0.049
epoch 37,	batch  1600,	training loss: 0.020
epoch 37,	batch  1610,	training loss: 0.041
epoch 37,	batch  1620,	training loss: 0.039
epoch 37,	batch  1630,	training loss: 0.019
epoch 37,	batch  1640,	training loss: 0.027
epoch 37,	batch  1650,	training loss: 0.028
epoch 37,	batch  1660,	training loss: 0.030
epoch 37,	batch  1670,	training loss: 0.013
epoch 37,	batch  1680,	training loss: 0.041
epoch 37,	batch  1690,	training loss: 0.027
epoch 37,	batch  1700,	training loss: 0.041
epoch 37,	batch  1710,	training loss: 0.042
epoch 37,	batch  1720,	training loss: 0.053
epoch 37,	batch  1730,	training loss: 0.052
epoch 37,	batch  1740,	training loss: 0.080
epoch 37,	batch  1750,	training loss: 0.045
epoch 37,	batch  1760,	training loss: 0.063
epoch 37,	batch  1770,	training loss: 0.072
epoch 37,	batch  1780,	training loss: 0.032
epoch 37,	batch  1790,	training loss: 0.055
epoch 37,	batch  1800,	training loss: 0.038
epoch 37,	batch  1810,	training loss: 0.014
epoch 37,	batch  1820,	training loss: 0.051
epoch 37,	batch  1830,	training loss: 0.035
epoch 37,	batch  1840,	training loss: 0.041
epoch 37,	batch  1850,	training loss: 0.035
epoch 37,	batch  1860,	training loss: 0.035
epoch 37,	batch  1870,	training loss: 0.034
epoch 37,	batch  1880,	training loss: 0.029
epoch 37,	batch  1890,	training loss: 0.046
epoch 37,	batch  1900,	training loss: 0.039
epoch 37,	batch  1910,	training loss: 0.056
epoch 37,	batch  1920,	training loss: 0.054
epoch 37,	batch  1930,	training loss: 0.041
epoch 37,	batch  1940,	training loss: 0.034
epoch 37,	batch  1950,	training loss: 0.042
epoch 37,	batch  1960,	training loss: 0.056
epoch 37,	batch  1970,	training loss: 0.035
epoch 37,	batch  1980,	training loss: 0.035
END OF EPOCH 37
Testing on validation set...
# correct:	6013/54904 = 10.95184321725193%
# off by 1:	10949/54904 = 19.942080722716014%

epoch 38,	batch    10,	training loss: 0.028
epoch 38,	batch    20,	training loss: 0.031
epoch 38,	batch    30,	training loss: 0.019
epoch 38,	batch    40,	training loss: 0.014
epoch 38,	batch    50,	training loss: 0.019
epoch 38,	batch    60,	training loss: 0.015
epoch 38,	batch    70,	training loss: 0.021
epoch 38,	batch    80,	training loss: 0.031
epoch 38,	batch    90,	training loss: 0.019
epoch 38,	batch   100,	training loss: 0.032
epoch 38,	batch   110,	training loss: 0.043
epoch 38,	batch   120,	training loss: 0.023
epoch 38,	batch   130,	training loss: 0.031
epoch 38,	batch   140,	training loss: 0.033
epoch 38,	batch   150,	training loss: 0.020
epoch 38,	batch   160,	training loss: 0.022
epoch 38,	batch   170,	training loss: 0.032
epoch 38,	batch   180,	training loss: 0.033
epoch 38,	batch   190,	training loss: 0.030
epoch 38,	batch   200,	training loss: 0.018
epoch 38,	batch   210,	training loss: 0.023
epoch 38,	batch   220,	training loss: 0.015
epoch 38,	batch   230,	training loss: 0.026
epoch 38,	batch   240,	training loss: 0.028
epoch 38,	batch   250,	training loss: 0.011
epoch 38,	batch   260,	training loss: 0.026
epoch 38,	batch   270,	training loss: 0.025
epoch 38,	batch   280,	training loss: 0.023
epoch 38,	batch   290,	training loss: 0.025
epoch 38,	batch   300,	training loss: 0.053
epoch 38,	batch   310,	training loss: 0.034
epoch 38,	batch   320,	training loss: 0.050
epoch 38,	batch   330,	training loss: 0.018
epoch 38,	batch   340,	training loss: 0.076
epoch 38,	batch   350,	training loss: 0.026
epoch 38,	batch   360,	training loss: 0.025
epoch 38,	batch   370,	training loss: 0.056
epoch 38,	batch   380,	training loss: 0.027
epoch 38,	batch   390,	training loss: 0.081
epoch 38,	batch   400,	training loss: 0.029
epoch 38,	batch   410,	training loss: 0.026
epoch 38,	batch   420,	training loss: 0.015
epoch 38,	batch   430,	training loss: 0.027
epoch 38,	batch   440,	training loss: 0.054
epoch 38,	batch   450,	training loss: 0.030
epoch 38,	batch   460,	training loss: 0.019
epoch 38,	batch   470,	training loss: 0.045
epoch 38,	batch   480,	training loss: 0.029
epoch 38,	batch   490,	training loss: 0.080
epoch 38,	batch   500,	training loss: 0.020
epoch 38,	batch   510,	training loss: 0.048
epoch 38,	batch   520,	training loss: 0.049
epoch 38,	batch   530,	training loss: 0.063
epoch 38,	batch   540,	training loss: 0.044
epoch 38,	batch   550,	training loss: 0.043
epoch 38,	batch   560,	training loss: 0.053
epoch 38,	batch   570,	training loss: 0.032
epoch 38,	batch   580,	training loss: 0.027
epoch 38,	batch   590,	training loss: 0.042
epoch 38,	batch   600,	training loss: 0.037
epoch 38,	batch   610,	training loss: 0.028
epoch 38,	batch   620,	training loss: 0.038
epoch 38,	batch   630,	training loss: 0.020
epoch 38,	batch   640,	training loss: 0.036
epoch 38,	batch   650,	training loss: 0.039
epoch 38,	batch   660,	training loss: 0.030
epoch 38,	batch   670,	training loss: 0.045
epoch 38,	batch   680,	training loss: 0.025
epoch 38,	batch   690,	training loss: 0.014
epoch 38,	batch   700,	training loss: 0.020
epoch 38,	batch   710,	training loss: 0.014
epoch 38,	batch   720,	training loss: 0.027
epoch 38,	batch   730,	training loss: 0.019
epoch 38,	batch   740,	training loss: 0.025
epoch 38,	batch   750,	training loss: 0.048
epoch 38,	batch   760,	training loss: 0.040
epoch 38,	batch   770,	training loss: 0.027
epoch 38,	batch   780,	training loss: 0.029
epoch 38,	batch   790,	training loss: 0.009
epoch 38,	batch   800,	training loss: 0.015
epoch 38,	batch   810,	training loss: 0.022
epoch 38,	batch   820,	training loss: 0.016
epoch 38,	batch   830,	training loss: 0.036
epoch 38,	batch   840,	training loss: 0.060
epoch 38,	batch   850,	training loss: 0.025
epoch 38,	batch   860,	training loss: 0.020
epoch 38,	batch   870,	training loss: 0.025
epoch 38,	batch   880,	training loss: 0.060
epoch 38,	batch   890,	training loss: 0.018
epoch 38,	batch   900,	training loss: 0.021
epoch 38,	batch   910,	training loss: 0.044
epoch 38,	batch   920,	training loss: 0.028
epoch 38,	batch   930,	training loss: 0.022
epoch 38,	batch   940,	training loss: 0.012
epoch 38,	batch   950,	training loss: 0.040
epoch 38,	batch   960,	training loss: 0.029
epoch 38,	batch   970,	training loss: 0.041
epoch 38,	batch   980,	training loss: 0.035
epoch 38,	batch   990,	training loss: 0.028
epoch 38,	batch  1000,	training loss: 0.028
epoch 38,	batch  1010,	training loss: 0.044
epoch 38,	batch  1020,	training loss: 0.028
epoch 38,	batch  1030,	training loss: 0.034
epoch 38,	batch  1040,	training loss: 0.043
epoch 38,	batch  1050,	training loss: 0.041
epoch 38,	batch  1060,	training loss: 0.031
epoch 38,	batch  1070,	training loss: 0.023
epoch 38,	batch  1080,	training loss: 0.044
epoch 38,	batch  1090,	training loss: 0.039
epoch 38,	batch  1100,	training loss: 0.027
epoch 38,	batch  1110,	training loss: 0.027
epoch 38,	batch  1120,	training loss: 0.038
epoch 38,	batch  1130,	training loss: 0.048
epoch 38,	batch  1140,	training loss: 0.038
epoch 38,	batch  1150,	training loss: 0.054
epoch 38,	batch  1160,	training loss: 0.132
epoch 38,	batch  1170,	training loss: 0.051
epoch 38,	batch  1180,	training loss: 0.054
epoch 38,	batch  1190,	training loss: 0.023
epoch 38,	batch  1200,	training loss: 0.021
epoch 38,	batch  1210,	training loss: 0.031
epoch 38,	batch  1220,	training loss: 0.016
epoch 38,	batch  1230,	training loss: 0.036
epoch 38,	batch  1240,	training loss: 0.023
epoch 38,	batch  1250,	training loss: 0.012
epoch 38,	batch  1260,	training loss: 0.051
epoch 38,	batch  1270,	training loss: 0.034
epoch 38,	batch  1280,	training loss: 0.038
epoch 38,	batch  1290,	training loss: 0.080
epoch 38,	batch  1300,	training loss: 0.041
epoch 38,	batch  1310,	training loss: 0.029
epoch 38,	batch  1320,	training loss: 0.099
epoch 38,	batch  1330,	training loss: 0.053
epoch 38,	batch  1340,	training loss: 0.028
epoch 38,	batch  1350,	training loss: 0.054
epoch 38,	batch  1360,	training loss: 0.021
epoch 38,	batch  1370,	training loss: 0.024
epoch 38,	batch  1380,	training loss: 0.037
epoch 38,	batch  1390,	training loss: 0.012
epoch 38,	batch  1400,	training loss: 0.055
epoch 38,	batch  1410,	training loss: 0.047
epoch 38,	batch  1420,	training loss: 0.093
epoch 38,	batch  1430,	training loss: 0.034
epoch 38,	batch  1440,	training loss: 0.042
epoch 38,	batch  1450,	training loss: 0.033
epoch 38,	batch  1460,	training loss: 0.023
epoch 38,	batch  1470,	training loss: 0.051
epoch 38,	batch  1480,	training loss: 0.062
epoch 38,	batch  1490,	training loss: 0.041
epoch 38,	batch  1500,	training loss: 0.030
epoch 38,	batch  1510,	training loss: 0.075
epoch 38,	batch  1520,	training loss: 0.025
epoch 38,	batch  1530,	training loss: 0.032
epoch 38,	batch  1540,	training loss: 0.021
epoch 38,	batch  1550,	training loss: 0.038
epoch 38,	batch  1560,	training loss: 0.038
epoch 38,	batch  1570,	training loss: 0.023
epoch 38,	batch  1580,	training loss: 0.038
epoch 38,	batch  1590,	training loss: 0.040
epoch 38,	batch  1600,	training loss: 0.032
epoch 38,	batch  1610,	training loss: 0.025
epoch 38,	batch  1620,	training loss: 0.040
epoch 38,	batch  1630,	training loss: 0.032
epoch 38,	batch  1640,	training loss: 0.061
epoch 38,	batch  1650,	training loss: 0.019
epoch 38,	batch  1660,	training loss: 0.044
epoch 38,	batch  1670,	training loss: 0.026
epoch 38,	batch  1680,	training loss: 0.037
epoch 38,	batch  1690,	training loss: 0.023
epoch 38,	batch  1700,	training loss: 0.067
epoch 38,	batch  1710,	training loss: 0.048
epoch 38,	batch  1720,	training loss: 0.031
epoch 38,	batch  1730,	training loss: 0.055
epoch 38,	batch  1740,	training loss: 0.041
epoch 38,	batch  1750,	training loss: 0.023
epoch 38,	batch  1760,	training loss: 0.067
epoch 38,	batch  1770,	training loss: 0.019
epoch 38,	batch  1780,	training loss: 0.030
epoch 38,	batch  1790,	training loss: 0.037
epoch 38,	batch  1800,	training loss: 0.038
epoch 38,	batch  1810,	training loss: 0.039
epoch 38,	batch  1820,	training loss: 0.051
epoch 38,	batch  1830,	training loss: 0.066
epoch 38,	batch  1840,	training loss: 0.088
epoch 38,	batch  1850,	training loss: 0.059
epoch 38,	batch  1860,	training loss: 0.057
epoch 38,	batch  1870,	training loss: 0.035
epoch 38,	batch  1880,	training loss: 0.061
epoch 38,	batch  1890,	training loss: 0.034
epoch 38,	batch  1900,	training loss: 0.065
epoch 38,	batch  1910,	training loss: 0.052
epoch 38,	batch  1920,	training loss: 0.056
epoch 38,	batch  1930,	training loss: 0.037
epoch 38,	batch  1940,	training loss: 0.116
epoch 38,	batch  1950,	training loss: 0.064
epoch 38,	batch  1960,	training loss: 0.030
epoch 38,	batch  1970,	training loss: 0.065
epoch 38,	batch  1980,	training loss: 0.042
END OF EPOCH 38
Testing on validation set...
# correct:	6121/54904 = 11.14855019670698%
# off by 1:	11074/54904 = 20.169750837826022%

epoch 39,	batch    10,	training loss: 0.016
epoch 39,	batch    20,	training loss: 0.037
epoch 39,	batch    30,	training loss: 0.027
epoch 39,	batch    40,	training loss: 0.021
epoch 39,	batch    50,	training loss: 0.048
epoch 39,	batch    60,	training loss: 0.058
epoch 39,	batch    70,	training loss: 0.053
epoch 39,	batch    80,	training loss: 0.030
epoch 39,	batch    90,	training loss: 0.038
epoch 39,	batch   100,	training loss: 0.040
epoch 39,	batch   110,	training loss: 0.074
epoch 39,	batch   120,	training loss: 0.041
epoch 39,	batch   130,	training loss: 0.072
epoch 39,	batch   140,	training loss: 0.069
epoch 39,	batch   150,	training loss: 0.047
epoch 39,	batch   160,	training loss: 0.055
epoch 39,	batch   170,	training loss: 0.039
epoch 39,	batch   180,	training loss: 0.083
epoch 39,	batch   190,	training loss: 0.037
epoch 39,	batch   200,	training loss: 0.024
epoch 39,	batch   210,	training loss: 0.019
epoch 39,	batch   220,	training loss: 0.025
epoch 39,	batch   230,	training loss: 0.044
epoch 39,	batch   240,	training loss: 0.015
epoch 39,	batch   250,	training loss: 0.028
epoch 39,	batch   260,	training loss: 0.023
epoch 39,	batch   270,	training loss: 0.045
epoch 39,	batch   280,	training loss: 0.015
epoch 39,	batch   290,	training loss: 0.024
epoch 39,	batch   300,	training loss: 0.018
epoch 39,	batch   310,	training loss: 0.022
epoch 39,	batch   320,	training loss: 0.019
epoch 39,	batch   330,	training loss: 0.054
epoch 39,	batch   340,	training loss: 0.014
epoch 39,	batch   350,	training loss: 0.016
epoch 39,	batch   360,	training loss: 0.025
epoch 39,	batch   370,	training loss: 0.061
epoch 39,	batch   380,	training loss: 0.026
epoch 39,	batch   390,	training loss: 0.017
epoch 39,	batch   400,	training loss: 0.044
epoch 39,	batch   410,	training loss: 0.019
epoch 39,	batch   420,	training loss: 0.071
epoch 39,	batch   430,	training loss: 0.022
epoch 39,	batch   440,	training loss: 0.017
epoch 39,	batch   450,	training loss: 0.028
epoch 39,	batch   460,	training loss: 0.030
epoch 39,	batch   470,	training loss: 0.031
epoch 39,	batch   480,	training loss: 0.026
epoch 39,	batch   490,	training loss: 0.049
epoch 39,	batch   500,	training loss: 0.019
epoch 39,	batch   510,	training loss: 0.036
epoch 39,	batch   520,	training loss: 0.022
epoch 39,	batch   530,	training loss: 0.024
epoch 39,	batch   540,	training loss: 0.090
epoch 39,	batch   550,	training loss: 0.051
epoch 39,	batch   560,	training loss: 0.025
epoch 39,	batch   570,	training loss: 0.016
epoch 39,	batch   580,	training loss: 0.024
epoch 39,	batch   590,	training loss: 0.031
epoch 39,	batch   600,	training loss: 0.054
epoch 39,	batch   610,	training loss: 0.032
epoch 39,	batch   620,	training loss: 0.021
epoch 39,	batch   630,	training loss: 0.032
epoch 39,	batch   640,	training loss: 0.021
epoch 39,	batch   650,	training loss: 0.034
epoch 39,	batch   660,	training loss: 0.022
epoch 39,	batch   670,	training loss: 0.021
epoch 39,	batch   680,	training loss: 0.049
epoch 39,	batch   690,	training loss: 0.023
epoch 39,	batch   700,	training loss: 0.049
epoch 39,	batch   710,	training loss: 0.015
epoch 39,	batch   720,	training loss: 0.027
epoch 39,	batch   730,	training loss: 0.016
epoch 39,	batch   740,	training loss: 0.018
epoch 39,	batch   750,	training loss: 0.023
epoch 39,	batch   760,	training loss: 0.026
epoch 39,	batch   770,	training loss: 0.015
epoch 39,	batch   780,	training loss: 0.038
epoch 39,	batch   790,	training loss: 0.031
epoch 39,	batch   800,	training loss: 0.048
epoch 39,	batch   810,	training loss: 0.026
epoch 39,	batch   820,	training loss: 0.017
epoch 39,	batch   830,	training loss: 0.029
epoch 39,	batch   840,	training loss: 0.049
epoch 39,	batch   850,	training loss: 0.025
epoch 39,	batch   860,	training loss: 0.058
epoch 39,	batch   870,	training loss: 0.022
epoch 39,	batch   880,	training loss: 0.031
epoch 39,	batch   890,	training loss: 0.018
epoch 39,	batch   900,	training loss: 0.019
epoch 39,	batch   910,	training loss: 0.032
epoch 39,	batch   920,	training loss: 0.041
epoch 39,	batch   930,	training loss: 0.037
epoch 39,	batch   940,	training loss: 0.037
epoch 39,	batch   950,	training loss: 0.060
epoch 39,	batch   960,	training loss: 0.045
epoch 39,	batch   970,	training loss: 0.042
epoch 39,	batch   980,	training loss: 0.026
epoch 39,	batch   990,	training loss: 0.026
epoch 39,	batch  1000,	training loss: 0.050
epoch 39,	batch  1010,	training loss: 0.065
epoch 39,	batch  1020,	training loss: 0.026
epoch 39,	batch  1030,	training loss: 0.034
epoch 39,	batch  1040,	training loss: 0.082
epoch 39,	batch  1050,	training loss: 0.033
epoch 39,	batch  1060,	training loss: 0.032
epoch 39,	batch  1070,	training loss: 0.034
epoch 39,	batch  1080,	training loss: 0.069
epoch 39,	batch  1090,	training loss: 0.036
epoch 39,	batch  1100,	training loss: 0.036
epoch 39,	batch  1110,	training loss: 0.021
epoch 39,	batch  1120,	training loss: 0.074
epoch 39,	batch  1130,	training loss: 0.034
epoch 39,	batch  1140,	training loss: 0.038
epoch 39,	batch  1150,	training loss: 0.026
epoch 39,	batch  1160,	training loss: 0.061
epoch 39,	batch  1170,	training loss: 0.038
epoch 39,	batch  1180,	training loss: 0.061
epoch 39,	batch  1190,	training loss: 0.097
epoch 39,	batch  1200,	training loss: 0.038
epoch 39,	batch  1210,	training loss: 0.052
epoch 39,	batch  1220,	training loss: 0.028
epoch 39,	batch  1230,	training loss: 0.024
epoch 39,	batch  1240,	training loss: 0.043
epoch 39,	batch  1250,	training loss: 0.027
epoch 39,	batch  1260,	training loss: 0.045
epoch 39,	batch  1270,	training loss: 0.028
epoch 39,	batch  1280,	training loss: 0.032
epoch 39,	batch  1290,	training loss: 0.035
epoch 39,	batch  1300,	training loss: 0.024
epoch 39,	batch  1310,	training loss: 0.033
epoch 39,	batch  1320,	training loss: 0.042
epoch 39,	batch  1330,	training loss: 0.036
epoch 39,	batch  1340,	training loss: 0.047
epoch 39,	batch  1350,	training loss: 0.041
epoch 39,	batch  1360,	training loss: 0.038
epoch 39,	batch  1370,	training loss: 0.013
epoch 39,	batch  1380,	training loss: 0.020
epoch 39,	batch  1390,	training loss: 0.089
epoch 39,	batch  1400,	training loss: 0.031
epoch 39,	batch  1410,	training loss: 0.025
epoch 39,	batch  1420,	training loss: 0.050
epoch 39,	batch  1430,	training loss: 0.033
epoch 39,	batch  1440,	training loss: 0.021
epoch 39,	batch  1450,	training loss: 0.038
epoch 39,	batch  1460,	training loss: 0.042
epoch 39,	batch  1470,	training loss: 0.026
epoch 39,	batch  1480,	training loss: 0.030
epoch 39,	batch  1490,	training loss: 0.028
epoch 39,	batch  1500,	training loss: 0.048
epoch 39,	batch  1510,	training loss: 0.020
epoch 39,	batch  1520,	training loss: 0.033
epoch 39,	batch  1530,	training loss: 0.044
epoch 39,	batch  1540,	training loss: 0.024
epoch 39,	batch  1550,	training loss: 0.030
epoch 39,	batch  1560,	training loss: 0.035
epoch 39,	batch  1570,	training loss: 0.020
epoch 39,	batch  1580,	training loss: 0.029
epoch 39,	batch  1590,	training loss: 0.041
epoch 39,	batch  1600,	training loss: 0.023
epoch 39,	batch  1610,	training loss: 0.027
epoch 39,	batch  1620,	training loss: 0.061
epoch 39,	batch  1630,	training loss: 0.028
epoch 39,	batch  1640,	training loss: 0.039
epoch 39,	batch  1650,	training loss: 0.036
epoch 39,	batch  1660,	training loss: 0.079
epoch 39,	batch  1670,	training loss: 0.024
epoch 39,	batch  1680,	training loss: 0.053
epoch 39,	batch  1690,	training loss: 0.036
epoch 39,	batch  1700,	training loss: 0.051
epoch 39,	batch  1710,	training loss: 0.029
epoch 39,	batch  1720,	training loss: 0.046
epoch 39,	batch  1730,	training loss: 0.056
epoch 39,	batch  1740,	training loss: 0.051
epoch 39,	batch  1750,	training loss: 0.027
epoch 39,	batch  1760,	training loss: 0.072
epoch 39,	batch  1770,	training loss: 0.071
epoch 39,	batch  1780,	training loss: 0.035
epoch 39,	batch  1790,	training loss: 0.086
epoch 39,	batch  1800,	training loss: 0.046
epoch 39,	batch  1810,	training loss: 0.048
epoch 39,	batch  1820,	training loss: 0.054
epoch 39,	batch  1830,	training loss: 0.052
epoch 39,	batch  1840,	training loss: 0.051
epoch 39,	batch  1850,	training loss: 0.044
epoch 39,	batch  1860,	training loss: 0.053
epoch 39,	batch  1870,	training loss: 0.070
epoch 39,	batch  1880,	training loss: 0.045
epoch 39,	batch  1890,	training loss: 0.049
epoch 39,	batch  1900,	training loss: 0.050
epoch 39,	batch  1910,	training loss: 0.051
epoch 39,	batch  1920,	training loss: 0.045
epoch 39,	batch  1930,	training loss: 0.076
epoch 39,	batch  1940,	training loss: 0.049
epoch 39,	batch  1950,	training loss: 0.070
epoch 39,	batch  1960,	training loss: 0.071
epoch 39,	batch  1970,	training loss: 0.024
epoch 39,	batch  1980,	training loss: 0.064
END OF EPOCH 39
Testing on validation set...
# correct:	5828/54904 = 10.614891446889116%
# off by 1:	10655/54904 = 19.406600611977268%

epoch 40,	batch    10,	training loss: 0.030
epoch 40,	batch    20,	training loss: 0.039
epoch 40,	batch    30,	training loss: 0.045
epoch 40,	batch    40,	training loss: 0.035
epoch 40,	batch    50,	training loss: 0.031
epoch 40,	batch    60,	training loss: 0.034
epoch 40,	batch    70,	training loss: 0.035
epoch 40,	batch    80,	training loss: 0.025
epoch 40,	batch    90,	training loss: 0.057
epoch 40,	batch   100,	training loss: 0.039
epoch 40,	batch   110,	training loss: 0.024
epoch 40,	batch   120,	training loss: 0.048
epoch 40,	batch   130,	training loss: 0.032
epoch 40,	batch   140,	training loss: 0.039
epoch 40,	batch   150,	training loss: 0.064
epoch 40,	batch   160,	training loss: 0.041
epoch 40,	batch   170,	training loss: 0.055
epoch 40,	batch   180,	training loss: 0.039
epoch 40,	batch   190,	training loss: 0.056
epoch 40,	batch   200,	training loss: 0.048
epoch 40,	batch   210,	training loss: 0.044
epoch 40,	batch   220,	training loss: 0.039
epoch 40,	batch   230,	training loss: 0.033
epoch 40,	batch   240,	training loss: 0.044
epoch 40,	batch   250,	training loss: 0.048
epoch 40,	batch   260,	training loss: 0.027
epoch 40,	batch   270,	training loss: 0.055
epoch 40,	batch   280,	training loss: 0.020
epoch 40,	batch   290,	training loss: 0.018
epoch 40,	batch   300,	training loss: 0.022
epoch 40,	batch   310,	training loss: 0.035
epoch 40,	batch   320,	training loss: 0.032
epoch 40,	batch   330,	training loss: 0.041
epoch 40,	batch   340,	training loss: 0.055
epoch 40,	batch   350,	training loss: 0.035
epoch 40,	batch   360,	training loss: 0.021
epoch 40,	batch   370,	training loss: 0.029
epoch 40,	batch   380,	training loss: 0.052
epoch 40,	batch   390,	training loss: 0.027
epoch 40,	batch   400,	training loss: 0.026
epoch 40,	batch   410,	training loss: 0.039
epoch 40,	batch   420,	training loss: 0.025
epoch 40,	batch   430,	training loss: 0.022
epoch 40,	batch   440,	training loss: 0.027
epoch 40,	batch   450,	training loss: 0.026
epoch 40,	batch   460,	training loss: 0.023
epoch 40,	batch   470,	training loss: 0.027
epoch 40,	batch   480,	training loss: 0.036
epoch 40,	batch   490,	training loss: 0.045
epoch 40,	batch   500,	training loss: 0.023
epoch 40,	batch   510,	training loss: 0.031
epoch 40,	batch   520,	training loss: 0.023
epoch 40,	batch   530,	training loss: 0.041
epoch 40,	batch   540,	training loss: 0.027
epoch 40,	batch   550,	training loss: 0.017
epoch 40,	batch   560,	training loss: 0.065
epoch 40,	batch   570,	training loss: 0.117
epoch 40,	batch   580,	training loss: 0.099
epoch 40,	batch   590,	training loss: 0.039
epoch 40,	batch   600,	training loss: 0.042
epoch 40,	batch   610,	training loss: 0.024
epoch 40,	batch   620,	training loss: 0.046
epoch 40,	batch   630,	training loss: 0.040
epoch 40,	batch   640,	training loss: 0.038
epoch 40,	batch   650,	training loss: 0.031
epoch 40,	batch   660,	training loss: 0.021
epoch 40,	batch   670,	training loss: 0.033
epoch 40,	batch   680,	training loss: 0.035
epoch 40,	batch   690,	training loss: 0.028
epoch 40,	batch   700,	training loss: 0.046
epoch 40,	batch   710,	training loss: 0.037
epoch 40,	batch   720,	training loss: 0.051
epoch 40,	batch   730,	training loss: 0.043
epoch 40,	batch   740,	training loss: 0.057
epoch 40,	batch   750,	training loss: 0.022
epoch 40,	batch   760,	training loss: 0.052
epoch 40,	batch   770,	training loss: 0.043
epoch 40,	batch   780,	training loss: 0.041
epoch 40,	batch   790,	training loss: 0.042
epoch 40,	batch   800,	training loss: 0.022
epoch 40,	batch   810,	training loss: 0.044
epoch 40,	batch   820,	training loss: 0.051
epoch 40,	batch   830,	training loss: 0.038
epoch 40,	batch   840,	training loss: 0.039
epoch 40,	batch   850,	training loss: 0.040
epoch 40,	batch   860,	training loss: 0.062
epoch 40,	batch   870,	training loss: 0.035
epoch 40,	batch   880,	training loss: 0.060
epoch 40,	batch   890,	training loss: 0.025
epoch 40,	batch   900,	training loss: 0.058
epoch 40,	batch   910,	training loss: 0.025
epoch 40,	batch   920,	training loss: 0.071
epoch 40,	batch   930,	training loss: 0.056
epoch 40,	batch   940,	training loss: 0.022
epoch 40,	batch   950,	training loss: 0.027
epoch 40,	batch   960,	training loss: 0.051
epoch 40,	batch   970,	training loss: 0.036
epoch 40,	batch   980,	training loss: 0.037
epoch 40,	batch   990,	training loss: 0.024
epoch 40,	batch  1000,	training loss: 0.050
epoch 40,	batch  1010,	training loss: 0.029
epoch 40,	batch  1020,	training loss: 0.039
epoch 40,	batch  1030,	training loss: 0.016
epoch 40,	batch  1040,	training loss: 0.036
epoch 40,	batch  1050,	training loss: 0.031
epoch 40,	batch  1060,	training loss: 0.018
epoch 40,	batch  1070,	training loss: 0.034
epoch 40,	batch  1080,	training loss: 0.029
epoch 40,	batch  1090,	training loss: 0.057
epoch 40,	batch  1100,	training loss: 0.059
epoch 40,	batch  1110,	training loss: 0.054
epoch 40,	batch  1120,	training loss: 0.101
epoch 40,	batch  1130,	training loss: 0.118
epoch 40,	batch  1140,	training loss: 0.065
epoch 40,	batch  1150,	training loss: 0.024
epoch 40,	batch  1160,	training loss: 0.069
epoch 40,	batch  1170,	training loss: 0.044
epoch 40,	batch  1180,	training loss: 0.022
epoch 40,	batch  1190,	training loss: 0.042
epoch 40,	batch  1200,	training loss: 0.074
epoch 40,	batch  1210,	training loss: 0.023
epoch 40,	batch  1220,	training loss: 0.036
epoch 40,	batch  1230,	training loss: 0.033
epoch 40,	batch  1240,	training loss: 0.066
epoch 40,	batch  1250,	training loss: 0.055
epoch 40,	batch  1260,	training loss: 0.034
epoch 40,	batch  1270,	training loss: 0.047
epoch 40,	batch  1280,	training loss: 0.038
epoch 40,	batch  1290,	training loss: 0.029
epoch 40,	batch  1300,	training loss: 0.084
epoch 40,	batch  1310,	training loss: 0.069
epoch 40,	batch  1320,	training loss: 0.052
epoch 40,	batch  1330,	training loss: 0.082
epoch 40,	batch  1340,	training loss: 0.055
epoch 40,	batch  1350,	training loss: 0.107
epoch 40,	batch  1360,	training loss: 0.046
epoch 40,	batch  1370,	training loss: 0.043
epoch 40,	batch  1380,	training loss: 0.035
epoch 40,	batch  1390,	training loss: 0.045
epoch 40,	batch  1400,	training loss: 0.042
epoch 40,	batch  1410,	training loss: 0.085
epoch 40,	batch  1420,	training loss: 0.074
epoch 40,	batch  1430,	training loss: 0.045
epoch 40,	batch  1440,	training loss: 0.048
epoch 40,	batch  1450,	training loss: 0.089
epoch 40,	batch  1460,	training loss: 0.033
epoch 40,	batch  1470,	training loss: 0.021
epoch 40,	batch  1480,	training loss: 0.113
epoch 40,	batch  1490,	training loss: 0.065
epoch 40,	batch  1500,	training loss: 0.063
epoch 40,	batch  1510,	training loss: 0.052
epoch 40,	batch  1520,	training loss: 0.058
epoch 40,	batch  1530,	training loss: 0.045
epoch 40,	batch  1540,	training loss: 0.038
epoch 40,	batch  1550,	training loss: 0.052
epoch 40,	batch  1560,	training loss: 0.017
epoch 40,	batch  1570,	training loss: 0.062
epoch 40,	batch  1580,	training loss: 0.072
epoch 40,	batch  1590,	training loss: 0.043
epoch 40,	batch  1600,	training loss: 0.046
epoch 40,	batch  1610,	training loss: 0.058
epoch 40,	batch  1620,	training loss: 0.034
epoch 40,	batch  1630,	training loss: 0.056
epoch 40,	batch  1640,	training loss: 0.043
epoch 40,	batch  1650,	training loss: 0.065
epoch 40,	batch  1660,	training loss: 0.085
epoch 40,	batch  1670,	training loss: 0.033
epoch 40,	batch  1680,	training loss: 0.061
epoch 40,	batch  1690,	training loss: 0.104
epoch 40,	batch  1700,	training loss: 0.061
epoch 40,	batch  1710,	training loss: 0.048
epoch 40,	batch  1720,	training loss: 0.053
epoch 40,	batch  1730,	training loss: 0.034
epoch 40,	batch  1740,	training loss: 0.034
epoch 40,	batch  1750,	training loss: 0.092
epoch 40,	batch  1760,	training loss: 0.031
epoch 40,	batch  1770,	training loss: 0.069
epoch 40,	batch  1780,	training loss: 0.048
epoch 40,	batch  1790,	training loss: 0.021
epoch 40,	batch  1800,	training loss: 0.130
epoch 40,	batch  1810,	training loss: 0.056
epoch 40,	batch  1820,	training loss: 0.083
epoch 40,	batch  1830,	training loss: 0.066
epoch 40,	batch  1840,	training loss: 0.048
epoch 40,	batch  1850,	training loss: 0.035
epoch 40,	batch  1860,	training loss: 0.048
epoch 40,	batch  1870,	training loss: 0.036
epoch 40,	batch  1880,	training loss: 0.122
epoch 40,	batch  1890,	training loss: 0.030
epoch 40,	batch  1900,	training loss: 0.054
epoch 40,	batch  1910,	training loss: 0.042
epoch 40,	batch  1920,	training loss: 0.064
epoch 40,	batch  1930,	training loss: 0.048
epoch 40,	batch  1940,	training loss: 0.054
epoch 40,	batch  1950,	training loss: 0.116
epoch 40,	batch  1960,	training loss: 0.042
epoch 40,	batch  1970,	training loss: 0.089
epoch 40,	batch  1980,	training loss: 0.046
END OF EPOCH 40
Testing on validation set...
# correct:	6516/54904 = 11.867987760454612%
# off by 1:	11607/54904 = 21.140536208655107%

epoch 41,	batch    10,	training loss: 0.031
epoch 41,	batch    20,	training loss: 0.045
epoch 41,	batch    30,	training loss: 0.045
epoch 41,	batch    40,	training loss: 0.067
epoch 41,	batch    50,	training loss: 0.029
epoch 41,	batch    60,	training loss: 0.033
epoch 41,	batch    70,	training loss: 0.067
epoch 41,	batch    80,	training loss: 0.023
epoch 41,	batch    90,	training loss: 0.022
epoch 41,	batch   100,	training loss: 0.055
epoch 41,	batch   110,	training loss: 0.086
epoch 41,	batch   120,	training loss: 0.059
epoch 41,	batch   130,	training loss: 0.046
epoch 41,	batch   140,	training loss: 0.054
epoch 41,	batch   150,	training loss: 0.051
epoch 41,	batch   160,	training loss: 0.058
epoch 41,	batch   170,	training loss: 0.058
epoch 41,	batch   180,	training loss: 0.049
epoch 41,	batch   190,	training loss: 0.023
epoch 41,	batch   200,	training loss: 0.029
epoch 41,	batch   210,	training loss: 0.046
epoch 41,	batch   220,	training loss: 0.033
epoch 41,	batch   230,	training loss: 0.021
epoch 41,	batch   240,	training loss: 0.030
epoch 41,	batch   250,	training loss: 0.038
epoch 41,	batch   260,	training loss: 0.044
epoch 41,	batch   270,	training loss: 0.020
epoch 41,	batch   280,	training loss: 0.028
epoch 41,	batch   290,	training loss: 0.038
epoch 41,	batch   300,	training loss: 0.044
epoch 41,	batch   310,	training loss: 0.040
epoch 41,	batch   320,	training loss: 0.024
epoch 41,	batch   330,	training loss: 0.033
epoch 41,	batch   340,	training loss: 0.029
epoch 41,	batch   350,	training loss: 0.024
epoch 41,	batch   360,	training loss: 0.044
epoch 41,	batch   370,	training loss: 0.022
epoch 41,	batch   380,	training loss: 0.032
epoch 41,	batch   390,	training loss: 0.035
epoch 41,	batch   400,	training loss: 0.039
epoch 41,	batch   410,	training loss: 0.040
epoch 41,	batch   420,	training loss: 0.078
epoch 41,	batch   430,	training loss: 0.028
epoch 41,	batch   440,	training loss: 0.040
epoch 41,	batch   450,	training loss: 0.064
epoch 41,	batch   460,	training loss: 0.045
epoch 41,	batch   470,	training loss: 0.028
epoch 41,	batch   480,	training loss: 0.068
epoch 41,	batch   490,	training loss: 0.030
epoch 41,	batch   500,	training loss: 0.027
epoch 41,	batch   510,	training loss: 0.027
epoch 41,	batch   520,	training loss: 0.030
epoch 41,	batch   530,	training loss: 0.042
epoch 41,	batch   540,	training loss: 0.012
epoch 41,	batch   550,	training loss: 0.036
epoch 41,	batch   560,	training loss: 0.023
epoch 41,	batch   570,	training loss: 0.061
epoch 41,	batch   580,	training loss: 0.031
epoch 41,	batch   590,	training loss: 0.038
epoch 41,	batch   600,	training loss: 0.057
epoch 41,	batch   610,	training loss: 0.031
epoch 41,	batch   620,	training loss: 0.035
epoch 41,	batch   630,	training loss: 0.026
epoch 41,	batch   640,	training loss: 0.052
epoch 41,	batch   650,	training loss: 0.034
epoch 41,	batch   660,	training loss: 0.040
epoch 41,	batch   670,	training loss: 0.020
epoch 41,	batch   680,	training loss: 0.025
epoch 41,	batch   690,	training loss: 0.024
epoch 41,	batch   700,	training loss: 0.019
epoch 41,	batch   710,	training loss: 0.052
epoch 41,	batch   720,	training loss: 0.024
epoch 41,	batch   730,	training loss: 0.023
epoch 41,	batch   740,	training loss: 0.026
epoch 41,	batch   750,	training loss: 0.014
epoch 41,	batch   760,	training loss: 0.029
epoch 41,	batch   770,	training loss: 0.058
epoch 41,	batch   780,	training loss: 0.042
epoch 41,	batch   790,	training loss: 0.026
epoch 41,	batch   800,	training loss: 0.057
epoch 41,	batch   810,	training loss: 0.022
epoch 41,	batch   820,	training loss: 0.073
epoch 41,	batch   830,	training loss: 0.061
epoch 41,	batch   840,	training loss: 0.049
epoch 41,	batch   850,	training loss: 0.023
epoch 41,	batch   860,	training loss: 0.030
epoch 41,	batch   870,	training loss: 0.023
epoch 41,	batch   880,	training loss: 0.018
epoch 41,	batch   890,	training loss: 0.051
epoch 41,	batch   900,	training loss: 0.037
epoch 41,	batch   910,	training loss: 0.073
epoch 41,	batch   920,	training loss: 0.063
epoch 41,	batch   930,	training loss: 0.074
epoch 41,	batch   940,	training loss: 0.047
epoch 41,	batch   950,	training loss: 0.052
epoch 41,	batch   960,	training loss: 0.027
epoch 41,	batch   970,	training loss: 0.035
epoch 41,	batch   980,	training loss: 0.031
epoch 41,	batch   990,	training loss: 0.029
epoch 41,	batch  1000,	training loss: 0.019
epoch 41,	batch  1010,	training loss: 0.028
epoch 41,	batch  1020,	training loss: 0.032
epoch 41,	batch  1030,	training loss: 0.036
epoch 41,	batch  1040,	training loss: 0.051
epoch 41,	batch  1050,	training loss: 0.053
epoch 41,	batch  1060,	training loss: 0.041
epoch 41,	batch  1070,	training loss: 0.034
epoch 41,	batch  1080,	training loss: 0.022
epoch 41,	batch  1090,	training loss: 0.038
epoch 41,	batch  1100,	training loss: 0.057
epoch 41,	batch  1110,	training loss: 0.052
epoch 41,	batch  1120,	training loss: 0.064
epoch 41,	batch  1130,	training loss: 0.052
epoch 41,	batch  1140,	training loss: 0.023
epoch 41,	batch  1150,	training loss: 0.047
epoch 41,	batch  1160,	training loss: 0.059
epoch 41,	batch  1170,	training loss: 0.045
epoch 41,	batch  1180,	training loss: 0.053
epoch 41,	batch  1190,	training loss: 0.026
epoch 41,	batch  1200,	training loss: 0.025
epoch 41,	batch  1210,	training loss: 0.059
epoch 41,	batch  1220,	training loss: 0.040
epoch 41,	batch  1230,	training loss: 0.042
epoch 41,	batch  1240,	training loss: 0.038
epoch 41,	batch  1250,	training loss: 0.033
epoch 41,	batch  1260,	training loss: 0.061
epoch 41,	batch  1270,	training loss: 0.074
epoch 41,	batch  1280,	training loss: 0.052
epoch 41,	batch  1290,	training loss: 0.061
epoch 41,	batch  1300,	training loss: 0.074
epoch 41,	batch  1310,	training loss: 0.029
epoch 41,	batch  1320,	training loss: 0.037
epoch 41,	batch  1330,	training loss: 0.070
epoch 41,	batch  1340,	training loss: 0.055
epoch 41,	batch  1350,	training loss: 0.034
epoch 41,	batch  1360,	training loss: 0.033
epoch 41,	batch  1370,	training loss: 0.041
epoch 41,	batch  1380,	training loss: 0.077
epoch 41,	batch  1390,	training loss: 0.041
epoch 41,	batch  1400,	training loss: 0.068
epoch 41,	batch  1410,	training loss: 0.035
epoch 41,	batch  1420,	training loss: 0.064
epoch 41,	batch  1430,	training loss: 0.064
epoch 41,	batch  1440,	training loss: 0.054
epoch 41,	batch  1450,	training loss: 0.041
epoch 41,	batch  1460,	training loss: 0.057
epoch 41,	batch  1470,	training loss: 0.104
epoch 41,	batch  1480,	training loss: 0.063
epoch 41,	batch  1490,	training loss: 0.068
epoch 41,	batch  1500,	training loss: 0.035
epoch 41,	batch  1510,	training loss: 0.054
epoch 41,	batch  1520,	training loss: 0.064
epoch 41,	batch  1530,	training loss: 0.053
epoch 41,	batch  1540,	training loss: 0.052
epoch 41,	batch  1550,	training loss: 0.062
epoch 41,	batch  1560,	training loss: 0.037
epoch 41,	batch  1570,	training loss: 0.033
epoch 41,	batch  1580,	training loss: 0.029
epoch 41,	batch  1590,	training loss: 0.086
epoch 41,	batch  1600,	training loss: 0.021
epoch 41,	batch  1610,	training loss: 0.024
epoch 41,	batch  1620,	training loss: 0.060
epoch 41,	batch  1630,	training loss: 0.052
epoch 41,	batch  1640,	training loss: 0.071
epoch 41,	batch  1650,	training loss: 0.109
epoch 41,	batch  1660,	training loss: 0.054
epoch 41,	batch  1670,	training loss: 0.065
epoch 41,	batch  1680,	training loss: 0.067
epoch 41,	batch  1690,	training loss: 0.038
epoch 41,	batch  1700,	training loss: 0.030
epoch 41,	batch  1710,	training loss: 0.037
epoch 41,	batch  1720,	training loss: 0.036
epoch 41,	batch  1730,	training loss: 0.031
epoch 41,	batch  1740,	training loss: 0.031
epoch 41,	batch  1750,	training loss: 0.048
epoch 41,	batch  1760,	training loss: 0.037
epoch 41,	batch  1770,	training loss: 0.053
epoch 41,	batch  1780,	training loss: 0.033
epoch 41,	batch  1790,	training loss: 0.076
epoch 41,	batch  1800,	training loss: 0.059
epoch 41,	batch  1810,	training loss: 0.042
epoch 41,	batch  1820,	training loss: 0.051
epoch 41,	batch  1830,	training loss: 0.048
epoch 41,	batch  1840,	training loss: 0.063
epoch 41,	batch  1850,	training loss: 0.059
epoch 41,	batch  1860,	training loss: 0.052
epoch 41,	batch  1870,	training loss: 0.082
epoch 41,	batch  1880,	training loss: 0.067
epoch 41,	batch  1890,	training loss: 0.045
epoch 41,	batch  1900,	training loss: 0.051
epoch 41,	batch  1910,	training loss: 0.061
epoch 41,	batch  1920,	training loss: 0.058
epoch 41,	batch  1930,	training loss: 0.053
epoch 41,	batch  1940,	training loss: 0.058
epoch 41,	batch  1950,	training loss: 0.104
epoch 41,	batch  1960,	training loss: 0.083
epoch 41,	batch  1970,	training loss: 0.049
epoch 41,	batch  1980,	training loss: 0.047
END OF EPOCH 41
Testing on validation set...
# correct:	6369/54904 = 11.60024770508524%
# off by 1:	11502/54904 = 20.9492933119627%

epoch 42,	batch    10,	training loss: 0.051
epoch 42,	batch    20,	training loss: 0.048
epoch 42,	batch    30,	training loss: 0.044
epoch 42,	batch    40,	training loss: 0.032
epoch 42,	batch    50,	training loss: 0.048
epoch 42,	batch    60,	training loss: 0.097
epoch 42,	batch    70,	training loss: 0.061
epoch 42,	batch    80,	training loss: 0.079
epoch 42,	batch    90,	training loss: 0.062
epoch 42,	batch   100,	training loss: 0.043
epoch 42,	batch   110,	training loss: 0.032
epoch 42,	batch   120,	training loss: 0.041
epoch 42,	batch   130,	training loss: 0.045
epoch 42,	batch   140,	training loss: 0.034
epoch 42,	batch   150,	training loss: 0.036
epoch 42,	batch   160,	training loss: 0.044
epoch 42,	batch   170,	training loss: 0.060
epoch 42,	batch   180,	training loss: 0.031
epoch 42,	batch   190,	training loss: 0.055
epoch 42,	batch   200,	training loss: 0.086
epoch 42,	batch   210,	training loss: 0.035
epoch 42,	batch   220,	training loss: 0.045
epoch 42,	batch   230,	training loss: 0.027
epoch 42,	batch   240,	training loss: 0.027
epoch 42,	batch   250,	training loss: 0.043
epoch 42,	batch   260,	training loss: 0.012
epoch 42,	batch   270,	training loss: 0.034
epoch 42,	batch   280,	training loss: 0.038
epoch 42,	batch   290,	training loss: 0.052
epoch 42,	batch   300,	training loss: 0.046
epoch 42,	batch   310,	training loss: 0.023
epoch 42,	batch   320,	training loss: 0.029
epoch 42,	batch   330,	training loss: 0.047
epoch 42,	batch   340,	training loss: 0.032
epoch 42,	batch   350,	training loss: 0.026
epoch 42,	batch   360,	training loss: 0.029
epoch 42,	batch   370,	training loss: 0.028
epoch 42,	batch   380,	training loss: 0.043
epoch 42,	batch   390,	training loss: 0.039
epoch 42,	batch   400,	training loss: 0.037
epoch 42,	batch   410,	training loss: 0.042
epoch 42,	batch   420,	training loss: 0.031
epoch 42,	batch   430,	training loss: 0.091
epoch 42,	batch   440,	training loss: 0.028
epoch 42,	batch   450,	training loss: 0.046
epoch 42,	batch   460,	training loss: 0.023
epoch 42,	batch   470,	training loss: 0.062
epoch 42,	batch   480,	training loss: 0.033
epoch 42,	batch   490,	training loss: 0.056
epoch 42,	batch   500,	training loss: 0.026
epoch 42,	batch   510,	training loss: 0.044
epoch 42,	batch   520,	training loss: 0.052
epoch 42,	batch   530,	training loss: 0.043
epoch 42,	batch   540,	training loss: 0.031
epoch 42,	batch   550,	training loss: 0.027
epoch 42,	batch   560,	training loss: 0.039
epoch 42,	batch   570,	training loss: 0.024
epoch 42,	batch   580,	training loss: 0.029
epoch 42,	batch   590,	training loss: 0.016
epoch 42,	batch   600,	training loss: 0.027
epoch 42,	batch   610,	training loss: 0.052
epoch 42,	batch   620,	training loss: 0.048
epoch 42,	batch   630,	training loss: 0.065
epoch 42,	batch   640,	training loss: 0.043
epoch 42,	batch   650,	training loss: 0.071
epoch 42,	batch   660,	training loss: 0.037
epoch 42,	batch   670,	training loss: 0.050
epoch 42,	batch   680,	training loss: 0.026
epoch 42,	batch   690,	training loss: 0.058
epoch 42,	batch   700,	training loss: 0.040
epoch 42,	batch   710,	training loss: 0.111
epoch 42,	batch   720,	training loss: 0.056
epoch 42,	batch   730,	training loss: 0.033
epoch 42,	batch   740,	training loss: 0.021
epoch 42,	batch   750,	training loss: 0.043
epoch 42,	batch   760,	training loss: 0.085
epoch 42,	batch   770,	training loss: 0.078
epoch 42,	batch   780,	training loss: 0.037
epoch 42,	batch   790,	training loss: 0.067
epoch 42,	batch   800,	training loss: 0.045
epoch 42,	batch   810,	training loss: 0.045
epoch 42,	batch   820,	training loss: 0.023
epoch 42,	batch   830,	training loss: 0.090
epoch 42,	batch   840,	training loss: 0.080
epoch 42,	batch   850,	training loss: 0.046
epoch 42,	batch   860,	training loss: 0.067
epoch 42,	batch   870,	training loss: 0.029
epoch 42,	batch   880,	training loss: 0.042
epoch 42,	batch   890,	training loss: 0.050
epoch 42,	batch   900,	training loss: 0.055
epoch 42,	batch   910,	training loss: 0.106
epoch 42,	batch   920,	training loss: 0.026
epoch 42,	batch   930,	training loss: 0.043
epoch 42,	batch   940,	training loss: 0.051
epoch 42,	batch   950,	training loss: 0.095
epoch 42,	batch   960,	training loss: 0.044
epoch 42,	batch   970,	training loss: 0.070
epoch 42,	batch   980,	training loss: 0.051
epoch 42,	batch   990,	training loss: 0.066
epoch 42,	batch  1000,	training loss: 0.042
epoch 42,	batch  1010,	training loss: 0.029
epoch 42,	batch  1020,	training loss: 0.031
epoch 42,	batch  1030,	training loss: 0.027
epoch 42,	batch  1040,	training loss: 0.024
epoch 42,	batch  1050,	training loss: 0.037
epoch 42,	batch  1060,	training loss: 0.032
epoch 42,	batch  1070,	training loss: 0.100
epoch 42,	batch  1080,	training loss: 0.034
epoch 42,	batch  1090,	training loss: 0.039
epoch 42,	batch  1100,	training loss: 0.060
epoch 42,	batch  1110,	training loss: 0.041
epoch 42,	batch  1120,	training loss: 0.051
epoch 42,	batch  1130,	training loss: 0.055
epoch 42,	batch  1140,	training loss: 0.098
epoch 42,	batch  1150,	training loss: 0.108
epoch 42,	batch  1160,	training loss: 0.055
epoch 42,	batch  1170,	training loss: 0.044
epoch 42,	batch  1180,	training loss: 0.070
epoch 42,	batch  1190,	training loss: 0.056
epoch 42,	batch  1200,	training loss: 0.070
epoch 42,	batch  1210,	training loss: 0.064
epoch 42,	batch  1220,	training loss: 0.032
epoch 42,	batch  1230,	training loss: 0.062
epoch 42,	batch  1240,	training loss: 0.039
epoch 42,	batch  1250,	training loss: 0.031
epoch 42,	batch  1260,	training loss: 0.038
epoch 42,	batch  1270,	training loss: 0.068
epoch 42,	batch  1280,	training loss: 0.025
epoch 42,	batch  1290,	training loss: 0.056
epoch 42,	batch  1300,	training loss: 0.098
epoch 42,	batch  1310,	training loss: 0.032
epoch 42,	batch  1320,	training loss: 0.054
epoch 42,	batch  1330,	training loss: 0.025
epoch 42,	batch  1340,	training loss: 0.036
epoch 42,	batch  1350,	training loss: 0.060
epoch 42,	batch  1360,	training loss: 0.052
epoch 42,	batch  1370,	training loss: 0.058
epoch 42,	batch  1380,	training loss: 0.039
epoch 42,	batch  1390,	training loss: 0.028
epoch 42,	batch  1400,	training loss: 0.043
epoch 42,	batch  1410,	training loss: 0.020
epoch 42,	batch  1420,	training loss: 0.037
epoch 42,	batch  1430,	training loss: 0.045
epoch 42,	batch  1440,	training loss: 0.037
epoch 42,	batch  1450,	training loss: 0.060
epoch 42,	batch  1460,	training loss: 0.074
epoch 42,	batch  1470,	training loss: 0.048
epoch 42,	batch  1480,	training loss: 0.049
epoch 42,	batch  1490,	training loss: 0.066
epoch 42,	batch  1500,	training loss: 0.064
epoch 42,	batch  1510,	training loss: 0.024
epoch 42,	batch  1520,	training loss: 0.085
epoch 42,	batch  1530,	training loss: 0.041
epoch 42,	batch  1540,	training loss: 0.060
epoch 42,	batch  1550,	training loss: 0.034
epoch 42,	batch  1560,	training loss: 0.040
epoch 42,	batch  1570,	training loss: 0.037
epoch 42,	batch  1580,	training loss: 0.034
epoch 42,	batch  1590,	training loss: 0.036
epoch 42,	batch  1600,	training loss: 0.068
epoch 42,	batch  1610,	training loss: 0.061
epoch 42,	batch  1620,	training loss: 0.033
epoch 42,	batch  1630,	training loss: 0.055
epoch 42,	batch  1640,	training loss: 0.035
epoch 42,	batch  1650,	training loss: 0.048
epoch 42,	batch  1660,	training loss: 0.058
epoch 42,	batch  1670,	training loss: 0.055
epoch 42,	batch  1680,	training loss: 0.037
epoch 42,	batch  1690,	training loss: 0.042
epoch 42,	batch  1700,	training loss: 0.031
epoch 42,	batch  1710,	training loss: 0.079
epoch 42,	batch  1720,	training loss: 0.058
epoch 42,	batch  1730,	training loss: 0.072
epoch 42,	batch  1740,	training loss: 0.062
epoch 42,	batch  1750,	training loss: 0.047
epoch 42,	batch  1760,	training loss: 0.071
epoch 42,	batch  1770,	training loss: 0.039
epoch 42,	batch  1780,	training loss: 0.055
epoch 42,	batch  1790,	training loss: 0.047
epoch 42,	batch  1800,	training loss: 0.047
epoch 42,	batch  1810,	training loss: 0.031
epoch 42,	batch  1820,	training loss: 0.091
epoch 42,	batch  1830,	training loss: 0.041
epoch 42,	batch  1840,	training loss: 0.041
epoch 42,	batch  1850,	training loss: 0.082
epoch 42,	batch  1860,	training loss: 0.055
epoch 42,	batch  1870,	training loss: 0.047
epoch 42,	batch  1880,	training loss: 0.041
epoch 42,	batch  1890,	training loss: 0.032
epoch 42,	batch  1900,	training loss: 0.024
epoch 42,	batch  1910,	training loss: 0.071
epoch 42,	batch  1920,	training loss: 0.072
epoch 42,	batch  1930,	training loss: 0.043
epoch 42,	batch  1940,	training loss: 0.068
epoch 42,	batch  1950,	training loss: 0.046
epoch 42,	batch  1960,	training loss: 0.104
epoch 42,	batch  1970,	training loss: 0.085
epoch 42,	batch  1980,	training loss: 0.079
END OF EPOCH 42
Testing on validation set...
# correct:	6079/54904 = 11.072053038030017%
# off by 1:	10951/54904 = 19.945723444557775%

epoch 43,	batch    10,	training loss: 0.027
epoch 43,	batch    20,	training loss: 0.062
epoch 43,	batch    30,	training loss: 0.032
epoch 43,	batch    40,	training loss: 0.028
epoch 43,	batch    50,	training loss: 0.073
epoch 43,	batch    60,	training loss: 0.023
epoch 43,	batch    70,	training loss: 0.038
epoch 43,	batch    80,	training loss: 0.069
epoch 43,	batch    90,	training loss: 0.029
epoch 43,	batch   100,	training loss: 0.071
epoch 43,	batch   110,	training loss: 0.045
epoch 43,	batch   120,	training loss: 0.043
epoch 43,	batch   130,	training loss: 0.027
epoch 43,	batch   140,	training loss: 0.035
epoch 43,	batch   150,	training loss: 0.058
epoch 43,	batch   160,	training loss: 0.033
epoch 43,	batch   170,	training loss: 0.035
epoch 43,	batch   180,	training loss: 0.014
epoch 43,	batch   190,	training loss: 0.038
epoch 43,	batch   200,	training loss: 0.046
epoch 43,	batch   210,	training loss: 0.025
epoch 43,	batch   220,	training loss: 0.026
epoch 43,	batch   230,	training loss: 0.108
epoch 43,	batch   240,	training loss: 0.035
epoch 43,	batch   250,	training loss: 0.028
epoch 43,	batch   260,	training loss: 0.028
epoch 43,	batch   270,	training loss: 0.029
epoch 43,	batch   280,	training loss: 0.070
epoch 43,	batch   290,	training loss: 0.049
epoch 43,	batch   300,	training loss: 0.012
epoch 43,	batch   310,	training loss: 0.044
epoch 43,	batch   320,	training loss: 0.019
epoch 43,	batch   330,	training loss: 0.025
epoch 43,	batch   340,	training loss: 0.048
epoch 43,	batch   350,	training loss: 0.064
epoch 43,	batch   360,	training loss: 0.034
epoch 43,	batch   370,	training loss: 0.065
epoch 43,	batch   380,	training loss: 0.025
epoch 43,	batch   390,	training loss: 0.024
epoch 43,	batch   400,	training loss: 0.039
epoch 43,	batch   410,	training loss: 0.031
epoch 43,	batch   420,	training loss: 0.035
epoch 43,	batch   430,	training loss: 0.041
epoch 43,	batch   440,	training loss: 0.047
epoch 43,	batch   450,	training loss: 0.032
epoch 43,	batch   460,	training loss: 0.024
epoch 43,	batch   470,	training loss: 0.032
epoch 43,	batch   480,	training loss: 0.053
epoch 43,	batch   490,	training loss: 0.022
epoch 43,	batch   500,	training loss: 0.038
epoch 43,	batch   510,	training loss: 0.048
epoch 43,	batch   520,	training loss: 0.017
epoch 43,	batch   530,	training loss: 0.062
epoch 43,	batch   540,	training loss: 0.045
epoch 43,	batch   550,	training loss: 0.025
epoch 43,	batch   560,	training loss: 0.020
epoch 43,	batch   570,	training loss: 0.037
epoch 43,	batch   580,	training loss: 0.056
epoch 43,	batch   590,	training loss: 0.048
epoch 43,	batch   600,	training loss: 0.056
epoch 43,	batch   610,	training loss: 0.040
epoch 43,	batch   620,	training loss: 0.031
epoch 43,	batch   630,	training loss: 0.060
epoch 43,	batch   640,	training loss: 0.060
epoch 43,	batch   650,	training loss: 0.034
epoch 43,	batch   660,	training loss: 0.040
epoch 43,	batch   670,	training loss: 0.037
epoch 43,	batch   680,	training loss: 0.019
epoch 43,	batch   690,	training loss: 0.056
epoch 43,	batch   700,	training loss: 0.122
epoch 43,	batch   710,	training loss: 0.078
epoch 43,	batch   720,	training loss: 0.043
epoch 43,	batch   730,	training loss: 0.027
epoch 43,	batch   740,	training loss: 0.046
epoch 43,	batch   750,	training loss: 0.059
epoch 43,	batch   760,	training loss: 0.061
epoch 43,	batch   770,	training loss: 0.066
epoch 43,	batch   780,	training loss: 0.055
epoch 43,	batch   790,	training loss: 0.045
epoch 43,	batch   800,	training loss: 0.027
epoch 43,	batch   810,	training loss: 0.050
epoch 43,	batch   820,	training loss: 0.055
epoch 43,	batch   830,	training loss: 0.038
epoch 43,	batch   840,	training loss: 0.029
epoch 43,	batch   850,	training loss: 0.021
epoch 43,	batch   860,	training loss: 0.034
epoch 43,	batch   870,	training loss: 0.072
epoch 43,	batch   880,	training loss: 0.065
epoch 43,	batch   890,	training loss: 0.038
epoch 43,	batch   900,	training loss: 0.061
epoch 43,	batch   910,	training loss: 0.043
epoch 43,	batch   920,	training loss: 0.046
epoch 43,	batch   930,	training loss: 0.037
epoch 43,	batch   940,	training loss: 0.036
epoch 43,	batch   950,	training loss: 0.028
epoch 43,	batch   960,	training loss: 0.036
epoch 43,	batch   970,	training loss: 0.070
epoch 43,	batch   980,	training loss: 0.076
epoch 43,	batch   990,	training loss: 0.080
epoch 43,	batch  1000,	training loss: 0.049
epoch 43,	batch  1010,	training loss: 0.061
epoch 43,	batch  1020,	training loss: 0.050
epoch 43,	batch  1030,	training loss: 0.035
epoch 43,	batch  1040,	training loss: 0.040
epoch 43,	batch  1050,	training loss: 0.027
epoch 43,	batch  1060,	training loss: 0.056
epoch 43,	batch  1070,	training loss: 0.032
epoch 43,	batch  1080,	training loss: 0.049
epoch 43,	batch  1090,	training loss: 0.049
epoch 43,	batch  1100,	training loss: 0.044
epoch 43,	batch  1110,	training loss: 0.042
epoch 43,	batch  1120,	training loss: 0.029
epoch 43,	batch  1130,	training loss: 0.053
epoch 43,	batch  1140,	training loss: 0.035
epoch 43,	batch  1150,	training loss: 0.041
epoch 43,	batch  1160,	training loss: 0.048
epoch 43,	batch  1170,	training loss: 0.047
epoch 43,	batch  1180,	training loss: 0.051
epoch 43,	batch  1190,	training loss: 0.041
epoch 43,	batch  1200,	training loss: 0.025
epoch 43,	batch  1210,	training loss: 0.062
epoch 43,	batch  1220,	training loss: 0.051
epoch 43,	batch  1230,	training loss: 0.040
epoch 43,	batch  1240,	training loss: 0.036
epoch 43,	batch  1250,	training loss: 0.059
epoch 43,	batch  1260,	training loss: 0.035
epoch 43,	batch  1270,	training loss: 0.041
epoch 43,	batch  1280,	training loss: 0.050
epoch 43,	batch  1290,	training loss: 0.057
epoch 43,	batch  1300,	training loss: 0.071
epoch 43,	batch  1310,	training loss: 0.058
epoch 43,	batch  1320,	training loss: 0.054
epoch 43,	batch  1330,	training loss: 0.048
epoch 43,	batch  1340,	training loss: 0.038
epoch 43,	batch  1350,	training loss: 0.034
epoch 43,	batch  1360,	training loss: 0.059
epoch 43,	batch  1370,	training loss: 0.075
epoch 43,	batch  1380,	training loss: 0.042
epoch 43,	batch  1390,	training loss: 0.093
epoch 43,	batch  1400,	training loss: 0.065
epoch 43,	batch  1410,	training loss: 0.027
epoch 43,	batch  1420,	training loss: 0.057
epoch 43,	batch  1430,	training loss: 0.036
epoch 43,	batch  1440,	training loss: 0.056
epoch 43,	batch  1450,	training loss: 0.041
epoch 43,	batch  1460,	training loss: 0.077
epoch 43,	batch  1470,	training loss: 0.042
epoch 43,	batch  1480,	training loss: 0.060
epoch 43,	batch  1490,	training loss: 0.043
epoch 43,	batch  1500,	training loss: 0.172
epoch 43,	batch  1510,	training loss: 0.053
epoch 43,	batch  1520,	training loss: 0.056
epoch 43,	batch  1530,	training loss: 0.039
epoch 43,	batch  1540,	training loss: 0.044
epoch 43,	batch  1550,	training loss: 0.039
epoch 43,	batch  1560,	training loss: 0.062
epoch 43,	batch  1570,	training loss: 0.096
epoch 43,	batch  1580,	training loss: 0.034
epoch 43,	batch  1590,	training loss: 0.039
epoch 43,	batch  1600,	training loss: 0.109
epoch 43,	batch  1610,	training loss: 0.044
epoch 43,	batch  1620,	training loss: 0.043
epoch 43,	batch  1630,	training loss: 0.053
epoch 43,	batch  1640,	training loss: 0.055
epoch 43,	batch  1650,	training loss: 0.050
epoch 43,	batch  1660,	training loss: 0.056
epoch 43,	batch  1670,	training loss: 0.027
epoch 43,	batch  1680,	training loss: 0.052
epoch 43,	batch  1690,	training loss: 0.032
epoch 43,	batch  1700,	training loss: 0.030
epoch 43,	batch  1710,	training loss: 0.027
epoch 43,	batch  1720,	training loss: 0.021
epoch 43,	batch  1730,	training loss: 0.040
epoch 43,	batch  1740,	training loss: 0.072
epoch 43,	batch  1750,	training loss: 0.028
epoch 43,	batch  1760,	training loss: 0.064
epoch 43,	batch  1770,	training loss: 0.063
epoch 43,	batch  1780,	training loss: 0.064
epoch 43,	batch  1790,	training loss: 0.061
epoch 43,	batch  1800,	training loss: 0.054
epoch 43,	batch  1810,	training loss: 0.057
epoch 43,	batch  1820,	training loss: 0.065
epoch 43,	batch  1830,	training loss: 0.043
epoch 43,	batch  1840,	training loss: 0.031
epoch 43,	batch  1850,	training loss: 0.088
epoch 43,	batch  1860,	training loss: 0.103
epoch 43,	batch  1870,	training loss: 0.072
epoch 43,	batch  1880,	training loss: 0.050
epoch 43,	batch  1890,	training loss: 0.058
epoch 43,	batch  1900,	training loss: 0.074
epoch 43,	batch  1910,	training loss: 0.037
epoch 43,	batch  1920,	training loss: 0.037
epoch 43,	batch  1930,	training loss: 0.023
epoch 43,	batch  1940,	training loss: 0.055
epoch 43,	batch  1950,	training loss: 0.054
epoch 43,	batch  1960,	training loss: 0.042
epoch 43,	batch  1970,	training loss: 0.104
epoch 43,	batch  1980,	training loss: 0.137
END OF EPOCH 43
Testing on validation set...
# correct:	5900/54904 = 10.746029433192481%
# off by 1:	10521/54904 = 19.162538248579338%

epoch 44,	batch    10,	training loss: 0.041
epoch 44,	batch    20,	training loss: 0.035
epoch 44,	batch    30,	training loss: 0.078
epoch 44,	batch    40,	training loss: 0.046
epoch 44,	batch    50,	training loss: 0.034
epoch 44,	batch    60,	training loss: 0.024
epoch 44,	batch    70,	training loss: 0.044
epoch 44,	batch    80,	training loss: 0.042
epoch 44,	batch    90,	training loss: 0.017
epoch 44,	batch   100,	training loss: 0.039
epoch 44,	batch   110,	training loss: 0.036
epoch 44,	batch   120,	training loss: 0.038
epoch 44,	batch   130,	training loss: 0.042
epoch 44,	batch   140,	training loss: 0.038
epoch 44,	batch   150,	training loss: 0.025
epoch 44,	batch   160,	training loss: 0.022
epoch 44,	batch   170,	training loss: 0.029
epoch 44,	batch   180,	training loss: 0.034
epoch 44,	batch   190,	training loss: 0.048
epoch 44,	batch   200,	training loss: 0.017
epoch 44,	batch   210,	training loss: 0.025
epoch 44,	batch   220,	training loss: 0.034
epoch 44,	batch   230,	training loss: 0.032
epoch 44,	batch   240,	training loss: 0.035
epoch 44,	batch   250,	training loss: 0.025
epoch 44,	batch   260,	training loss: 0.033
epoch 44,	batch   270,	training loss: 0.012
epoch 44,	batch   280,	training loss: 0.050
epoch 44,	batch   290,	training loss: 0.019
epoch 44,	batch   300,	training loss: 0.022
epoch 44,	batch   310,	training loss: 0.038
epoch 44,	batch   320,	training loss: 0.028
epoch 44,	batch   330,	training loss: 0.030
epoch 44,	batch   340,	training loss: 0.019
epoch 44,	batch   350,	training loss: 0.044
epoch 44,	batch   360,	training loss: 0.035
epoch 44,	batch   370,	training loss: 0.071
epoch 44,	batch   380,	training loss: 0.023
epoch 44,	batch   390,	training loss: 0.074
epoch 44,	batch   400,	training loss: 0.036
epoch 44,	batch   410,	training loss: 0.037
epoch 44,	batch   420,	training loss: 0.048
epoch 44,	batch   430,	training loss: 0.026
epoch 44,	batch   440,	training loss: 0.020
epoch 44,	batch   450,	training loss: 0.022
epoch 44,	batch   460,	training loss: 0.024
epoch 44,	batch   470,	training loss: 0.024
epoch 44,	batch   480,	training loss: 0.027
epoch 44,	batch   490,	training loss: 0.029
epoch 44,	batch   500,	training loss: 0.027
epoch 44,	batch   510,	training loss: 0.024
epoch 44,	batch   520,	training loss: 0.027
epoch 44,	batch   530,	training loss: 0.040
epoch 44,	batch   540,	training loss: 0.066
epoch 44,	batch   550,	training loss: 0.032
epoch 44,	batch   560,	training loss: 0.029
epoch 44,	batch   570,	training loss: 0.023
epoch 44,	batch   580,	training loss: 0.038
epoch 44,	batch   590,	training loss: 0.015
epoch 44,	batch   600,	training loss: 0.052
epoch 44,	batch   610,	training loss: 0.033
epoch 44,	batch   620,	training loss: 0.049
epoch 44,	batch   630,	training loss: 0.043
epoch 44,	batch   640,	training loss: 0.044
epoch 44,	batch   650,	training loss: 0.060
epoch 44,	batch   660,	training loss: 0.025
epoch 44,	batch   670,	training loss: 0.033
epoch 44,	batch   680,	training loss: 0.018
epoch 44,	batch   690,	training loss: 0.050
epoch 44,	batch   700,	training loss: 0.028
epoch 44,	batch   710,	training loss: 0.032
epoch 44,	batch   720,	training loss: 0.045
epoch 44,	batch   730,	training loss: 0.044
epoch 44,	batch   740,	training loss: 0.036
epoch 44,	batch   750,	training loss: 0.028
epoch 44,	batch   760,	training loss: 0.047
epoch 44,	batch   770,	training loss: 0.026
epoch 44,	batch   780,	training loss: 0.048
epoch 44,	batch   790,	training loss: 0.032
epoch 44,	batch   800,	training loss: 0.034
epoch 44,	batch   810,	training loss: 0.049
epoch 44,	batch   820,	training loss: 0.043
epoch 44,	batch   830,	training loss: 0.038
epoch 44,	batch   840,	training loss: 0.024
epoch 44,	batch   850,	training loss: 0.020
epoch 44,	batch   860,	training loss: 0.045
epoch 44,	batch   870,	training loss: 0.038
epoch 44,	batch   880,	training loss: 0.040
epoch 44,	batch   890,	training loss: 0.053
epoch 44,	batch   900,	training loss: 0.029
epoch 44,	batch   910,	training loss: 0.030
epoch 44,	batch   920,	training loss: 0.048
epoch 44,	batch   930,	training loss: 0.058
epoch 44,	batch   940,	training loss: 0.027
epoch 44,	batch   950,	training loss: 0.037
epoch 44,	batch   960,	training loss: 0.069
epoch 44,	batch   970,	training loss: 0.032
epoch 44,	batch   980,	training loss: 0.020
epoch 44,	batch   990,	training loss: 0.037
epoch 44,	batch  1000,	training loss: 0.060
epoch 44,	batch  1010,	training loss: 0.032
epoch 44,	batch  1020,	training loss: 0.041
epoch 44,	batch  1030,	training loss: 0.024
epoch 44,	batch  1040,	training loss: 0.039
epoch 44,	batch  1050,	training loss: 0.070
epoch 44,	batch  1060,	training loss: 0.034
epoch 44,	batch  1070,	training loss: 0.040
epoch 44,	batch  1080,	training loss: 0.029
epoch 44,	batch  1090,	training loss: 0.026
epoch 44,	batch  1100,	training loss: 0.014
epoch 44,	batch  1110,	training loss: 0.036
epoch 44,	batch  1120,	training loss: 0.009
epoch 44,	batch  1130,	training loss: 0.033
epoch 44,	batch  1140,	training loss: 0.035
epoch 44,	batch  1150,	training loss: 0.109
epoch 44,	batch  1160,	training loss: 0.041
epoch 44,	batch  1170,	training loss: 0.073
epoch 44,	batch  1180,	training loss: 0.027
epoch 44,	batch  1190,	training loss: 0.025
epoch 44,	batch  1200,	training loss: 0.047
epoch 44,	batch  1210,	training loss: 0.048
epoch 44,	batch  1220,	training loss: 0.050
epoch 44,	batch  1230,	training loss: 0.031
epoch 44,	batch  1240,	training loss: 0.024
epoch 44,	batch  1250,	training loss: 0.040
epoch 44,	batch  1260,	training loss: 0.041
epoch 44,	batch  1270,	training loss: 0.045
epoch 44,	batch  1280,	training loss: 0.047
epoch 44,	batch  1290,	training loss: 0.052
epoch 44,	batch  1300,	training loss: 0.033
epoch 44,	batch  1310,	training loss: 0.042
epoch 44,	batch  1320,	training loss: 0.029
epoch 44,	batch  1330,	training loss: 0.016
epoch 44,	batch  1340,	training loss: 0.045
epoch 44,	batch  1350,	training loss: 0.027
epoch 44,	batch  1360,	training loss: 0.044
epoch 44,	batch  1370,	training loss: 0.061
epoch 44,	batch  1380,	training loss: 0.025
epoch 44,	batch  1390,	training loss: 0.067
epoch 44,	batch  1400,	training loss: 0.052
epoch 44,	batch  1410,	training loss: 0.042
epoch 44,	batch  1420,	training loss: 0.025
epoch 44,	batch  1430,	training loss: 0.035
epoch 44,	batch  1440,	training loss: 0.050
epoch 44,	batch  1450,	training loss: 0.040
epoch 44,	batch  1460,	training loss: 0.022
epoch 44,	batch  1470,	training loss: 0.028
epoch 44,	batch  1480,	training loss: 0.038
epoch 44,	batch  1490,	training loss: 0.048
epoch 44,	batch  1500,	training loss: 0.046
epoch 44,	batch  1510,	training loss: 0.086
epoch 44,	batch  1520,	training loss: 0.079
epoch 44,	batch  1530,	training loss: 0.046
epoch 44,	batch  1540,	training loss: 0.053
epoch 44,	batch  1550,	training loss: 0.054
epoch 44,	batch  1560,	training loss: 0.035
epoch 44,	batch  1570,	training loss: 0.058
epoch 44,	batch  1580,	training loss: 0.081
epoch 44,	batch  1590,	training loss: 0.063
epoch 44,	batch  1600,	training loss: 0.028
epoch 44,	batch  1610,	training loss: 0.030
epoch 44,	batch  1620,	training loss: 0.038
epoch 44,	batch  1630,	training loss: 0.062
epoch 44,	batch  1640,	training loss: 0.055
epoch 44,	batch  1650,	training loss: 0.048
epoch 44,	batch  1660,	training loss: 0.032
epoch 44,	batch  1670,	training loss: 0.043
epoch 44,	batch  1680,	training loss: 0.021
epoch 44,	batch  1690,	training loss: 0.051
epoch 44,	batch  1700,	training loss: 0.025
epoch 44,	batch  1710,	training loss: 0.063
epoch 44,	batch  1720,	training loss: 0.047
epoch 44,	batch  1730,	training loss: 0.048
epoch 44,	batch  1740,	training loss: 0.034
epoch 44,	batch  1750,	training loss: 0.042
epoch 44,	batch  1760,	training loss: 0.029
epoch 44,	batch  1770,	training loss: 0.041
epoch 44,	batch  1780,	training loss: 0.032
epoch 44,	batch  1790,	training loss: 0.045
epoch 44,	batch  1800,	training loss: 0.062
epoch 44,	batch  1810,	training loss: 0.045
epoch 44,	batch  1820,	training loss: 0.030
epoch 44,	batch  1830,	training loss: 0.055
epoch 44,	batch  1840,	training loss: 0.072
epoch 44,	batch  1850,	training loss: 0.065
epoch 44,	batch  1860,	training loss: 0.056
epoch 44,	batch  1870,	training loss: 0.042
epoch 44,	batch  1880,	training loss: 0.051
epoch 44,	batch  1890,	training loss: 0.033
epoch 44,	batch  1900,	training loss: 0.062
epoch 44,	batch  1910,	training loss: 0.057
epoch 44,	batch  1920,	training loss: 0.058
epoch 44,	batch  1930,	training loss: 0.057
epoch 44,	batch  1940,	training loss: 0.047
epoch 44,	batch  1950,	training loss: 0.031
epoch 44,	batch  1960,	training loss: 0.087
epoch 44,	batch  1970,	training loss: 0.050
epoch 44,	batch  1980,	training loss: 0.052
END OF EPOCH 44
Testing on validation set...
# correct:	5904/54904 = 10.753314876876003%
# off by 1:	10573/54904 = 19.257249016465103%

epoch 45,	batch    10,	training loss: 0.012
epoch 45,	batch    20,	training loss: 0.052
epoch 45,	batch    30,	training loss: 0.015
epoch 45,	batch    40,	training loss: 0.034
epoch 45,	batch    50,	training loss: 0.017
epoch 45,	batch    60,	training loss: 0.036
epoch 45,	batch    70,	training loss: 0.014
epoch 45,	batch    80,	training loss: 0.060
epoch 45,	batch    90,	training loss: 0.014
epoch 45,	batch   100,	training loss: 0.039
epoch 45,	batch   110,	training loss: 0.020
epoch 45,	batch   120,	training loss: 0.026
epoch 45,	batch   130,	training loss: 0.038
epoch 45,	batch   140,	training loss: 0.019
epoch 45,	batch   150,	training loss: 0.011
epoch 45,	batch   160,	training loss: 0.024
epoch 45,	batch   170,	training loss: 0.015
epoch 45,	batch   180,	training loss: 0.035
epoch 45,	batch   190,	training loss: 0.030
epoch 45,	batch   200,	training loss: 0.034
epoch 45,	batch   210,	training loss: 0.020
epoch 45,	batch   220,	training loss: 0.016
epoch 45,	batch   230,	training loss: 0.039
epoch 45,	batch   240,	training loss: 0.024
epoch 45,	batch   250,	training loss: 0.028
epoch 45,	batch   260,	training loss: 0.020
epoch 45,	batch   270,	training loss: 0.051
epoch 45,	batch   280,	training loss: 0.015
epoch 45,	batch   290,	training loss: 0.058
epoch 45,	batch   300,	training loss: 0.034
epoch 45,	batch   310,	training loss: 0.033
epoch 45,	batch   320,	training loss: 0.045
epoch 45,	batch   330,	training loss: 0.041
epoch 45,	batch   340,	training loss: 0.028
epoch 45,	batch   350,	training loss: 0.015
epoch 45,	batch   360,	training loss: 0.043
epoch 45,	batch   370,	training loss: 0.028
epoch 45,	batch   380,	training loss: 0.038
epoch 45,	batch   390,	training loss: 0.029
epoch 45,	batch   400,	training loss: 0.023
epoch 45,	batch   410,	training loss: 0.032
epoch 45,	batch   420,	training loss: 0.014
epoch 45,	batch   430,	training loss: 0.029
epoch 45,	batch   440,	training loss: 0.017
epoch 45,	batch   450,	training loss: 0.032
epoch 45,	batch   460,	training loss: 0.083
epoch 45,	batch   470,	training loss: 0.024
epoch 45,	batch   480,	training loss: 0.036
epoch 45,	batch   490,	training loss: 0.041
epoch 45,	batch   500,	training loss: 0.037
epoch 45,	batch   510,	training loss: 0.020
epoch 45,	batch   520,	training loss: 0.021
epoch 45,	batch   530,	training loss: 0.023
epoch 45,	batch   540,	training loss: 0.013
epoch 45,	batch   550,	training loss: 0.022
epoch 45,	batch   560,	training loss: 0.097
epoch 45,	batch   570,	training loss: 0.046
epoch 45,	batch   580,	training loss: 0.026
epoch 45,	batch   590,	training loss: 0.019
epoch 45,	batch   600,	training loss: 0.020
epoch 45,	batch   610,	training loss: 0.012
epoch 45,	batch   620,	training loss: 0.021
epoch 45,	batch   630,	training loss: 0.012
epoch 45,	batch   640,	training loss: 0.031
epoch 45,	batch   650,	training loss: 0.017
epoch 45,	batch   660,	training loss: 0.068
epoch 45,	batch   670,	training loss: 0.014
epoch 45,	batch   680,	training loss: 0.039
epoch 45,	batch   690,	training loss: 0.021
epoch 45,	batch   700,	training loss: 0.024
epoch 45,	batch   710,	training loss: 0.016
epoch 45,	batch   720,	training loss: 0.026
epoch 45,	batch   730,	training loss: 0.018
epoch 45,	batch   740,	training loss: 0.011
epoch 45,	batch   750,	training loss: 0.017
epoch 45,	batch   760,	training loss: 0.062
epoch 45,	batch   770,	training loss: 0.034
epoch 45,	batch   780,	training loss: 0.028
epoch 45,	batch   790,	training loss: 0.026
epoch 45,	batch   800,	training loss: 0.053
epoch 45,	batch   810,	training loss: 0.030
epoch 45,	batch   820,	training loss: 0.051
epoch 45,	batch   830,	training loss: 0.029
epoch 45,	batch   840,	training loss: 0.024
epoch 45,	batch   850,	training loss: 0.022
epoch 45,	batch   860,	training loss: 0.026
epoch 45,	batch   870,	training loss: 0.025
epoch 45,	batch   880,	training loss: 0.015
epoch 45,	batch   890,	training loss: 0.012
epoch 45,	batch   900,	training loss: 0.018
epoch 45,	batch   910,	training loss: 0.039
epoch 45,	batch   920,	training loss: 0.010
epoch 45,	batch   930,	training loss: 0.039
epoch 45,	batch   940,	training loss: 0.036
epoch 45,	batch   950,	training loss: 0.018
epoch 45,	batch   960,	training loss: 0.039
epoch 45,	batch   970,	training loss: 0.027
epoch 45,	batch   980,	training loss: 0.023
epoch 45,	batch   990,	training loss: 0.033
epoch 45,	batch  1000,	training loss: 0.015
epoch 45,	batch  1010,	training loss: 0.028
epoch 45,	batch  1020,	training loss: 0.020
epoch 45,	batch  1030,	training loss: 0.030
epoch 45,	batch  1040,	training loss: 0.039
epoch 45,	batch  1050,	training loss: 0.016
epoch 45,	batch  1060,	training loss: 0.032
epoch 45,	batch  1070,	training loss: 0.018
epoch 45,	batch  1080,	training loss: 0.028
epoch 45,	batch  1090,	training loss: 0.018
epoch 45,	batch  1100,	training loss: 0.032
epoch 45,	batch  1110,	training loss: 0.039
epoch 45,	batch  1120,	training loss: 0.046
epoch 45,	batch  1130,	training loss: 0.023
epoch 45,	batch  1140,	training loss: 0.049
epoch 45,	batch  1150,	training loss: 0.048
epoch 45,	batch  1160,	training loss: 0.040
epoch 45,	batch  1170,	training loss: 0.031
epoch 45,	batch  1180,	training loss: 0.059
epoch 45,	batch  1190,	training loss: 0.053
epoch 45,	batch  1200,	training loss: 0.037
epoch 45,	batch  1210,	training loss: 0.035
epoch 45,	batch  1220,	training loss: 0.026
epoch 45,	batch  1230,	training loss: 0.033
epoch 45,	batch  1240,	training loss: 0.049
epoch 45,	batch  1250,	training loss: 0.042
epoch 45,	batch  1260,	training loss: 0.029
epoch 45,	batch  1270,	training loss: 0.013
epoch 45,	batch  1280,	training loss: 0.014
epoch 45,	batch  1290,	training loss: 0.032
epoch 45,	batch  1300,	training loss: 0.015
epoch 45,	batch  1310,	training loss: 0.024
epoch 45,	batch  1320,	training loss: 0.027
epoch 45,	batch  1330,	training loss: 0.028
epoch 45,	batch  1340,	training loss: 0.047
epoch 45,	batch  1350,	training loss: 0.014
epoch 45,	batch  1360,	training loss: 0.040
epoch 45,	batch  1370,	training loss: 0.022
epoch 45,	batch  1380,	training loss: 0.027
epoch 45,	batch  1390,	training loss: 0.079
epoch 45,	batch  1400,	training loss: 0.064
epoch 45,	batch  1410,	training loss: 0.025
epoch 45,	batch  1420,	training loss: 0.053
epoch 45,	batch  1430,	training loss: 0.043
epoch 45,	batch  1440,	training loss: 0.033
epoch 45,	batch  1450,	training loss: 0.037
epoch 45,	batch  1460,	training loss: 0.031
epoch 45,	batch  1470,	training loss: 0.028
epoch 45,	batch  1480,	training loss: 0.052
epoch 45,	batch  1490,	training loss: 0.030
epoch 45,	batch  1500,	training loss: 0.024
epoch 45,	batch  1510,	training loss: 0.019
epoch 45,	batch  1520,	training loss: 0.017
epoch 45,	batch  1530,	training loss: 0.018
epoch 45,	batch  1540,	training loss: 0.030
epoch 45,	batch  1550,	training loss: 0.045
epoch 45,	batch  1560,	training loss: 0.101
epoch 45,	batch  1570,	training loss: 0.042
epoch 45,	batch  1580,	training loss: 0.056
epoch 45,	batch  1590,	training loss: 0.029
epoch 45,	batch  1600,	training loss: 0.023
epoch 45,	batch  1610,	training loss: 0.025
epoch 45,	batch  1620,	training loss: 0.015
epoch 45,	batch  1630,	training loss: 0.044
epoch 45,	batch  1640,	training loss: 0.026
epoch 45,	batch  1650,	training loss: 0.014
epoch 45,	batch  1660,	training loss: 0.036
epoch 45,	batch  1670,	training loss: 0.024
epoch 45,	batch  1680,	training loss: 0.021
epoch 45,	batch  1690,	training loss: 0.028
epoch 45,	batch  1700,	training loss: 0.058
epoch 45,	batch  1710,	training loss: 0.018
epoch 45,	batch  1720,	training loss: 0.027
epoch 45,	batch  1730,	training loss: 0.049
epoch 45,	batch  1740,	training loss: 0.023
epoch 45,	batch  1750,	training loss: 0.020
epoch 45,	batch  1760,	training loss: 0.015
epoch 45,	batch  1770,	training loss: 0.037
epoch 45,	batch  1780,	training loss: 0.044
epoch 45,	batch  1790,	training loss: 0.029
epoch 45,	batch  1800,	training loss: 0.033
epoch 45,	batch  1810,	training loss: 0.035
epoch 45,	batch  1820,	training loss: 0.036
epoch 45,	batch  1830,	training loss: 0.022
epoch 45,	batch  1840,	training loss: 0.043
epoch 45,	batch  1850,	training loss: 0.018
epoch 45,	batch  1860,	training loss: 0.075
epoch 45,	batch  1870,	training loss: 0.038
epoch 45,	batch  1880,	training loss: 0.014
epoch 45,	batch  1890,	training loss: 0.046
epoch 45,	batch  1900,	training loss: 0.030
epoch 45,	batch  1910,	training loss: 0.028
epoch 45,	batch  1920,	training loss: 0.041
epoch 45,	batch  1930,	training loss: 0.046
epoch 45,	batch  1940,	training loss: 0.025
epoch 45,	batch  1950,	training loss: 0.057
epoch 45,	batch  1960,	training loss: 0.020
epoch 45,	batch  1970,	training loss: 0.021
epoch 45,	batch  1980,	training loss: 0.041
END OF EPOCH 45
Testing on validation set...
# correct:	5860/54904 = 10.673174996357279%
# off by 1:	10933/54904 = 19.91293894798193%

epoch 46,	batch    10,	training loss: 0.025
epoch 46,	batch    20,	training loss: 0.047
epoch 46,	batch    30,	training loss: 0.008
epoch 46,	batch    40,	training loss: 0.021
epoch 46,	batch    50,	training loss: 0.025
epoch 46,	batch    60,	training loss: 0.025
epoch 46,	batch    70,	training loss: 0.017
epoch 46,	batch    80,	training loss: 0.023
epoch 46,	batch    90,	training loss: 0.010
epoch 46,	batch   100,	training loss: 0.011
epoch 46,	batch   110,	training loss: 0.033
epoch 46,	batch   120,	training loss: 0.009
epoch 46,	batch   130,	training loss: 0.013
epoch 46,	batch   140,	training loss: 0.010
epoch 46,	batch   150,	training loss: 0.013
epoch 46,	batch   160,	training loss: 0.027
epoch 46,	batch   170,	training loss: 0.020
epoch 46,	batch   180,	training loss: 0.040
epoch 46,	batch   190,	training loss: 0.028
epoch 46,	batch   200,	training loss: 0.026
epoch 46,	batch   210,	training loss: 0.013
epoch 46,	batch   220,	training loss: 0.030
epoch 46,	batch   230,	training loss: 0.048
epoch 46,	batch   240,	training loss: 0.044
epoch 46,	batch   250,	training loss: 0.011
epoch 46,	batch   260,	training loss: 0.021
epoch 46,	batch   270,	training loss: 0.015
epoch 46,	batch   280,	training loss: 0.018
epoch 46,	batch   290,	training loss: 0.011
epoch 46,	batch   300,	training loss: 0.042
epoch 46,	batch   310,	training loss: 0.030
epoch 46,	batch   320,	training loss: 0.022
epoch 46,	batch   330,	training loss: 0.042
epoch 46,	batch   340,	training loss: 0.023
epoch 46,	batch   350,	training loss: 0.027
epoch 46,	batch   360,	training loss: 0.038
epoch 46,	batch   370,	training loss: 0.013
epoch 46,	batch   380,	training loss: 0.045
epoch 46,	batch   390,	training loss: 0.011
epoch 46,	batch   400,	training loss: 0.030
epoch 46,	batch   410,	training loss: 0.040
epoch 46,	batch   420,	training loss: 0.021
epoch 46,	batch   430,	training loss: 0.022
epoch 46,	batch   440,	training loss: 0.029
epoch 46,	batch   450,	training loss: 0.018
epoch 46,	batch   460,	training loss: 0.036
epoch 46,	batch   470,	training loss: 0.048
epoch 46,	batch   480,	training loss: 0.021
epoch 46,	batch   490,	training loss: 0.057
epoch 46,	batch   500,	training loss: 0.043
epoch 46,	batch   510,	training loss: 0.027
epoch 46,	batch   520,	training loss: 0.015
epoch 46,	batch   530,	training loss: 0.035
epoch 46,	batch   540,	training loss: 0.025
epoch 46,	batch   550,	training loss: 0.018
epoch 46,	batch   560,	training loss: 0.015
epoch 46,	batch   570,	training loss: 0.054
epoch 46,	batch   580,	training loss: 0.020
epoch 46,	batch   590,	training loss: 0.028
epoch 46,	batch   600,	training loss: 0.016
epoch 46,	batch   610,	training loss: 0.019
epoch 46,	batch   620,	training loss: 0.024
epoch 46,	batch   630,	training loss: 0.028
epoch 46,	batch   640,	training loss: 0.024
epoch 46,	batch   650,	training loss: 0.013
epoch 46,	batch   660,	training loss: 0.030
epoch 46,	batch   670,	training loss: 0.026
epoch 46,	batch   680,	training loss: 0.016
epoch 46,	batch   690,	training loss: 0.051
epoch 46,	batch   700,	training loss: 0.017
epoch 46,	batch   710,	training loss: 0.042
epoch 46,	batch   720,	training loss: 0.035
epoch 46,	batch   730,	training loss: 0.037
epoch 46,	batch   740,	training loss: 0.065
epoch 46,	batch   750,	training loss: 0.024
epoch 46,	batch   760,	training loss: 0.027
epoch 46,	batch   770,	training loss: 0.026
epoch 46,	batch   780,	training loss: 0.039
epoch 46,	batch   790,	training loss: 0.009
epoch 46,	batch   800,	training loss: 0.019
epoch 46,	batch   810,	training loss: 0.022
epoch 46,	batch   820,	training loss: 0.026
epoch 46,	batch   830,	training loss: 0.017
epoch 46,	batch   840,	training loss: 0.027
epoch 46,	batch   850,	training loss: 0.024
epoch 46,	batch   860,	training loss: 0.021
epoch 46,	batch   870,	training loss: 0.051
epoch 46,	batch   880,	training loss: 0.026
epoch 46,	batch   890,	training loss: 0.015
epoch 46,	batch   900,	training loss: 0.017
epoch 46,	batch   910,	training loss: 0.040
epoch 46,	batch   920,	training loss: 0.032
epoch 46,	batch   930,	training loss: 0.023
epoch 46,	batch   940,	training loss: 0.034
epoch 46,	batch   950,	training loss: 0.037
epoch 46,	batch   960,	training loss: 0.025
epoch 46,	batch   970,	training loss: 0.023
epoch 46,	batch   980,	training loss: 0.011
epoch 46,	batch   990,	training loss: 0.012
epoch 46,	batch  1000,	training loss: 0.070
epoch 46,	batch  1010,	training loss: 0.031
epoch 46,	batch  1020,	training loss: 0.021
epoch 46,	batch  1030,	training loss: 0.030
epoch 46,	batch  1040,	training loss: 0.028
epoch 46,	batch  1050,	training loss: 0.015
epoch 46,	batch  1060,	training loss: 0.018
epoch 46,	batch  1070,	training loss: 0.022
epoch 46,	batch  1080,	training loss: 0.024
epoch 46,	batch  1090,	training loss: 0.038
epoch 46,	batch  1100,	training loss: 0.018
epoch 46,	batch  1110,	training loss: 0.040
epoch 46,	batch  1120,	training loss: 0.022
epoch 46,	batch  1130,	training loss: 0.022
epoch 46,	batch  1140,	training loss: 0.042
epoch 46,	batch  1150,	training loss: 0.033
epoch 46,	batch  1160,	training loss: 0.028
epoch 46,	batch  1170,	training loss: 0.081
epoch 46,	batch  1180,	training loss: 0.022
epoch 46,	batch  1190,	training loss: 0.029
epoch 46,	batch  1200,	training loss: 0.025
epoch 46,	batch  1210,	training loss: 0.021
epoch 46,	batch  1220,	training loss: 0.016
epoch 46,	batch  1230,	training loss: 0.023
epoch 46,	batch  1240,	training loss: 0.030
epoch 46,	batch  1250,	training loss: 0.016
epoch 46,	batch  1260,	training loss: 0.012
epoch 46,	batch  1270,	training loss: 0.017
epoch 46,	batch  1280,	training loss: 0.010
epoch 46,	batch  1290,	training loss: 0.066
epoch 46,	batch  1300,	training loss: 0.022
epoch 46,	batch  1310,	training loss: 0.008
epoch 46,	batch  1320,	training loss: 0.022
epoch 46,	batch  1330,	training loss: 0.042
epoch 46,	batch  1340,	training loss: 0.057
epoch 46,	batch  1350,	training loss: 0.017
epoch 46,	batch  1360,	training loss: 0.022
epoch 46,	batch  1370,	training loss: 0.027
epoch 46,	batch  1380,	training loss: 0.031
epoch 46,	batch  1390,	training loss: 0.021
epoch 46,	batch  1400,	training loss: 0.040
epoch 46,	batch  1410,	training loss: 0.018
epoch 46,	batch  1420,	training loss: 0.008
epoch 46,	batch  1430,	training loss: 0.015
epoch 46,	batch  1440,	training loss: 0.044
epoch 46,	batch  1450,	training loss: 0.021
epoch 46,	batch  1460,	training loss: 0.012
epoch 46,	batch  1470,	training loss: 0.028
epoch 46,	batch  1480,	training loss: 0.016
epoch 46,	batch  1490,	training loss: 0.014
epoch 46,	batch  1500,	training loss: 0.033
epoch 46,	batch  1510,	training loss: 0.025
epoch 46,	batch  1520,	training loss: 0.026
epoch 46,	batch  1530,	training loss: 0.035
epoch 46,	batch  1540,	training loss: 0.025
epoch 46,	batch  1550,	training loss: 0.016
epoch 46,	batch  1560,	training loss: 0.021
epoch 46,	batch  1570,	training loss: 0.028
epoch 46,	batch  1580,	training loss: 0.013
epoch 46,	batch  1590,	training loss: 0.023
epoch 46,	batch  1600,	training loss: 0.020
epoch 46,	batch  1610,	training loss: 0.027
epoch 46,	batch  1620,	training loss: 0.019
epoch 46,	batch  1630,	training loss: 0.087
epoch 46,	batch  1640,	training loss: 0.010
epoch 46,	batch  1650,	training loss: 0.045
epoch 46,	batch  1660,	training loss: 0.028
epoch 46,	batch  1670,	training loss: 0.025
epoch 46,	batch  1680,	training loss: 0.026
epoch 46,	batch  1690,	training loss: 0.024
epoch 46,	batch  1700,	training loss: 0.038
epoch 46,	batch  1710,	training loss: 0.023
epoch 46,	batch  1720,	training loss: 0.025
epoch 46,	batch  1730,	training loss: 0.032
epoch 46,	batch  1740,	training loss: 0.033
epoch 46,	batch  1750,	training loss: 0.026
epoch 46,	batch  1760,	training loss: 0.032
epoch 46,	batch  1770,	training loss: 0.018
epoch 46,	batch  1780,	training loss: 0.019
epoch 46,	batch  1790,	training loss: 0.014
epoch 46,	batch  1800,	training loss: 0.033
epoch 46,	batch  1810,	training loss: 0.023
epoch 46,	batch  1820,	training loss: 0.017
epoch 46,	batch  1830,	training loss: 0.100
epoch 46,	batch  1840,	training loss: 0.012
epoch 46,	batch  1850,	training loss: 0.014
epoch 46,	batch  1860,	training loss: 0.021
epoch 46,	batch  1870,	training loss: 0.042
epoch 46,	batch  1880,	training loss: 0.036
epoch 46,	batch  1890,	training loss: 0.025
epoch 46,	batch  1900,	training loss: 0.020
epoch 46,	batch  1910,	training loss: 0.013
epoch 46,	batch  1920,	training loss: 0.018
epoch 46,	batch  1930,	training loss: 0.020
epoch 46,	batch  1940,	training loss: 0.019
epoch 46,	batch  1950,	training loss: 0.036
epoch 46,	batch  1960,	training loss: 0.053
epoch 46,	batch  1970,	training loss: 0.024
epoch 46,	batch  1980,	training loss: 0.010
END OF EPOCH 46
Testing on validation set...
# correct:	5793/54904 = 10.551143814658312%
# off by 1:	10795/54904 = 19.66159114090048%

epoch 47,	batch    10,	training loss: 0.023
epoch 47,	batch    20,	training loss: 0.015
epoch 47,	batch    30,	training loss: 0.010
epoch 47,	batch    40,	training loss: 0.013
epoch 47,	batch    50,	training loss: 0.020
epoch 47,	batch    60,	training loss: 0.033
epoch 47,	batch    70,	training loss: 0.025
epoch 47,	batch    80,	training loss: 0.026
epoch 47,	batch    90,	training loss: 0.008
epoch 47,	batch   100,	training loss: 0.032
epoch 47,	batch   110,	training loss: 0.016
epoch 47,	batch   120,	training loss: 0.008
epoch 47,	batch   130,	training loss: 0.016
epoch 47,	batch   140,	training loss: 0.031
epoch 47,	batch   150,	training loss: 0.009
epoch 47,	batch   160,	training loss: 0.010
epoch 47,	batch   170,	training loss: 0.040
epoch 47,	batch   180,	training loss: 0.026
epoch 47,	batch   190,	training loss: 0.023
epoch 47,	batch   200,	training loss: 0.022
epoch 47,	batch   210,	training loss: 0.019
epoch 47,	batch   220,	training loss: 0.012
epoch 47,	batch   230,	training loss: 0.009
epoch 47,	batch   240,	training loss: 0.020
epoch 47,	batch   250,	training loss: 0.025
epoch 47,	batch   260,	training loss: 0.026
epoch 47,	batch   270,	training loss: 0.010
epoch 47,	batch   280,	training loss: 0.033
epoch 47,	batch   290,	training loss: 0.010
epoch 47,	batch   300,	training loss: 0.019
epoch 47,	batch   310,	training loss: 0.024
epoch 47,	batch   320,	training loss: 0.013
epoch 47,	batch   330,	training loss: 0.018
epoch 47,	batch   340,	training loss: 0.017
epoch 47,	batch   350,	training loss: 0.022
epoch 47,	batch   360,	training loss: 0.026
epoch 47,	batch   370,	training loss: 0.020
epoch 47,	batch   380,	training loss: 0.015
epoch 47,	batch   390,	training loss: 0.009
epoch 47,	batch   400,	training loss: 0.015
epoch 47,	batch   410,	training loss: 0.016
epoch 47,	batch   420,	training loss: 0.019
epoch 47,	batch   430,	training loss: 0.008
epoch 47,	batch   440,	training loss: 0.008
epoch 47,	batch   450,	training loss: 0.010
epoch 47,	batch   460,	training loss: 0.013
epoch 47,	batch   470,	training loss: 0.006
epoch 47,	batch   480,	training loss: 0.017
epoch 47,	batch   490,	training loss: 0.027
epoch 47,	batch   500,	training loss: 0.013
epoch 47,	batch   510,	training loss: 0.010
epoch 47,	batch   520,	training loss: 0.012
epoch 47,	batch   530,	training loss: 0.014
epoch 47,	batch   540,	training loss: 0.031
epoch 47,	batch   550,	training loss: 0.011
epoch 47,	batch   560,	training loss: 0.019
epoch 47,	batch   570,	training loss: 0.009
epoch 47,	batch   580,	training loss: 0.024
epoch 47,	batch   590,	training loss: 0.032
epoch 47,	batch   600,	training loss: 0.020
epoch 47,	batch   610,	training loss: 0.023
epoch 47,	batch   620,	training loss: 0.027
epoch 47,	batch   630,	training loss: 0.017
epoch 47,	batch   640,	training loss: 0.039
epoch 47,	batch   650,	training loss: 0.012
epoch 47,	batch   660,	training loss: 0.013
epoch 47,	batch   670,	training loss: 0.027
epoch 47,	batch   680,	training loss: 0.011
epoch 47,	batch   690,	training loss: 0.013
epoch 47,	batch   700,	training loss: 0.012
epoch 47,	batch   710,	training loss: 0.049
epoch 47,	batch   720,	training loss: 0.013
epoch 47,	batch   730,	training loss: 0.016
epoch 47,	batch   740,	training loss: 0.016
epoch 47,	batch   750,	training loss: 0.039
epoch 47,	batch   760,	training loss: 0.014
epoch 47,	batch   770,	training loss: 0.019
epoch 47,	batch   780,	training loss: 0.026
epoch 47,	batch   790,	training loss: 0.010
epoch 47,	batch   800,	training loss: 0.023
epoch 47,	batch   810,	training loss: 0.019
epoch 47,	batch   820,	training loss: 0.008
epoch 47,	batch   830,	training loss: 0.010
epoch 47,	batch   840,	training loss: 0.015
epoch 47,	batch   850,	training loss: 0.024
epoch 47,	batch   860,	training loss: 0.022
epoch 47,	batch   870,	training loss: 0.041
epoch 47,	batch   880,	training loss: 0.032
epoch 47,	batch   890,	training loss: 0.020
epoch 47,	batch   900,	training loss: 0.018
epoch 47,	batch   910,	training loss: 0.018
epoch 47,	batch   920,	training loss: 0.024
epoch 47,	batch   930,	training loss: 0.008
epoch 47,	batch   940,	training loss: 0.017
epoch 47,	batch   950,	training loss: 0.017
epoch 47,	batch   960,	training loss: 0.012
epoch 47,	batch   970,	training loss: 0.019
epoch 47,	batch   980,	training loss: 0.020
epoch 47,	batch   990,	training loss: 0.008
epoch 47,	batch  1000,	training loss: 0.018
epoch 47,	batch  1010,	training loss: 0.019
epoch 47,	batch  1020,	training loss: 0.023
epoch 47,	batch  1030,	training loss: 0.019
epoch 47,	batch  1040,	training loss: 0.009
epoch 47,	batch  1050,	training loss: 0.020
epoch 47,	batch  1060,	training loss: 0.019
epoch 47,	batch  1070,	training loss: 0.006
epoch 47,	batch  1080,	training loss: 0.019
epoch 47,	batch  1090,	training loss: 0.021
epoch 47,	batch  1100,	training loss: 0.020
epoch 47,	batch  1110,	training loss: 0.024
epoch 47,	batch  1120,	training loss: 0.045
epoch 47,	batch  1130,	training loss: 0.029
epoch 47,	batch  1140,	training loss: 0.075
epoch 47,	batch  1150,	training loss: 0.037
epoch 47,	batch  1160,	training loss: 0.011
epoch 47,	batch  1170,	training loss: 0.052
epoch 47,	batch  1180,	training loss: 0.033
epoch 47,	batch  1190,	training loss: 0.049
epoch 47,	batch  1200,	training loss: 0.034
epoch 47,	batch  1210,	training loss: 0.032
epoch 47,	batch  1220,	training loss: 0.019
epoch 47,	batch  1230,	training loss: 0.025
epoch 47,	batch  1240,	training loss: 0.038
epoch 47,	batch  1250,	training loss: 0.027
epoch 47,	batch  1260,	training loss: 0.033
epoch 47,	batch  1270,	training loss: 0.028
epoch 47,	batch  1280,	training loss: 0.025
epoch 47,	batch  1290,	training loss: 0.019
epoch 47,	batch  1300,	training loss: 0.030
epoch 47,	batch  1310,	training loss: 0.024
epoch 47,	batch  1320,	training loss: 0.038
epoch 47,	batch  1330,	training loss: 0.037
epoch 47,	batch  1340,	training loss: 0.007
epoch 47,	batch  1350,	training loss: 0.021
epoch 47,	batch  1360,	training loss: 0.030
epoch 47,	batch  1370,	training loss: 0.027
epoch 47,	batch  1380,	training loss: 0.026
epoch 47,	batch  1390,	training loss: 0.028
epoch 47,	batch  1400,	training loss: 0.018
epoch 47,	batch  1410,	training loss: 0.032
epoch 47,	batch  1420,	training loss: 0.017
epoch 47,	batch  1430,	training loss: 0.042
epoch 47,	batch  1440,	training loss: 0.027
epoch 47,	batch  1450,	training loss: 0.048
epoch 47,	batch  1460,	training loss: 0.035
epoch 47,	batch  1470,	training loss: 0.031
epoch 47,	batch  1480,	training loss: 0.034
epoch 47,	batch  1490,	training loss: 0.021
epoch 47,	batch  1500,	training loss: 0.023
epoch 47,	batch  1510,	training loss: 0.031
epoch 47,	batch  1520,	training loss: 0.031
epoch 47,	batch  1530,	training loss: 0.022
epoch 47,	batch  1540,	training loss: 0.018
epoch 47,	batch  1550,	training loss: 0.025
epoch 47,	batch  1560,	training loss: 0.011
epoch 47,	batch  1570,	training loss: 0.026
epoch 47,	batch  1580,	training loss: 0.017
epoch 47,	batch  1590,	training loss: 0.008
epoch 47,	batch  1600,	training loss: 0.021
epoch 47,	batch  1610,	training loss: 0.020
epoch 47,	batch  1620,	training loss: 0.020
epoch 47,	batch  1630,	training loss: 0.012
epoch 47,	batch  1640,	training loss: 0.008
epoch 47,	batch  1650,	training loss: 0.018
epoch 47,	batch  1660,	training loss: 0.020
epoch 47,	batch  1670,	training loss: 0.048
epoch 47,	batch  1680,	training loss: 0.011
epoch 47,	batch  1690,	training loss: 0.022
epoch 47,	batch  1700,	training loss: 0.029
epoch 47,	batch  1710,	training loss: 0.012
epoch 47,	batch  1720,	training loss: 0.035
epoch 47,	batch  1730,	training loss: 0.023
epoch 47,	batch  1740,	training loss: 0.019
epoch 47,	batch  1750,	training loss: 0.050
epoch 47,	batch  1760,	training loss: 0.022
epoch 47,	batch  1770,	training loss: 0.039
epoch 47,	batch  1780,	training loss: 0.008
epoch 47,	batch  1790,	training loss: 0.015
epoch 47,	batch  1800,	training loss: 0.013
epoch 47,	batch  1810,	training loss: 0.030
epoch 47,	batch  1820,	training loss: 0.025
epoch 47,	batch  1830,	training loss: 0.026
epoch 47,	batch  1840,	training loss: 0.023
epoch 47,	batch  1850,	training loss: 0.010
epoch 47,	batch  1860,	training loss: 0.027
epoch 47,	batch  1870,	training loss: 0.039
epoch 47,	batch  1880,	training loss: 0.023
epoch 47,	batch  1890,	training loss: 0.025
epoch 47,	batch  1900,	training loss: 0.049
epoch 47,	batch  1910,	training loss: 0.019
epoch 47,	batch  1920,	training loss: 0.027
epoch 47,	batch  1930,	training loss: 0.037
epoch 47,	batch  1940,	training loss: 0.049
epoch 47,	batch  1950,	training loss: 0.017
epoch 47,	batch  1960,	training loss: 0.021
epoch 47,	batch  1970,	training loss: 0.024
epoch 47,	batch  1980,	training loss: 0.015
END OF EPOCH 47
Testing on validation set...
# correct:	5599/54904 = 10.197799796007576%
# off by 1:	10386/54904 = 18.916654524260526%

epoch 48,	batch    10,	training loss: 0.019
epoch 48,	batch    20,	training loss: 0.040
epoch 48,	batch    30,	training loss: 0.038
epoch 48,	batch    40,	training loss: 0.017
epoch 48,	batch    50,	training loss: 0.036
epoch 48,	batch    60,	training loss: 0.024
epoch 48,	batch    70,	training loss: 0.017
epoch 48,	batch    80,	training loss: 0.011
epoch 48,	batch    90,	training loss: 0.016
epoch 48,	batch   100,	training loss: 0.020
epoch 48,	batch   110,	training loss: 0.008
epoch 48,	batch   120,	training loss: 0.011
epoch 48,	batch   130,	training loss: 0.014
epoch 48,	batch   140,	training loss: 0.025
epoch 48,	batch   150,	training loss: 0.013
epoch 48,	batch   160,	training loss: 0.013
epoch 48,	batch   170,	training loss: 0.012
epoch 48,	batch   180,	training loss: 0.011
epoch 48,	batch   190,	training loss: 0.027
epoch 48,	batch   200,	training loss: 0.011
epoch 48,	batch   210,	training loss: 0.018
epoch 48,	batch   220,	training loss: 0.016
epoch 48,	batch   230,	training loss: 0.006
epoch 48,	batch   240,	training loss: 0.023
epoch 48,	batch   250,	training loss: 0.009
epoch 48,	batch   260,	training loss: 0.029
epoch 48,	batch   270,	training loss: 0.021
epoch 48,	batch   280,	training loss: 0.013
epoch 48,	batch   290,	training loss: 0.010
epoch 48,	batch   300,	training loss: 0.009
epoch 48,	batch   310,	training loss: 0.014
epoch 48,	batch   320,	training loss: 0.004
epoch 48,	batch   330,	training loss: 0.011
epoch 48,	batch   340,	training loss: 0.019
epoch 48,	batch   350,	training loss: 0.011
epoch 48,	batch   360,	training loss: 0.009
epoch 48,	batch   370,	training loss: 0.030
epoch 48,	batch   380,	training loss: 0.033
epoch 48,	batch   390,	training loss: 0.019
epoch 48,	batch   400,	training loss: 0.012
epoch 48,	batch   410,	training loss: 0.013
epoch 48,	batch   420,	training loss: 0.017
epoch 48,	batch   430,	training loss: 0.019
epoch 48,	batch   440,	training loss: 0.009
epoch 48,	batch   450,	training loss: 0.017
epoch 48,	batch   460,	training loss: 0.041
epoch 48,	batch   470,	training loss: 0.031
epoch 48,	batch   480,	training loss: 0.009
epoch 48,	batch   490,	training loss: 0.007
epoch 48,	batch   500,	training loss: 0.011
epoch 48,	batch   510,	training loss: 0.016
epoch 48,	batch   520,	training loss: 0.020
epoch 48,	batch   530,	training loss: 0.038
epoch 48,	batch   540,	training loss: 0.025
epoch 48,	batch   550,	training loss: 0.022
epoch 48,	batch   560,	training loss: 0.012
epoch 48,	batch   570,	training loss: 0.007
epoch 48,	batch   580,	training loss: 0.015
epoch 48,	batch   590,	training loss: 0.014
epoch 48,	batch   600,	training loss: 0.023
epoch 48,	batch   610,	training loss: 0.011
epoch 48,	batch   620,	training loss: 0.011
epoch 48,	batch   630,	training loss: 0.010
epoch 48,	batch   640,	training loss: 0.007
epoch 48,	batch   650,	training loss: 0.017
epoch 48,	batch   660,	training loss: 0.016
epoch 48,	batch   670,	training loss: 0.005
epoch 48,	batch   680,	training loss: 0.006
epoch 48,	batch   690,	training loss: 0.034
epoch 48,	batch   700,	training loss: 0.009
epoch 48,	batch   710,	training loss: 0.009
epoch 48,	batch   720,	training loss: 0.015
epoch 48,	batch   730,	training loss: 0.013
epoch 48,	batch   740,	training loss: 0.008
epoch 48,	batch   750,	training loss: 0.005
epoch 48,	batch   760,	training loss: 0.009
epoch 48,	batch   770,	training loss: 0.023
epoch 48,	batch   780,	training loss: 0.028
epoch 48,	batch   790,	training loss: 0.027
epoch 48,	batch   800,	training loss: 0.014
epoch 48,	batch   810,	training loss: 0.029
epoch 48,	batch   820,	training loss: 0.006
epoch 48,	batch   830,	training loss: 0.013
epoch 48,	batch   840,	training loss: 0.007
epoch 48,	batch   850,	training loss: 0.044
epoch 48,	batch   860,	training loss: 0.014
epoch 48,	batch   870,	training loss: 0.007
epoch 48,	batch   880,	training loss: 0.024
epoch 48,	batch   890,	training loss: 0.029
epoch 48,	batch   900,	training loss: 0.056
epoch 48,	batch   910,	training loss: 0.023
epoch 48,	batch   920,	training loss: 0.020
epoch 48,	batch   930,	training loss: 0.033
epoch 48,	batch   940,	training loss: 0.017
epoch 48,	batch   950,	training loss: 0.021
epoch 48,	batch   960,	training loss: 0.016
epoch 48,	batch   970,	training loss: 0.015
epoch 48,	batch   980,	training loss: 0.011
epoch 48,	batch   990,	training loss: 0.009
epoch 48,	batch  1000,	training loss: 0.029
epoch 48,	batch  1010,	training loss: 0.009
epoch 48,	batch  1020,	training loss: 0.028
epoch 48,	batch  1030,	training loss: 0.024
epoch 48,	batch  1040,	training loss: 0.030
epoch 48,	batch  1050,	training loss: 0.025
epoch 48,	batch  1060,	training loss: 0.029
epoch 48,	batch  1070,	training loss: 0.010
epoch 48,	batch  1080,	training loss: 0.022
epoch 48,	batch  1090,	training loss: 0.016
epoch 48,	batch  1100,	training loss: 0.027
epoch 48,	batch  1110,	training loss: 0.029
epoch 48,	batch  1120,	training loss: 0.030
epoch 48,	batch  1130,	training loss: 0.017
epoch 48,	batch  1140,	training loss: 0.025
epoch 48,	batch  1150,	training loss: 0.014
epoch 48,	batch  1160,	training loss: 0.022
epoch 48,	batch  1170,	training loss: 0.036
epoch 48,	batch  1180,	training loss: 0.025
epoch 48,	batch  1190,	training loss: 0.012
epoch 48,	batch  1200,	training loss: 0.016
epoch 48,	batch  1210,	training loss: 0.030
epoch 48,	batch  1220,	training loss: 0.015
epoch 48,	batch  1230,	training loss: 0.039
epoch 48,	batch  1240,	training loss: 0.035
epoch 48,	batch  1250,	training loss: 0.007
epoch 48,	batch  1260,	training loss: 0.017
epoch 48,	batch  1270,	training loss: 0.021
epoch 48,	batch  1280,	training loss: 0.008
epoch 48,	batch  1290,	training loss: 0.023
epoch 48,	batch  1300,	training loss: 0.029
epoch 48,	batch  1310,	training loss: 0.026
epoch 48,	batch  1320,	training loss: 0.021
epoch 48,	batch  1330,	training loss: 0.033
epoch 48,	batch  1340,	training loss: 0.040
epoch 48,	batch  1350,	training loss: 0.025
epoch 48,	batch  1360,	training loss: 0.025
epoch 48,	batch  1370,	training loss: 0.025
epoch 48,	batch  1380,	training loss: 0.046
epoch 48,	batch  1390,	training loss: 0.031
epoch 48,	batch  1400,	training loss: 0.032
epoch 48,	batch  1410,	training loss: 0.041
epoch 48,	batch  1420,	training loss: 0.058
epoch 48,	batch  1430,	training loss: 0.065
epoch 48,	batch  1440,	training loss: 0.026
epoch 48,	batch  1450,	training loss: 0.028
epoch 48,	batch  1460,	training loss: 0.041
epoch 48,	batch  1470,	training loss: 0.023
epoch 48,	batch  1480,	training loss: 0.022
epoch 48,	batch  1490,	training loss: 0.060
epoch 48,	batch  1500,	training loss: 0.018
epoch 48,	batch  1510,	training loss: 0.037
epoch 48,	batch  1520,	training loss: 0.023
epoch 48,	batch  1530,	training loss: 0.028
epoch 48,	batch  1540,	training loss: 0.027
epoch 48,	batch  1550,	training loss: 0.015
epoch 48,	batch  1560,	training loss: 0.013
epoch 48,	batch  1570,	training loss: 0.021
epoch 48,	batch  1580,	training loss: 0.041
epoch 48,	batch  1590,	training loss: 0.026
epoch 48,	batch  1600,	training loss: 0.037
epoch 48,	batch  1610,	training loss: 0.042
epoch 48,	batch  1620,	training loss: 0.019
epoch 48,	batch  1630,	training loss: 0.041
epoch 48,	batch  1640,	training loss: 0.015
epoch 48,	batch  1650,	training loss: 0.017
epoch 48,	batch  1660,	training loss: 0.022
epoch 48,	batch  1670,	training loss: 0.027
epoch 48,	batch  1680,	training loss: 0.049
epoch 48,	batch  1690,	training loss: 0.015
epoch 48,	batch  1700,	training loss: 0.024
epoch 48,	batch  1710,	training loss: 0.019
epoch 48,	batch  1720,	training loss: 0.010
epoch 48,	batch  1730,	training loss: 0.012
epoch 48,	batch  1740,	training loss: 0.020
epoch 48,	batch  1750,	training loss: 0.050
epoch 48,	batch  1760,	training loss: 0.029
epoch 48,	batch  1770,	training loss: 0.038
epoch 48,	batch  1780,	training loss: 0.018
epoch 48,	batch  1790,	training loss: 0.036
epoch 48,	batch  1800,	training loss: 0.030
epoch 48,	batch  1810,	training loss: 0.020
epoch 48,	batch  1820,	training loss: 0.037
epoch 48,	batch  1830,	training loss: 0.039
epoch 48,	batch  1840,	training loss: 0.026
epoch 48,	batch  1850,	training loss: 0.051
epoch 48,	batch  1860,	training loss: 0.054
epoch 48,	batch  1870,	training loss: 0.046
epoch 48,	batch  1880,	training loss: 0.020
epoch 48,	batch  1890,	training loss: 0.028
epoch 48,	batch  1900,	training loss: 0.021
epoch 48,	batch  1910,	training loss: 0.017
epoch 48,	batch  1920,	training loss: 0.017
epoch 48,	batch  1930,	training loss: 0.017
epoch 48,	batch  1940,	training loss: 0.021
epoch 48,	batch  1950,	training loss: 0.034
epoch 48,	batch  1960,	training loss: 0.011
epoch 48,	batch  1970,	training loss: 0.019
epoch 48,	batch  1980,	training loss: 0.010
END OF EPOCH 48
Testing on validation set...
# correct:	6026/54904 = 10.975520909223372%
# off by 1:	10707/54904 = 19.501311379863033%

epoch 49,	batch    10,	training loss: 0.021
epoch 49,	batch    20,	training loss: 0.007
epoch 49,	batch    30,	training loss: 0.027
epoch 49,	batch    40,	training loss: 0.016
epoch 49,	batch    50,	training loss: 0.020
epoch 49,	batch    60,	training loss: 0.019
epoch 49,	batch    70,	training loss: 0.016
epoch 49,	batch    80,	training loss: 0.045
epoch 49,	batch    90,	training loss: 0.017
epoch 49,	batch   100,	training loss: 0.015
epoch 49,	batch   110,	training loss: 0.008
epoch 49,	batch   120,	training loss: 0.020
epoch 49,	batch   130,	training loss: 0.013
epoch 49,	batch   140,	training loss: 0.008
epoch 49,	batch   150,	training loss: 0.008
epoch 49,	batch   160,	training loss: 0.012
epoch 49,	batch   170,	training loss: 0.014
epoch 49,	batch   180,	training loss: 0.019
epoch 49,	batch   190,	training loss: 0.040
epoch 49,	batch   200,	training loss: 0.011
epoch 49,	batch   210,	training loss: 0.018
epoch 49,	batch   220,	training loss: 0.013
epoch 49,	batch   230,	training loss: 0.010
epoch 49,	batch   240,	training loss: 0.018
epoch 49,	batch   250,	training loss: 0.022
epoch 49,	batch   260,	training loss: 0.009
epoch 49,	batch   270,	training loss: 0.026
epoch 49,	batch   280,	training loss: 0.021
epoch 49,	batch   290,	training loss: 0.018
epoch 49,	batch   300,	training loss: 0.015
epoch 49,	batch   310,	training loss: 0.013
epoch 49,	batch   320,	training loss: 0.028
epoch 49,	batch   330,	training loss: 0.023
epoch 49,	batch   340,	training loss: 0.013
epoch 49,	batch   350,	training loss: 0.007
epoch 49,	batch   360,	training loss: 0.012
epoch 49,	batch   370,	training loss: 0.014
epoch 49,	batch   380,	training loss: 0.008
epoch 49,	batch   390,	training loss: 0.026
epoch 49,	batch   400,	training loss: 0.027
epoch 49,	batch   410,	training loss: 0.038
epoch 49,	batch   420,	training loss: 0.029
epoch 49,	batch   430,	training loss: 0.012
epoch 49,	batch   440,	training loss: 0.026
epoch 49,	batch   450,	training loss: 0.045
epoch 49,	batch   460,	training loss: 0.021
epoch 49,	batch   470,	training loss: 0.053
epoch 49,	batch   480,	training loss: 0.022
epoch 49,	batch   490,	training loss: 0.025
epoch 49,	batch   500,	training loss: 0.009
epoch 49,	batch   510,	training loss: 0.017
epoch 49,	batch   520,	training loss: 0.011
epoch 49,	batch   530,	training loss: 0.006
epoch 49,	batch   540,	training loss: 0.016
epoch 49,	batch   550,	training loss: 0.013
epoch 49,	batch   560,	training loss: 0.014
epoch 49,	batch   570,	training loss: 0.010
epoch 49,	batch   580,	training loss: 0.074
epoch 49,	batch   590,	training loss: 0.016
epoch 49,	batch   600,	training loss: 0.014
epoch 49,	batch   610,	training loss: 0.011
epoch 49,	batch   620,	training loss: 0.027
epoch 49,	batch   630,	training loss: 0.017
epoch 49,	batch   640,	training loss: 0.015
epoch 49,	batch   650,	training loss: 0.020
epoch 49,	batch   660,	training loss: 0.011
epoch 49,	batch   670,	training loss: 0.014
epoch 49,	batch   680,	training loss: 0.007
epoch 49,	batch   690,	training loss: 0.006
epoch 49,	batch   700,	training loss: 0.060
epoch 49,	batch   710,	training loss: 0.012
epoch 49,	batch   720,	training loss: 0.032
epoch 49,	batch   730,	training loss: 0.028
epoch 49,	batch   740,	training loss: 0.020
epoch 49,	batch   750,	training loss: 0.025
epoch 49,	batch   760,	training loss: 0.010
epoch 49,	batch   770,	training loss: 0.021
epoch 49,	batch   780,	training loss: 0.012
epoch 49,	batch   790,	training loss: 0.021
epoch 49,	batch   800,	training loss: 0.014
epoch 49,	batch   810,	training loss: 0.018
epoch 49,	batch   820,	training loss: 0.017
epoch 49,	batch   830,	training loss: 0.010
epoch 49,	batch   840,	training loss: 0.030
epoch 49,	batch   850,	training loss: 0.006
epoch 49,	batch   860,	training loss: 0.070
epoch 49,	batch   870,	training loss: 0.037
epoch 49,	batch   880,	training loss: 0.018
epoch 49,	batch   890,	training loss: 0.011
epoch 49,	batch   900,	training loss: 0.040
epoch 49,	batch   910,	training loss: 0.022
epoch 49,	batch   920,	training loss: 0.026
epoch 49,	batch   930,	training loss: 0.033
epoch 49,	batch   940,	training loss: 0.014
epoch 49,	batch   950,	training loss: 0.034
epoch 49,	batch   960,	training loss: 0.038
epoch 49,	batch   970,	training loss: 0.019
epoch 49,	batch   980,	training loss: 0.021
epoch 49,	batch   990,	training loss: 0.023
epoch 49,	batch  1000,	training loss: 0.012
epoch 49,	batch  1010,	training loss: 0.037
epoch 49,	batch  1020,	training loss: 0.053
epoch 49,	batch  1030,	training loss: 0.023
epoch 49,	batch  1040,	training loss: 0.014
epoch 49,	batch  1050,	training loss: 0.012
epoch 49,	batch  1060,	training loss: 0.014
epoch 49,	batch  1070,	training loss: 0.005
epoch 49,	batch  1080,	training loss: 0.044
epoch 49,	batch  1090,	training loss: 0.034
epoch 49,	batch  1100,	training loss: 0.013
epoch 49,	batch  1110,	training loss: 0.018
epoch 49,	batch  1120,	training loss: 0.026
epoch 49,	batch  1130,	training loss: 0.039
epoch 49,	batch  1140,	training loss: 0.020
epoch 49,	batch  1150,	training loss: 0.018
epoch 49,	batch  1160,	training loss: 0.023
epoch 49,	batch  1170,	training loss: 0.028
epoch 49,	batch  1180,	training loss: 0.013
epoch 49,	batch  1190,	training loss: 0.043
epoch 49,	batch  1200,	training loss: 0.017
epoch 49,	batch  1210,	training loss: 0.017
epoch 49,	batch  1220,	training loss: 0.022
epoch 49,	batch  1230,	training loss: 0.077
epoch 49,	batch  1240,	training loss: 0.022
epoch 49,	batch  1250,	training loss: 0.017
epoch 49,	batch  1260,	training loss: 0.027
epoch 49,	batch  1270,	training loss: 0.025
epoch 49,	batch  1280,	training loss: 0.015
epoch 49,	batch  1290,	training loss: 0.023
epoch 49,	batch  1300,	training loss: 0.037
epoch 49,	batch  1310,	training loss: 0.030
epoch 49,	batch  1320,	training loss: 0.023
epoch 49,	batch  1330,	training loss: 0.019
epoch 49,	batch  1340,	training loss: 0.031
epoch 49,	batch  1350,	training loss: 0.035
epoch 49,	batch  1360,	training loss: 0.011
epoch 49,	batch  1370,	training loss: 0.028
epoch 49,	batch  1380,	training loss: 0.015
epoch 49,	batch  1390,	training loss: 0.031
epoch 49,	batch  1400,	training loss: 0.012
epoch 49,	batch  1410,	training loss: 0.059
epoch 49,	batch  1420,	training loss: 0.019
epoch 49,	batch  1430,	training loss: 0.016
epoch 49,	batch  1440,	training loss: 0.019
epoch 49,	batch  1450,	training loss: 0.043
epoch 49,	batch  1460,	training loss: 0.016
epoch 49,	batch  1470,	training loss: 0.025
epoch 49,	batch  1480,	training loss: 0.019
epoch 49,	batch  1490,	training loss: 0.033
epoch 49,	batch  1500,	training loss: 0.009
epoch 49,	batch  1510,	training loss: 0.022
epoch 49,	batch  1520,	training loss: 0.014
epoch 49,	batch  1530,	training loss: 0.012
epoch 49,	batch  1540,	training loss: 0.023
epoch 49,	batch  1550,	training loss: 0.011
epoch 49,	batch  1560,	training loss: 0.018
epoch 49,	batch  1570,	training loss: 0.037
epoch 49,	batch  1580,	training loss: 0.006
epoch 49,	batch  1590,	training loss: 0.012
epoch 49,	batch  1600,	training loss: 0.010
epoch 49,	batch  1610,	training loss: 0.021
epoch 49,	batch  1620,	training loss: 0.021
epoch 49,	batch  1630,	training loss: 0.013
epoch 49,	batch  1640,	training loss: 0.009
epoch 49,	batch  1650,	training loss: 0.009
epoch 49,	batch  1660,	training loss: 0.031
epoch 49,	batch  1670,	training loss: 0.026
epoch 49,	batch  1680,	training loss: 0.023
epoch 49,	batch  1690,	training loss: 0.026
epoch 49,	batch  1700,	training loss: 0.014
epoch 49,	batch  1710,	training loss: 0.022
epoch 49,	batch  1720,	training loss: 0.028
epoch 49,	batch  1730,	training loss: 0.012
epoch 49,	batch  1740,	training loss: 0.018
epoch 49,	batch  1750,	training loss: 0.011
epoch 49,	batch  1760,	training loss: 0.014
epoch 49,	batch  1770,	training loss: 0.034
epoch 49,	batch  1780,	training loss: 0.015
epoch 49,	batch  1790,	training loss: 0.027
epoch 49,	batch  1800,	training loss: 0.037
epoch 49,	batch  1810,	training loss: 0.087
epoch 49,	batch  1820,	training loss: 0.023
epoch 49,	batch  1830,	training loss: 0.011
epoch 49,	batch  1840,	training loss: 0.019
epoch 49,	batch  1850,	training loss: 0.010
epoch 49,	batch  1860,	training loss: 0.017
epoch 49,	batch  1870,	training loss: 0.024
epoch 49,	batch  1880,	training loss: 0.028
epoch 49,	batch  1890,	training loss: 0.022
epoch 49,	batch  1900,	training loss: 0.017
epoch 49,	batch  1910,	training loss: 0.027
epoch 49,	batch  1920,	training loss: 0.034
epoch 49,	batch  1930,	training loss: 0.014
epoch 49,	batch  1940,	training loss: 0.029
epoch 49,	batch  1950,	training loss: 0.087
epoch 49,	batch  1960,	training loss: 0.018
epoch 49,	batch  1970,	training loss: 0.021
epoch 49,	batch  1980,	training loss: 0.013
END OF EPOCH 49
Testing on validation set...
# correct:	6020/54904 = 10.964592743698091%
# off by 1:	11001/54904 = 20.03679149060178%

epoch 50,	batch    10,	training loss: 0.017
epoch 50,	batch    20,	training loss: 0.043
epoch 50,	batch    30,	training loss: 0.060
epoch 50,	batch    40,	training loss: 0.010
epoch 50,	batch    50,	training loss: 0.014
epoch 50,	batch    60,	training loss: 0.018
epoch 50,	batch    70,	training loss: 0.012
epoch 50,	batch    80,	training loss: 0.040
epoch 50,	batch    90,	training loss: 0.018
epoch 50,	batch   100,	training loss: 0.023
epoch 50,	batch   110,	training loss: 0.006
epoch 50,	batch   120,	training loss: 0.014
epoch 50,	batch   130,	training loss: 0.015
epoch 50,	batch   140,	training loss: 0.016
epoch 50,	batch   150,	training loss: 0.016
epoch 50,	batch   160,	training loss: 0.031
epoch 50,	batch   170,	training loss: 0.007
epoch 50,	batch   180,	training loss: 0.019
epoch 50,	batch   190,	training loss: 0.013
epoch 50,	batch   200,	training loss: 0.040
epoch 50,	batch   210,	training loss: 0.017
epoch 50,	batch   220,	training loss: 0.010
epoch 50,	batch   230,	training loss: 0.014
epoch 50,	batch   240,	training loss: 0.022
epoch 50,	batch   250,	training loss: 0.008
epoch 50,	batch   260,	training loss: 0.003
epoch 50,	batch   270,	training loss: 0.011
epoch 50,	batch   280,	training loss: 0.019
epoch 50,	batch   290,	training loss: 0.007
epoch 50,	batch   300,	training loss: 0.038
epoch 50,	batch   310,	training loss: 0.005
epoch 50,	batch   320,	training loss: 0.006
epoch 50,	batch   330,	training loss: 0.011
epoch 50,	batch   340,	training loss: 0.009
epoch 50,	batch   350,	training loss: 0.022
epoch 50,	batch   360,	training loss: 0.004
epoch 50,	batch   370,	training loss: 0.024
epoch 50,	batch   380,	training loss: 0.041
epoch 50,	batch   390,	training loss: 0.009
epoch 50,	batch   400,	training loss: 0.015
epoch 50,	batch   410,	training loss: 0.009
epoch 50,	batch   420,	training loss: 0.020
epoch 50,	batch   430,	training loss: 0.010
epoch 50,	batch   440,	training loss: 0.016
epoch 50,	batch   450,	training loss: 0.006
epoch 50,	batch   460,	training loss: 0.012
epoch 50,	batch   470,	training loss: 0.014
epoch 50,	batch   480,	training loss: 0.015
epoch 50,	batch   490,	training loss: 0.028
epoch 50,	batch   500,	training loss: 0.008
epoch 50,	batch   510,	training loss: 0.014
epoch 50,	batch   520,	training loss: 0.047
epoch 50,	batch   530,	training loss: 0.006
epoch 50,	batch   540,	training loss: 0.015
epoch 50,	batch   550,	training loss: 0.011
epoch 50,	batch   560,	training loss: 0.010
epoch 50,	batch   570,	training loss: 0.016
epoch 50,	batch   580,	training loss: 0.004
epoch 50,	batch   590,	training loss: 0.017
epoch 50,	batch   600,	training loss: 0.007
epoch 50,	batch   610,	training loss: 0.023
epoch 50,	batch   620,	training loss: 0.009
epoch 50,	batch   630,	training loss: 0.028
epoch 50,	batch   640,	training loss: 0.014
epoch 50,	batch   650,	training loss: 0.021
epoch 50,	batch   660,	training loss: 0.015
epoch 50,	batch   670,	training loss: 0.022
epoch 50,	batch   680,	training loss: 0.006
epoch 50,	batch   690,	training loss: 0.009
epoch 50,	batch   700,	training loss: 0.011
epoch 50,	batch   710,	training loss: 0.009
epoch 50,	batch   720,	training loss: 0.020
epoch 50,	batch   730,	training loss: 0.011
epoch 50,	batch   740,	training loss: 0.010
epoch 50,	batch   750,	training loss: 0.064
epoch 50,	batch   760,	training loss: 0.015
epoch 50,	batch   770,	training loss: 0.016
epoch 50,	batch   780,	training loss: 0.017
epoch 50,	batch   790,	training loss: 0.012
epoch 50,	batch   800,	training loss: 0.010
epoch 50,	batch   810,	training loss: 0.022
epoch 50,	batch   820,	training loss: 0.006
epoch 50,	batch   830,	training loss: 0.030
epoch 50,	batch   840,	training loss: 0.010
epoch 50,	batch   850,	training loss: 0.018
epoch 50,	batch   860,	training loss: 0.008
epoch 50,	batch   870,	training loss: 0.012
epoch 50,	batch   880,	training loss: 0.012
epoch 50,	batch   890,	training loss: 0.014
epoch 50,	batch   900,	training loss: 0.025
epoch 50,	batch   910,	training loss: 0.004
epoch 50,	batch   920,	training loss: 0.013
epoch 50,	batch   930,	training loss: 0.031
epoch 50,	batch   940,	training loss: 0.011
epoch 50,	batch   950,	training loss: 0.008
epoch 50,	batch   960,	training loss: 0.021
epoch 50,	batch   970,	training loss: 0.019
epoch 50,	batch   980,	training loss: 0.027
epoch 50,	batch   990,	training loss: 0.007
epoch 50,	batch  1000,	training loss: 0.009
epoch 50,	batch  1010,	training loss: 0.033
epoch 50,	batch  1020,	training loss: 0.022
epoch 50,	batch  1030,	training loss: 0.021
epoch 50,	batch  1040,	training loss: 0.011
epoch 50,	batch  1050,	training loss: 0.016
epoch 50,	batch  1060,	training loss: 0.040
epoch 50,	batch  1070,	training loss: 0.006
epoch 50,	batch  1080,	training loss: 0.021
epoch 50,	batch  1090,	training loss: 0.012
epoch 50,	batch  1100,	training loss: 0.010
epoch 50,	batch  1110,	training loss: 0.008
epoch 50,	batch  1120,	training loss: 0.035
epoch 50,	batch  1130,	training loss: 0.022
epoch 50,	batch  1140,	training loss: 0.004
epoch 50,	batch  1150,	training loss: 0.014
epoch 50,	batch  1160,	training loss: 0.013
epoch 50,	batch  1170,	training loss: 0.012
epoch 50,	batch  1180,	training loss: 0.012
epoch 50,	batch  1190,	training loss: 0.028
epoch 50,	batch  1200,	training loss: 0.016
epoch 50,	batch  1210,	training loss: 0.046
epoch 50,	batch  1220,	training loss: 0.016
epoch 50,	batch  1230,	training loss: 0.019
epoch 50,	batch  1240,	training loss: 0.011
epoch 50,	batch  1250,	training loss: 0.012
epoch 50,	batch  1260,	training loss: 0.026
epoch 50,	batch  1270,	training loss: 0.015
epoch 50,	batch  1280,	training loss: 0.020
epoch 50,	batch  1290,	training loss: 0.019
epoch 50,	batch  1300,	training loss: 0.017
epoch 50,	batch  1310,	training loss: 0.045
epoch 50,	batch  1320,	training loss: 0.009
epoch 50,	batch  1330,	training loss: 0.038
epoch 50,	batch  1340,	training loss: 0.009
epoch 50,	batch  1350,	training loss: 0.023
epoch 50,	batch  1360,	training loss: 0.025
epoch 50,	batch  1370,	training loss: 0.017
epoch 50,	batch  1380,	training loss: 0.031
epoch 50,	batch  1390,	training loss: 0.027
epoch 50,	batch  1400,	training loss: 0.015
epoch 50,	batch  1410,	training loss: 0.068
epoch 50,	batch  1420,	training loss: 0.029
epoch 50,	batch  1430,	training loss: 0.011
epoch 50,	batch  1440,	training loss: 0.011
epoch 50,	batch  1450,	training loss: 0.009
epoch 50,	batch  1460,	training loss: 0.023
epoch 50,	batch  1470,	training loss: 0.010
epoch 50,	batch  1480,	training loss: 0.019
epoch 50,	batch  1490,	training loss: 0.013
epoch 50,	batch  1500,	training loss: 0.030
epoch 50,	batch  1510,	training loss: 0.007
epoch 50,	batch  1520,	training loss: 0.013
epoch 50,	batch  1530,	training loss: 0.013
epoch 50,	batch  1540,	training loss: 0.020
epoch 50,	batch  1550,	training loss: 0.068
epoch 50,	batch  1560,	training loss: 0.029
epoch 50,	batch  1570,	training loss: 0.030
epoch 50,	batch  1580,	training loss: 0.028
epoch 50,	batch  1590,	training loss: 0.016
epoch 50,	batch  1600,	training loss: 0.027
epoch 50,	batch  1610,	training loss: 0.019
epoch 50,	batch  1620,	training loss: 0.011
epoch 50,	batch  1630,	training loss: 0.015
epoch 50,	batch  1640,	training loss: 0.015
epoch 50,	batch  1650,	training loss: 0.013
epoch 50,	batch  1660,	training loss: 0.022
epoch 50,	batch  1670,	training loss: 0.015
epoch 50,	batch  1680,	training loss: 0.026
epoch 50,	batch  1690,	training loss: 0.025
epoch 50,	batch  1700,	training loss: 0.032
epoch 50,	batch  1710,	training loss: 0.036
epoch 50,	batch  1720,	training loss: 0.015
epoch 50,	batch  1730,	training loss: 0.029
epoch 50,	batch  1740,	training loss: 0.008
epoch 50,	batch  1750,	training loss: 0.024
epoch 50,	batch  1760,	training loss: 0.028
epoch 50,	batch  1770,	training loss: 0.057
epoch 50,	batch  1780,	training loss: 0.019
epoch 50,	batch  1790,	training loss: 0.028
epoch 50,	batch  1800,	training loss: 0.012
epoch 50,	batch  1810,	training loss: 0.009
epoch 50,	batch  1820,	training loss: 0.021
epoch 50,	batch  1830,	training loss: 0.017
epoch 50,	batch  1840,	training loss: 0.037
epoch 50,	batch  1850,	training loss: 0.015
epoch 50,	batch  1860,	training loss: 0.069
epoch 50,	batch  1870,	training loss: 0.009
epoch 50,	batch  1880,	training loss: 0.032
epoch 50,	batch  1890,	training loss: 0.018
epoch 50,	batch  1900,	training loss: 0.026
epoch 50,	batch  1910,	training loss: 0.035
epoch 50,	batch  1920,	training loss: 0.022
epoch 50,	batch  1930,	training loss: 0.020
epoch 50,	batch  1940,	training loss: 0.020
epoch 50,	batch  1950,	training loss: 0.014
epoch 50,	batch  1960,	training loss: 0.010
epoch 50,	batch  1970,	training loss: 0.018
epoch 50,	batch  1980,	training loss: 0.022
END OF EPOCH 50
Testing on validation set...
# correct:	5919/54904 = 10.780635290689203%
# off by 1:	10596/54904 = 19.299140317645346%

FINISHED TRAINING
SAVING NETWORK
